{
    "fetched_at": "2021-01-22T13:04:34.830402",
    "news": [
        {
            "authors": [],
            "title": "Are you a robot?",
            "contents": "Why did this happen?\n\nPlease make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy.",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [],
            "title": "A Computational Model for Intelligent Manufacturing",
            "contents": "By maximizing the flexibility of human inspection, mobility, and processing, Nanotronics R&D department leverages AI for optimal integration.\n\nBy Andrew Sundstrom, Senior R&D Engineer, Nanotronics\n\nDue to human limits and blind spots, and static process control, manufacturing errors often go undetected or unreported and are propagated. To make factories more efficient, resilient, and secure, we need to begin by reconsidering how errors are detected and remedied in the assembly line by using a more dynamic approach. Artificial Intelligence Process Control (AIPC) finds solutions to defects in near real-time, during the manufacturing process.\n\nIn conventional assembly workflows, we rely on human operators to detect manufacturing errors and ascertain how to remedy them. Even highly trained and experienced operators often lack the time and response rate necessary to make the correct and often subtle changes fast enough to have the desired effect on the system.\n\nOperators may also miss a defect in a part early in the manufacturing cycle and continue downstream processing as usual. When a defect remains unresolved until the final inspection, it means a failure of the finished goods and wasted production material, time, and labor \u2014 a potential yield loss reaching millions of dollars.\n\nDespite node-by-node process monitoring and adherence to statistical process control* (SPC) protocols (e.g. Six Sigma), manufacturers still rely on final human inspections of the finished product to detect and assess part defects and nodal corrections for subsequent parts.\n\nThe process waste is not just in the time and material of assembling a faulty part, but also in the lack of system-wide feedback on the cause and possible solutions to a single defect. Many errors are not the result of a one-time random occurrence, but rather, derive from an unrecognized inherent action that occurs at low frequency, such as human fatigue or environmental variations.\n\nThe typical approach of replacing error-prone, low-performing human operators with higher-performing employees is unsustainable, and does not address a more complex, unrecognized issue. The same can be said for environmental variations caused by atypical ambient conditions, for example those caused by climate change, which are outside of control specifications but are starting to become more frequent. In missing the solution to one defect, we may be missing a solution to a pattern.\n\nTo account for these limitations, and to illustrate the salient principles and problems, my team and I developed a system and Deep Learning model for the detection of process-intrinsic errors and feedback actions to correct for errors. Taken together, we call this approach Artificial Intelligence Process Control (AIPC).\n\nOur first application for AIPC\u2014folding a paper airplane\u2014may seem simple in description. Yet we found that the task of folding a paper airplane contains direct parallels to complex manual assembly lines consisting of more nodes.\n\nAIPC is a dynamic, interventional approach to process control, where each node subsequent to the node causing a detected error is woven into an optimization problem\u2014the error recovery problem\u2014and actively controlled to instantiate a solution to it. With the airplane folding model, we inserted various errors early on in the process and gave a computational model that uses targeted Gated Recurrent Unit (GRU) learning the task to efficiently search the vast space of possible recovery paths through the remaining steps in the assembly process and find the optimal one.\n\nWe chose to use a GRU model instead of a Deep Reinforcement Learning one because of its evident predictive power for structural recovery and performance and its relatively short training time. By developing and testing a GRU model on a simplified task, we leveraged simulations we developed for this problem that would not necessarily be available for a more complex problem, but the model can be applied to complex systems through transfer learning, with a shallower learning curve than if natively developed.\n\nOur results demonstrated that a trained, predictive GRU model could give targeted corrective responses to a wide range of errors introduced or detected early in the formation of a complex manufactured object. These corrective responses were significantly superior to those given by a reasonable corrective strategy of applying error-subsequent parameter values directly from the canonical object specification.\n\nMoreover, these results are encouraging from a Deep Learning perspective, since they demonstrate a GRU model can perform well on a problem where the sequence to be generated is selected from a vast combinatorial space, arising from a sequence-contingent manufacturing process that has no analytical solution. This highlights the practical application of Deep Learning for manufacturing, and validates the AIPC approach, which finds solutions in near real-time, while each cycle is ongoing, rather than at the end of a given cycle.\n\nNanotronics R&D is working with several industrial collaborators to deploy prototype AIPC solutions across a range of factory scales and types. One of the promising applications of AIPC to emerge from our research and development is securing industrial production from sophisticated cyberattacks \u2014 the subject of our next column.\n\nAndrew Sundstrom received his BA in computer science from Cornell University, and his MS in computer science and PhD in computational biology from the Courant Institute of Mathematical Sciences at NYU. He was a Senior Research Scientist at the Courant Institute, a Postdoctoral Research Fellow at the Icahn School of Medicine at Mount Sinai, and Assistant Professor of Genetics and Genomics Sciences at Mount Sinai. Andrew joined Nanotronics in 2018 as a Senior R&D Engineer, investigating how to employ machine learning and other methods from artificial intelligence to make factories and industrial processes smarter, more resilient, more productive, and more secure. Andrew has 28 years of professional experience, 18 of which are outside of academia, where he has made contributions as a software engineer and researcher at Prime Factors, Nortel Networks, IBM T.J. Watson Research Center, and Morgan Stanley. Andrew has 9 peer-reviewed publications (5 as first author) and 2 issued patents. His complete bio is available at aesundstrom.com.\n\nReference: Andrew Sundstrom, et al. \u201cA computational model for decision-making and assembly optimization in manufacturing\u201d. Proceedings of the American Control Conference 2020, Denver, Colorado, USA (31 Jul 2020). [doi: 10.23919/ACC45564.2020.9147715]\n\n* A static, non-interventional approach to process control, invented by Walter A. Stewhard and developed by W. Edwards Deming, where well-defined statistical properties are passively observed to pass or fail at each node, and only after the last node\u2019s processing is a decision made as to whether to keep or discard the manufactured product.",
            "published_at": "2021-01-21T18:09:34+00:00"
        },
        {
            "authors": [],
            "title": "How We Ported Linux to the M1",
            "contents": "1. Apple special sauce\n\nWhen Apple released their desktop products with the M1 processor in November 2020, quite a few people in the tech community were surprised by the excellent performance of these systems. But those who have been following the development of Apple phone chipsets closely knew that the evolutionary path Apple followed would result in a powerful 64-bit ARM processor.\n\nAt Corellium, we've been tracking the Apple mobile ecosystem since iPhone 6, released in 2014 with two 64-bit cores. Since then, Apple has been focusing their energy on building faster chips, preferring to improve single-threaded performance over throwing more cores on the chip. This approach was enabled by their in-house hardware design team, and resulted in unique parts with a broad feature set, leading the industry in terms of architectural features.\n\nIt also made Apple silicon rather distinct from all other 64-bit ARM hardware in terms of both CPU core and peripherals. Our Corellium virtualization platform has been providing security researchers with unparalleled insight into how operating systems and programs work on Apple ARM processors. But in the process of developing our virtualization system, we also gain knowledge about the hardware we are modeling, and this knowledge can be best refined by testing it against real hardware - which we have only been able to do with the emergence of checkm8, an exploit that let us load programs onto Apple smartphones. This led directly to the Sandcastle project, where we built a kernel port to the A10 processor in early 2020.\n\nSo when Apple decided to allow installing custom kernels on the Macs with M1 processor, we were very happy to try building another Linux port to further our understanding of the hardware platform. As we were creating a model of the processor for our security research product, we were working on the Linux port in parallel.\n\n2. Starting the port\n\nMany components of the M1 are shared with Apple mobile SoCs, which gave us a good running start. But when writing Linux drivers, it became very apparent how non-standard Apple SoCs really are. Our virtual environment is extremely flexible in terms of models it can accommodate; but on the Linux side, the 64-bit ARM world has largely settled on a well-defined set of building blocks and firmware interfaces - nearly none of which were used on the M1.\n\nTo start with, Apple CPUs boot the operating system kernel in a different way. The bootloader, traditionally called iBoot, loads an executable object file in a format called Mach-O, optionally compressed and wrapped in a signed ASN.1 based wrapper format called IMG4. For comparison, normal Linux on 64-bit ARM starts as a flat binary image (optionally compressed and put in one of the few container formats), or a Windows-style \"PE\" executable on UEFI platforms.\n\nBut the real surprises start when further CPU cores are brought up. On other 64-bit ARM systems, this is done by calling the firmware through an interface called PSCI (a few systems use poll-tables, but the firmware is still responsible for them). But on M1, CPU cores start at an address specified by a MMIO register (set to a specific offset within the kernel image, then locked, by the bootloader), and simply begin running the kernel.\n\nIf that wasn't enough, Apple designed their own interrupt controller, the Apple Interrupt Controller (AIC), not compatible with either of the major ARM GIC standards. And not only that: the timer interrupts - normally connected to a regular per-CPU interrupt on ARM - are instead routed to the FIQ, an abstruse architectural feature, seen more frequently in the old 32-bit ARM days. Naturally, Linux kernel did not support delivering any interrupts via the FIQ path, so we had to add that.\n\nWhen you try to get multiple processors in a system to talk to each other, you have to provide a set of inter-processor interrupts (IPIs). On older Apple SoCs, those were handled similarly to IRQs, by executing MMIO accesses to the AIC. But on newer ones, Apple uses a set of processor core registers to dispatch and acknowledge IPIs, and they are - again - delivered as FIQs. So the FIQ support was really quite important. Fortunately, our work on virtual models in our security research product has prepared us for this.\n\nAfter working out a few more hardware quirks, and adding a pre-loader that acts as a wrapper for Linux and provides a trampoline for starting processor cores, we could set a framebuffer and were greeted with the sight of eight penguins representing the eight cores of the M1.\n\n3. Need input!\n\nUnfortunately, since we do not have a UART cable for the M1 Macs, we had to find another way to add a keyboard (and maybe even a mouse). There are fundamentally three paths to do that on the M1 Mac Mini: the built-in USB host in the M1 chip (serves the Thunderbolt/USB ports), the xHCI USB host on PCIe (serves the type A ports) and Bluetooth.\n\nWhile we won't get into the details of Apple Bluetooth, we'll note it uses a non-standard PCIe-based protocol that is supported in our virtualization product, and would require not only bringing up PCIe ports on the M1 chip, but also writing a custom kernel driver for this protocol. That made it seem like the worst choice for getting this done quickly.\n\nThis means we had a choice between bringing up PCIe and using the standard kernel xHCI driver, or bringing up the built-in USB controller. Apple has been using the Synopsys DWC3 dual-role USB controller for a while in their chips, and it has a Linux kernel driver. Unfortunately, Apple is also in the habit of adding custom logic around the controller, so this ended up being a fair bit of work.\n\nBoth the PCIe and the built-in DWC3 USB controller on M1 use IOMMUs, called DARTs. Apple has been refining their DART design in a consistent, evolutionary way, resulting in an excellent, full-featured IOMMU. The last version even has support for sub-page memory protection, rarely seen elsewhere. (We had a blog post on IOMMUs and other similar devices last month.)\n\nTo actually connect the USB port inside the M1 to the USB type-C connectors on the back of the Mac Mini, we had to interact with a chip on I2C (which means GPIO and I2C drivers) which has customized firmware. We've seen the protocol for these while building our virtualized models; nothing is a big surprise if you have a bird's eye view of the system.\n\nAfter a few days of figuring out the details of USB, we were finally able to connect an external USB hub and connect a keyboard, mouse and a Flash drive, opening the possibility for running a normal desktop Linux distribution.\n\nTutorial\n\n1. Download the Ubuntu rootfs\n\nThe first step to booting Linux on your Mac Mini M1 is to download the Ubuntu POC rootfs available here. We used a Raspberry Pi image because it was a live USB boot image, so we only had to make minor modifications to boot it.\n\n2. Extract the image\n\nYou will need a minimum 16G external USB drive. Extract the image by typing:\n\ntar -xjvf ubuntu-20.10-preinstalled-desktop-arm64+raspi.img.bz2\n\nThen, use disk utility to locate the name of the external disk. Finally, copy the image to the USB drive using:\n\nsudo dd if=ubuntu-20.10-preinstalled-desktop-arm64+raspi.img of=/dev/rYOURUSBDISK bs=1m\n\n3. Connect to the Mac\n\nConnect your USB drive to the Mac Mini M1 using a dongle via the USB C port. The USB A ports are not currently supported.\n\n4. Boot into 1TR\n\nTo boot into 1TR (the one true recovery OS), turn off your Mac Mini M1 and then hold Power until you see \"loading options\". Once it loads, you can select the terminal option from the menu bar at the top.\n\n5. Install the Custom Kernel\n\nThe next step is to install the custom kernel. We have made a script that makes this step easier for you. You can run it by typing:\n\n/bin/bash -c \"$(curl -fsSL https://downloads.corellium.info/linuxsetup.sh)\"\n\nThe script will prompt you for your username and password. One you see it print \"Kernel installed\" it's safe to type reboot.\n\n6. Login\n\nOnce you're booted, you'll be prompted for a login. The username is \"pi\" and the password is \"raspberry.\" The root password is also \"raspberry.\"\n\n7. Reverting to MacOS\n\nTo revert to booting MacOS, in 1TR open terminal and type bputil -n\n\n------\n\nIf you're interested in supporting our work on open source projects like these, please consider donating on our behalf to the EFF, who work tirelessly to defend security researchers and protect the digital rights of users and developers. You should also consider supporting the work being done by the folks over at Asahi Linux.\n\nWe'd like to extend a very special thanks to the engineers behind PongoOS for contributing their expertise and collaboration. We're looking forward to updating with a version that uses PongoOS as the bootloader!"
        },
        {
            "authors": [],
            "title": "How to Build an Escrow Product",
            "contents": "Introduction\n\nEscrow is a payment setup where the payer sends funds to a third party rather than directly to the payee. If certain conditions are met, the third party routes the funds to the recipient; if not, the funds get returned to the sender. Escrow is particularly useful for high value payments because the payer is guaranteed their money back if the payee doesn\u2019t meet the conditions.\n\nMany Americans encounter escrow when they buy or sell a home. In the US, the final step in buying a home is referred to as the \u201cclosing process.\u201d While it can vary from state to state, the basic premise is that the buyer delivers the funds to a third party, the seller signs over the deed (or title) to the property, and that third party then transfers the funds to the seller once all the documents are complete. The third party is often referred to as a \u201ctitle and escrow agent.\u201d\n\nThis post takes the sale of a residential home and walks through how to build the flow of funds associated with the escrow process. We hope this guide is useful in demystifying how an escrow company might automate its payment operations using Modern Treasury.\n\nThe User Experience\n\nLet\u2019s say you\u2019re building a title and escrow company in the US that helps clients sell their homes online. The user experience steps might look something like this:\n\n\n\nBillie Buyer commits to buying a house. Billie will make a 20% down payment in cash and will take out a mortgage for the remaining 80%. You assign an escrow bank account to the transaction, collect the two incoming wire transfers from Billie Buyer and the lender, and inform the seller when the funds are in your possession. At that point, Sandy Seller signs over the property. Once you\u2019ve determined all the documentation is completed correctly, you disburse the full purchase amount to Sandy Seller via wire transfer.\n\nThe flow of funds is straightforward: two incoming wire transfers, and one outgoing wire transfer. This can get significantly more complicated if there are multiple sources of funds, or if there are other parties to be paid, such as tax authorities or other settlement agents. But for the sake of clarity, we\u2019ll start with this simple scenario.\n\nPayment Ops Architecture\n\n\n\nTo start, you should decide on the account structure itself. Escrow accounts have specific legal designations at banks and not all banks offer them. In addition, you should decide whether to use a single bank account for multiple transactions or to use virtual accounts.\n\n\n\nVirtual accounts are accounts with unique account numbers within a physical bank account. They are much faster to provision than real bank accounts and guarantee one-to-one reconciliation. For our architecture, using virtual accounts as escrow accounts is ideal because we can create one account per house sale. This helps us segregate funds in our reconciliation, as opposed to mixing the funds of multiple house sales in a single account. Also, single transactions per virtual account create a more manageable audit trail.\n\n\n\n\u200d\n\n\n\nAPI Calls and Timings\n\nWith our payment ops architecture in hand, we\u2019re ready to build.\n\n\n\nStep 1: Assign a Virtual Account to a sale and share numbers with Buyer and Lender\n\nAs soon as you learn of an impending house sale, you create a virtual account and share its details with Billie Buyer. You might also generate an invoice for Billie Buyer to share with their mortgage lender, along with specific instructions as to the amount and breakdown to be included in the wires.\n\n\n\nTo do so, you\u2019d make an Modern Treasury API request like the following:\n\n\n\ncurl --request POST \\\n\n-u ORGANIZATION_ID:API_KEY \\\n\n--url https://app.moderntreasury.com/api/virtual_accounts \\\n\n-H 'Content-Type: application/json' \\\n\n-d '{\n\n\"name\": \"Sale of 123 Main Street\",\n\n\"internal_account_id\": \"c743edb7-4059-496a-94b8-06fc081156fd\",\n\n\"account_details\": [\n\n{\n\n\"account_number\": \"2000001\",\n\n\"account_number_type\": \"other\"\n\n}\n\n]\n\n}'\n\n\u200dThe response includes the unique routing details for the newly created account. You might also want to add additional information about the sale in the account\u2019s metadata, for example: details about the buyer, seller, property in question, expected close date, any unique arrangements, and other information.\n\nStep 2: Create Expected Payments to monitor the wires you expect to receive\n\n\n\nThough not required, it\u2019s useful to create Expected Payments for the two incoming wires you expect to receive. This way, Modern Treasury will notify your system via webhook whether a given payment succeeds within your specified time range, or if it is overdue:\n\n\n\ncurl --request POST \\\n\n-u ORGANIZATION_ID:API_KEY \\\n\n--url https://app.moderntreasury.com/api/expected_payments \\\n\n-H 'Content-Type: application/json' \\\n\n-d '{\n\n\"description\": \"Sale of 123 Main Street (Billie\u2019s Portion)\",\n\n\"internal_account_id\": \"c743edb7-4059-496a-94b8-06fc081156fd\",\n\n\"virtual_account_id\": \"virtual-account-id\",\n\n\"direction\": \"credit\",\n\n\"amount_upper_bound\": 6000000,\n\n\"amount_lower_bound\": 6000000,\n\n\"date_upper_bound\": \"2021-01-15\"\n\n}'\n\n\u200dSome banks will notify you in advance of a payment settling in an account. Modern Treasury captures that notification in the form of an Incoming Payment Detail object. Subscribing to Incoming Payment Detail webhooks will be the first indication from the bank that the wires will settle soon.\n\nStep 3: Disburse funds\n\nOnce the funds have arrived and the documents have been signed, you can disburse the total amount due to Sandy Seller. Let\u2019s say Sandy Seller is getting $300,000 in net proceeds, which will go out via a single wire. Note that the amount is in cents, not dollars, so it is presented as 30000000:\n\n\u200d\n\ncurl --request POST \\\n\n-u ORGANIZATION_ID:API_KEY \\\n\n--url https://app.moderntreasury.com/api/payment_orders \\\n\n-H 'Content-Type: application/json' \\\n\n-d '{\n\n\"description\": \"Sale of 123 Main Street (Sandy\u2019s Payout)\",\n\n\"type\": \"wire\",\n\n\"amount\": 30000000,\n\n\"direction\": \"credit\",\n\n\"currency\": \"USD\",\n\n\"originating_account\": \"virtual-account-id\",\n\n\"receiving_account_id\": \"sandy-external-account-id\",\n\n\"counterparty_id\": \"sandy-counterparty-id\"\n\n}'\n\n\u200dOnce the payment goes through, the account will be debited $300,000 and the transaction will be complete.\n\nLastly, you want to add metadata to the payment for future reference. In addition to metadata tags (eg. key value pairs such as Type: Residential or State: Colorado) you might also attach the PDF for the legal docs, or the CSV calculating the amounts. This will be useful for finance teams that might have questions about this payment in the future.\n\nThat\u2019s it. You have just built a sophisticated payment ops architecture for a house sale with a few API calls.\n\nIn reality, of course, things can be more elaborate. Billie Buyer could have multiple sources of funds for the down payment, as they may be receiving funds from family or from a service like Haus. They may also be working with multiple lenders if they have a second mortgage. And depending on the state and county the house is located in, the escrow company may have to disburse funds for sales tax or other fees.\n\n\n\n\n\nWhat Can Go Wrong?\n\nPayment operations software, such as Modern Treasury, can manage not only the happy path as described above but also edge cases when things go wrong. A common issue could be that a seller could mistype their bank account details, causing the payment to be returned. Modern Treasury supports account verification with providers like Plaid, so both you and the seller can be confident that funds are routed to the right destination.\n\nCustomer service issues are another type of problem that can arise, such as when a lender sends the incorrect amount. In that case, there might be an additional transaction necessary or an explanatory note for future audits. Modern Treasury supports tracking funds using double-entry Ledgers. This way, ops teams can refer to an immutable history of the transaction data, since all money movements are captured and displayed. In the future, accountants and auditors won\u2019t be left wondering about a wire, since contextual information will be tied to the bank statement transaction in question.\n\n\n\nMore?\n\nThis post laid out a simple yet robust foundation for payment ops at an escrow company, leaving plenty of room for future functionality.\n\nFor any specific questions, or to see how Modern Treasury can help make your company\u2019s payment ops simple, scalable, and secure, sign up today.\n\nNote: Modern Treasury empowers teams to make payment operations simple, scalable, and secure. The \u201cGuides\u201d series walks through representative businesses or payment processes and explains step by step how best to go about building them from scratch. \u200d\n\n"
        },
        {
            "authors": [],
            "title": "All Your Chats In One App",
            "contents": "We decided to open source all our bridges to enable you to audit how Beeper connects to each chat network and verify the security of your data. The side effect is that you may self host if you prefer.\n\nThere are two options for self hosting Beeper:\n\nOn-premises, managed by Beeper: run our install script on your amd64 server or 4gb Raspberry Pi and run all bridges locally on your own hardware. This option requires a Beeper subscription. Self-host the full stack: The simplest and free way to self-host the full Matrix+bridges stack is with this Ansible script\n\nNB: the Beeper client app is not open source, but the stack will work with Element (open source Matrix client)."
        },
        {
            "authors": [
                "Tanel Poder"
            ],
            "title": "RAM is the new disk \u2013 and how to measure its performance \u2013 Part 3 \u2013 CPU Instructions & Cycles",
            "contents": "If you haven\u2019t read the previous parts of this series yet, here are the links: [ Part 1 | Part 2 ].\n\nA Refresher\n\nIn the first part of this series I said that RAM access is the slow component of a modern in-memory database engine and for performance you\u2019d want to reduce RAM access as much as possible. Reduced memory traffic thanks to the new columnar data formats is the most important enabler for the awesome In-Memory processing performance and SIMD is just icing on the cake.\n\nIn the second part I also showed how to measure the CPU efficiency of your (Oracle) process using a Linux perf stat command. How well your applications actually utilize your CPU execution units depends on many factors. The biggest factor is your process\u2019es cache efficiency that depends on the CPU cache size and your application\u2019s memory access patterns. Regardless of what the OS CPU accounting tools like top or vmstat may show you, your \u201c100% busy\u201d CPUs may actually spend a significant amount of their cycles internally idle, with a stalled pipeline, waiting for some event (like a memory line arrival from RAM) to happen.\n\nLuckily there are plenty of tools for measuring what\u2019s actually going on inside the CPUs, thanks to modern processors having CPU Performance Counters (CPC) built in to them.\n\nA key derived metric for understanding CPU-efficiency is the IPC (instructions per cycle). Years ago people were actually talking about the inverse metric CPI (cycles per instruction) as on average it took more than one CPU cycle to complete an instruction\u2019s execution (again, due to the abovementioned reasons like memory stalls). However, thanks to today\u2019s superscalar processors with out-of-order execution on a modern CPU\u2019s multiple execution units \u2013 and with large CPU caches \u2013 a well-optimized application can execute multiple instructions per a single CPU cycle, thus it\u2019s more natural to use the IPC (instructions-per-cycle) metric. With IPC, higher is better.\n\nHere\u2019s a trimmed snippet from the previous article, a process that was doing a fully cached full table scan of an Oracle table (stored in plain old row-oriented format):\n\nPerformance counter stats for process id '34783': 27373.819908 task-clock # 0.912 CPUs utilized 86,428,653,040 cycles # 3.157 GHz [33.33%] 32,115,412,877 instructions # 0.37 insns per cycle # 2.39 stalled cycles per insn [40.00%] 76,697,049,420 stalled-cycles-frontend # 88.74% frontend cycles idle [40.00%] 58,627,393,395 stalled-cycles-backend # 67.83% backend cycles idle [40.00%] 256,440,384 cache-references # 9.368 M/sec [26.67%] 222,036,981 cache-misses # 86.584 % of all cache refs [26.66%] 30.000601214 seconds time elapsed\n\nThe IPC of the above task is pretty bad \u2013 the CPU managed to complete only 0.37 instructions per CPU cycle. On average every instruction execution was stalled in the execution pipeline for 2.39 CPU cycles.\n\nNote: Various additional metrics can be used for drilling down into why the CPUs spent so much time stalling (like cache misses & RAM access). I covered the typical perf stat metrics in the part 2 of this series so won\u2019t go in more detail here.\n\nTest Scenarios\n\nThe goal of my experiments was to measure the number CPU-efficiency of different data scanning approaches in Oracle \u2013 on different data storage formats. I focused only on data scanning and filtering, not joins or aggregations. I ensured that everything would be cached in Oracle\u2019s buffer cache or in-memory column store for all test runs \u2013 so disk IO was not a factor here (again, read more about my test environment setup in part 2 of this series).\n\nThe queries I ran were mostly variations of this:\n\nSELECT COUNT(cust_valid) FROM customers_nopart c WHERE cust_id > 0\n\nAlthough I was after testing the full table scanning speeds, I also added two examples of scanning through the entire table\u2019s rows via index range scans. This allows me to show how inefficient index range scans can be when accessing a large part of a table\u2019s rows even when all is cached in memory. Even though you see different WHERE clauses in some of the tests, they all are designed so that they go through all rows of the table (just using different access patterns and code paths).\n\nThe descriptions of test runs should be self-explanatory:\n\n1. INDEX RANGE SCAN BAD CLUSTERING FACTOR\n\nSELECT /*+ MONITOR INDEX(c(cust_postal_code)) */ COUNT(cust_valid) FROM customers_nopart c WHERE cust_postal_code > '0';\n\n2. INDEX RANGE SCAN GOOD CLUSTERING FACTOR\n\nSELECT /*+ MONITOR INDEX(c(cust_id)) */ COUNT(cust_valid) FROM customers_nopart c WHERE cust_id > 0;\n\n3. FULL TABLE SCAN BUFFER CACHE (NO INMEMORY)\n\nSELECT /*+ MONITOR FULL(c) NO_INMEMORY */ COUNT(cust_valid) FROM customers_nopart c WHERE cust_id > 0;\n\n4. FULL TABLE SCAN IN MEMORY WITH WHERE cust_id > 0\n\nSELECT /*+ MONITOR FULL(c) INMEMORY */ COUNT(cust_valid) FROM customers_nopart c WHERE cust_id > 0;\n\n5. FULL TABLE SCAN IN MEMORY WITHOUT WHERE CLAUSE\n\nSELECT /*+ MONITOR FULL(c) INMEMORY */ COUNT(cust_valid) FROM customers_nopart c;\n\n6. FULL TABLE SCAN VIA BUFFER CACHE OF HCC QUERY LOW COLUMNAR-COMPRESSED TABLE\n\nSELECT /*+ MONITOR */ COUNT(cust_valid) FROM customers_nopart_hcc_ql WHERE cust_id > 0\n\nNote how all experiments except the last one are scanning the same physical table just with different options (like index scan or in-memory access path) enabled. The last experiment is against a copy of the same table (same columns, same rows), but just physically formatted in the HCC format (and fully cached in buffer cache).\n\nTest Results: Raw Numbers\n\nIt is not enough to just look into the CPU performance counters of different experiments, they are too low level. For the full picture, we also want to know how much work (like logical IOs etc) the application was doing and how many rows were eventually processed in each case. Also I verified that I did get the exact desired execution plans, access paths and that no physical IOs or other wait events happened using the usual Oracle metrics (see the log below).\n\nHere\u2019s the experiment log file with full performance numbers from SQL Monitoring reports, Snapper and perf stat:\n\nI also put all these numbers (plus some derived values) into a spreadsheet. I\u2019ve pasted a screenshot of the data below for convenience, but you can access the entire spreadsheet with its raw data and charts here (note that the spreadsheet has multiple tabs and configurable pivot charts in it):\n\nRaw perf stat data from the experiments:\n\nNow let\u2019s plot some charts!\n\nTest Results: CPU Instructions\n\nLet\u2019s start from something simple and gradually work our way deeper. I will start from listing the task-clock-ms metric that shows the CPU time usage of the Oracle process in milliseconds for each of my test table scans. This metric comes from the OS-level and not from within the CPU:\n\nCPU time used for scanning the dataset (in milliseconds)\n\nAs I mentioned earlier, I added two index (full) range scan based approaches for comparison. Looks like the index-based \u201cfull table scans\u201d seen in first and second columns are using the most CPU-time as the OS sees it (~120 and close to 40 seconds of CPU respectively).\n\nNow let\u2019s see how many CPU instructions (how much work \u201crequested\u201d from CPU) the Oracle process executed for scanning the same dataset using different access paths and storage formats:\n\nCPU instructions executed for scanning the dataset\n\nWow, the index-based approaches seem to be issuing multiple times more CPU instructions per query execution than any of the full table scans. Whatever loops the Oracle process is executing for processing the index-based query, it runs more of them. Or whatever functions it calls within those loops, the functions are \u201cfatter\u201d. Or both.\n\nLet\u2019s look into an Oracle-level metric session logical reads to see how many buffer gets it is doing:\n\nBuffer gets done for a table scan\n\nWow, using the index with bad clustering factor (1st bar) causes Oracle to do over 60M logical IOs, while the table scans do around 1.6M of logical IOs each. Retrieving all rows of a table via an index range scan is super-inefficient, given that the underlying table size is only 1613824 blocks.\n\nThis inefficiency is due to index range scans having to re-visit the same datablocks multiple times (up to one visit per row, depending on the clustering factor of the index used). This would cause another logical IO and use more CPU cycles for each buffer re-visit, except in cases where Oracle has managed to keep a buffer pinned since last visit. The index range scan with a good clustering factor needs to do much fewer logical IOs as given the more \u201clocal\u201d clustered table access pattern, the re-visited buffers are much more likely found already looked-up and pinned (shown as the buffer is pinned count metric in V$SESSTAT).\n\nKnowing that my test table has 69,642,625 rows in it, I can also derive an average CPU instructions per row processed metric from the total instruction amounts:\n\nCPU instructions executed per row processed\n\nThe same numbers in tabular form:\n\nIndeed there seem to be radical code path differences (that come from underlying data and cache structure differences) that make an index-based lookup use thousands of instructions per row processed, while an in-memory scan with a single predicate used only 102 instructions per row processed on average. The in-memory counting without any predicates didn\u2019t need to execute any data comparison logic in it, so could do its data access and counting with only 43 instructions per row on average.\n\nSo far I\u2019ve shown you some basic stuff. As this article is about studying the full table scan efficiency, I will omit the index-access metrics from further charts. The raw metrics are all available in the raw text file and spreadsheet mentioned above.\n\nHere are again the buffer gets of only the four different full table scan test cases:\n\nAll test cases except the HCC-compressed table scan cause the same amount of buffer gets (~1.6M) as this is the original table\u2019s size in blocks. The HCC table is only slightly smaller \u2013 didn\u2019t get great compression with the query low setting.\n\nNow let\u2019s check the number CPU instructions executed by these test runs:\n\nCPU instructions executed for full table scans\n\nWow, despite the table sizes and number of logical IOs being relatively similar, the amount of machine code the Oracle process executes is wildly different! Remember, all that my query is doing is just scanning and filtering the data followed with a basic COUNT(column) operation \u2013 no additional sorting, joining is done. The in-memory access paths (column 3 & 4) get away with executing much fewer CPU instructions than the regular buffered tables in row-format and HCC format (columns 1 & 2 in the chart).\n\nAll the above shows that not all logical IOs are equal, depending on your workload and execution plans (how many block visits, how many rows extracted per block visit) and underlying storage formats (regular row-format, HCC in buffer cache or compressed columns in In-Memory column store), you may end up doing a different amount of CPU work per row retrieved for your query.\n\nThis was true before the In-Memory option and even more noticeable with the In-Memory option. But more about this in a future article.\n\nTest Results: CPU Cycles\n\nLet\u2019s go deeper. We already looked into how many buffer gets and CPU instructions the process executed for the different test cases. Now let\u2019s look into how much actual CPU time (in form of CPU cycles) these tests consumed. I added the CPU cycles metric to instructions for that:\n\nCPU instructions and cycles used for full table scans\n\nHey, what? How come the regular row-oriented block format table scan (TABLE BUFCACHE) takes over twice more CPU cycles compared to its instructions executed?\n\nAlso, how come all the other table access methods use noticeably less CPU cycles than the number of instructions they\u2019ve executed?\n\nIf you paid attention to this article (and previous ones) you\u2019ll already know why. In the 1st example (TABLE BUFCACHE) the CPU must have been \u201cwaiting\u201d for something a lot, instructions having spent multiple cycles \u201cidle\u201d, stalled in the pipeline, waiting for some event or necessary condition to happen (like a memory line arriving from RAM).\n\nFor example, if you are constantly waiting for the \u201crandom\u201d RAM lines you want to access due to inefficient memory structures for scanning (like Oracle\u2019s row-oriented datablocks), the CPU will be bottlenecked by RAM access. The CPU\u2019s internal execution units, other than the load-store units, would be idle most of the time. The OS top command would still show you 100% utilization of a CPU by your process, but in reality you could squeeze much more out of your CPU if it didn\u2019t have to wait for RAM so much.\n\nIn the other 3 examples above (columns 2-4), apparently there is no serious RAM (or other pipeline-stalling) bottleneck as in all cases we are able to use the multiple execution units of modern superscalar CPUs to complete more than one instruction per CPU cycle. Of course more improvements might be possible, but more about this in a following post.\n\nFor now I\u2019ll conclude this (lengthy) post with one more chart with the fundamental derived metric instructions per cycle (IPC):\n\nThe IPC metric is derived from the previously shown instructions and CPU cycles metrics by a simple division. Higher IPC is better as it means that your CPU execution units are more utilized, it gets more done. However, as IPC is a ratio, you should never look into the IPC value alone, always look into it together with instructions and cycles metrics. It\u2019s better to execute 1 Million instructions with IPC of 0.5 than 1 Billion instructions with an IPC of 3 \u2013 but looking into IPC in isolation doesn\u2019t tell you how much work was actually done. Additionally, you\u2019d want to use your application level metrics that give you an indication of how much application work got done (I used Oracle\u2019s buffer gets and rows processed metrics for this).\n\nLooks like there\u2019s at least 2 more parts left in this series (advanced metrics and a summary), but let\u2019s see how it goes. Sorry for any typos, it\u2019s getting quite late and I\u2019ll fix \u2019em some other day :)\n\nDiscussion & Further reading",
            "published_at": "2015-11-30T00:00:00"
        },
        {
            "authors": [],
            "title": "Are you a robot?",
            "contents": "Why did this happen?\n\nPlease make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy.",
            "published_at": "2021-01-22T00:00:00"
        },
        {
            "authors": [],
            "title": "This is 2021: what\u2019s coming in free/libre software",
            "contents": "There are many reasons to look back at 2020 with a whole range of emotions but I would rather look forward.\n\nThe GIMP team is now focusing on completing version 3.0. It\u2019s too early to say if it will be out in 2021. But unless everything changes dramatically, we\u2019ll see several more 2.99.x releases at the very least.\n\nFeature-wise, not much new is planned, although there\u2019s a new Paint Select tool being worked on by Thomas Manni. And before you ask: no, it\u2019s not AI-based. It\u2019s also designed to do quick binary selection for now, so not very good for selecting strands of hair and suchlike.\n\nAdmittedly, I avoid talking about Glimpse and Glimpse-NX much. I have two major reasons for that.\n\nFirst, every public conversation about the fork of GIMP ends up in someone attacking either the Glimpse or the GIMP team, and that\u2019s unproductive and tiresome. And then the progress isn\u2019t all that interesting so far.\n\nThe fork is just rebranding and no new features or UX fixes (unless removing the bell pepper brush is your idea of finally making it right for everyone), and then Glimpse-NX \u2014 at least for the public eye \u2014 exists only as UI mockups. They did get Bilal Elmoussaoui (GNOME team) to create Rust bindings to GEGL for them last autumn, but that\u2019s all as far as I can tell.\n\nSo the current pace of the project is not very impressive (again, as a GIMP contributor, I\u2019m biased) and I\u2019m not sure how much we are going to see in 2021.\n\nThat said, I think having a whole new image editor based on GEGL would be lovely. I don\u2019t see why Glimpse-NX couldn\u2019t be that project. A proof-of-concept application that would load an image, apply a filter, and export it back sounds feasible. It\u2019s something one could iterate upon. So maybe that\u2019s how they are going to play it.\n\nThe fine folks over at Krita posted a 2020 report where they listed major challenges they will be facing this year: the completion of resources management rewrite that currently blocks v5.0 release, the port to Apple M1, the launching of a new development fund (akin to that of Blender), and more.\n\nThey also have just released version 4.4.2 with mesh gradients, mesh transform tool, new gradient editor etc.\n\nI think it\u2019s safe to say that we might see MyPaint 2.0.2 later this year with some new features and quite a few bugfixes. There haven\u2019t been much groundbreaking development since 2.0.1 released in May 2020.\n\nOn the other hand, there are nice new features available in GitHub forks and not all of them have been turned into pull requests. But some were. E.g. there\u2019s a guy who added a perspective mode with vanishing points and all, and it\u2019s almost ready, although it\u2019s been in the works for almost 5 years now.\n\nThe topic of fullscreen color management implementation in Wayland is back, and it\u2019s a kinda frustrating story. In a nutshell:\n\npeople who are now working on this (Collabora developers) seem to have little experience with color management but they appear to be motivated to hack on the code;\n\nall the while people who have a crapload of experience with color management have had bad experience discussing this before, do not like the approach by the new team, and don\u2019t seem excited to contribute to this new effort (Graeme\u2019s spec proposal is still available).\n\nSo we might end up with an implementation that is not suitable for professional work. At this point, there\u2019s no telling what will happen. Personally, I keep an open mind about it, but the quality of conversations is not good in a way that there is no visible action in response to substantiated criticism. And so the conversation isn\u2019t getting anyone any further.\n\nNow that Inkscape 1.1 alpha is out for everyone to take for a spin, I\u2019m positive we are not so far from the final release. There is a lot packed into the coming update, see the draft of the release notes. I\u2019m really looking forward to whatever they have planned for when 1.1 is out, there is still so much to do!\n\nThere will be another 1.5.x release of Scribus that has some new PDF importing features and includes a lot of small but refining UI changes, likely before April. Another release should follow later in the year.\n\nThey definitely don\u2019t want major UI changes in the main development branch anymore until they release 1.6, and that sounds like wrapping up for the big release to me. Which means it\u2019s only a matter of how many under-the-hood changes still need to happen.\n\nIt\u2019s been a long time since I last looked at Sigil, mostly because I lost my Nook device some years ago and never got to ordering a replacement. Meanwhile, Sigil got better. Like, really better.\n\nThey came up with what seems to be a working solution for dealing with both EPUB2 and EPUB3 in the same application, they updated the UI just enough to be recognizable yet somehow nicer, and then they added some boring yet useful utility features.\n\nI do like where Kevin Hendricks and Doug Massay are taking the project to. Version 1.5.0 will probably be out in the first half of 2021.\n\nThanks to Aur\u00e9lien Pierre, the last few releases of darktable introduced a more articulated division between scene-referred and display-referred workflows, and it looks like more people are contributing to that effort now. There are also some interesting things going on in pull requests:\n\na new color balance RGB filter working in JzAzBz color space, with built-in gamut checking;\n\na new diffusion module to add or remove lens blur, hazing, blooming etc.;\n\na new chromatic aberrations correction module that sits in the right place of the modules chain and produces less halos than defringe;\n\nthere\u2019s also a work-in-progress option to correct lens distortions straight from the RAW Exif for camera vendors that write the distortion in there \u2014 I think I mentioned that one in one of the last weekly recaps.\n\nSomething you can already play with if you build from Git:\n\na new demosaicing method, Ratio Corrected Demosaicing (based on this research), is aimed at reducing pixel overshooting;\n\ncolor calibration module can now create an ad-hoc internal color profile from a color checker.\n\nOnce again, the amount of contributions is getting large enough for maybe more than one major release a year.\n\nMeanwhile, there are currently 63 bugs that need to be fixed for RawTherapee 5.9 to be released. I can\u2019t give you a ETA for it and I\u2019m not sure anybody can. The project is very active, and if you ever came across its fans online, you probably admired how strongly they feel about RT being superior in processing quality and ease of use as compared to some other tools :)\n\nThe Siril team is planning to release version 0.99.8 soon and then hopefully v1.0. They\u2019ve completely rewritten image conversion, revamped memory management, and added astrometry support (6 catalogs supported currently).\n\nThe Synfig team is completing the migration from autotools to CMake, and their next step would be to make it possible to build the program with MS Visual Studio. The expectation is that more contributors would join then thanks to a lower threshold. After that, the rendering code really needs attention.\n\nIn 2020, VGC by Boris Dalstein was \u201cpre-incubated\u201d at the BIC de Montpellier startup incubator and additionally obtained a public grant from CNC-RIAM. This means Boris will be able to hire another developer to work on the project.\n\nThe situation with enve is\u2026 Well, complicated, I guess. In summer 2020, Maurycy Liebner announced that the development is on hold due to health issues. In December, he started pushing some commits to the public repository on GitHub again, not at the old rate though. So it\u2019s unclear if he\u2019s resuming the work.\n\nI think it would help to have some sort of a framework for more people to contribute, even basic things like a roadmap and info for newly arrived developers.\n\nNatron is one of the projects that I find hard to speculate about. The only active developer right now is Ole-Andr\u00e9 Rodlie, and his most recent work currently lives in his own GitHub fork. He says he\u2019ll soon start making pull requests to the upstream GitHub project (which, let\u2019s be frank, is a bit like sending PRs to yourself).\n\nThere could be a few visual tweaks coming next but unlikely at the scale of a UI redesign that is being discussed on Pixls since May 2020.\n\nHowever, the new website for the project, designed by the same contributor, has good chances to go live soon. How much that will help attracting new developers is hard to say. Probably not much, as projects like Natron really need full-time involvement (which I discussed at length in 2018, and not much changed since then).\n\nThis is a rather sad turn of events, as it leaves us Blender as the only working free/libre node-based compositing solution (unless you count upcoming Olive 0.2 in). Ramen is long gone, and ButtleOFX developers gave up around 2017. And while I love Blender to pieces, the idea that it is the only option is somehow worrying.\n\nEverything Nodes, new asset browser, Vulkan support in EEVEE, new character animation tools and more \u2014 all work planned for Blender in 2021 sounds great. There isn\u2019t really much to add here, apart from mentioning that some of these things will be possible thanks to corporate funding.\n\nVersion 2.92 is expected in late February, with features like easily drawing 3D primitives, a whole bunch of sculpting improvements, tracing image sequences to Grease Pencil strokes, cryptomatte support in EEVEE, and so much more.\n\nI\u2019d be a damn fool if I left out Dust3D. Jeremy HU did a lot of experimenting in 2020, and this year, his plan is to merge a lot of that into the main program. He\u2019ll do more experimenting with his quad remesher first though.\n\nOne major roadblock is writing his own code instead of CGAL libraries that are license-incompatible with Dust3D (GPL in CGAL vs MIT in Dust3D).\n\nBut yes, you should generally be looking forward to a new version of the program with better UI, great quads mesh generation, easy creature animation generation, and without complex third party dependencies.\n\nLubos Lenco is working towards releasing ArmorPaint 1.0 in 2021. He\u2019s also planning an iPadOS release and improved Android builds. Apart from that, the usual stuff \u2014 fixing bugs, adding requested features.\n\nThere is never a release schedule for FreeCAD but everything is pointing at v0.19 release some time in 2021. In fact, Yorik van Havre talked about that in his latest project update. The release notes draft page is a huge beast to tame. Essentially, you are looking at:\n\ntons of improvements in pretty much all workbenches, especially Arch, BIM, and TechDraw;\n\nadd-on management;\n\nvarious UI updates;\n\nlots of bugfixes.\n\nWhich is great as people do all sorts of wonderful tinkery things with FreeCAD.\n\nBlenderBIM is one of the most exciting projects lately. Dion Moult currently has two major tasks to accomplish:\n\nImproving drawings generation to bring them on par with the quality and complexity that existing mature proprietary CAD and BIM applications provide. Add incremental editing to vastly decrease the amount of roundtripping errors, some of them being the result of Blender\u2019s data model not even designed to support 100% of the IFC specification.\n\nDion is also extremely active in the IfcOpenShell project that is critical for BlenderBIM. As a matter of fact, he has already made almost as many commits to the source code repository as Thomas Krijnen since its inception ca. 10 years ago.\n\nSpeaking of which, if you are into AEC, OSarch.org now has a blog.\n\nThe OpenSCAD team seems to be planning a new release, the latest release candidate was made available just a week ago.\n\nUnfortunately it doesn\u2019t look like LibreCAD v3 is going to happen this year. The project is still seeing some action but just not at the scale to sing \u2018release time\u2019 in an angel voice.\n\nI do expect Andrew Mustun to keep producing QCAD Community Edition releases though. Whether we like it or not, his approach to licensing appears to be working. At the very least, he\u2019s in a position where he can hack on it on a regular basis.\n\nAnd to quit this chapter on a really positive note, SolveSpace v3.0 is just around the corner. They sent out a call for translations update several days ago, so it\u2019s really happening!\n\nMoreover, Reini Urban recently started working on replacing libdxfrw with LibreDWG for DWG support in SolveSpace. This is going to be really interesting as LibreDWG is now mature enough, and as of the latest release, its API allows constructing DWG data inside CAD applications (not just export).\n\nThe Kdenlive team is likely to participate at GSoC this year, so probably expect more interesting things to happen. They will also follow MLT development, and beyond upcoming v7, the TODO list now mentions 10-bit and HDR support (in the \u2018Future\u2019 section).\n\nOlive development is going to be interesting. I\u2019d say expect v0.2 with all cutting tools working and maybe get used to the idea of using nightly builds for color grading tools (roughly the plan for v0.3). Matt recently published a video (see below) that he cut entirely with Olive from the main development branch, so it\u2019s the real deal.\n\nBlender VSE is already undergoing a major revamp. The focus is on several key areas: performance, better tools and UX, better media management, better i/o workflow (think clip preview and in/out marks for 3-point editing). You can get a better idea by visiting this overview task page.\n\nOne major nitpick I have there is that the plans do not seem to directly mention the handling of 10-bit data. Which, after closing the relevant bug report in 2013 as invalid (sic!), seems odd.\n\nI don\u2019t really track other projects all that actively. What I can say about Pitivi though is that you shouldn\u2019t judge the activity of its developers by the commit log. All the really interesting stuff is happening in the Merge Requests section.\n\nThe development of Ardour 7.0 is going strong but we might still see another 6.x release with fixes and small improvements before the major update happens. Please do note that there are no actual public claims v7.0 is going to be released this year though.\n\nZrythm is getting really close to v1.0 release. It\u2019s still at the stage where users are, like, OMG, I think I can complete a song with this! But look, every other DAW started there. And the only other EDM-centered free/libre tool on Linux is LMMS. Which is well on its way to 1.3.0 release later this year, by the way.\n\nI like a lot what\u2019s going on with MuseScore too. They\u2019ve just released v3.6 with score rendering improvements and a whole new font for score engraving. The path to v4.0 is not an easy one, they are pretty much rewriting the application because it was difficult to expand the old code base. So please don\u2019t make me predict the ETA of that major update!\n\nIn 2020, there was a surge of VST3 adoption in free/libre DAWs and sequencers, and judging by what I hear from some plugin developers, they are eager to support it as well. So we might see more of that this year.\n\n2021 could also be the year when LSP replaces Calf Studio Gear as universally suggested audio plugins suite on Linux too. Its not like Calf was dead. It\u2019s a combination of several factors like slow development and unsolved DSP issues. And then, speaking from experience, there\u2019s nothing like your eardrum being pierced by output from Calf Reverb to make you look elsewhere.\n\nFinally, the amount of available LV2 plugins is getting closer to the point where there\u2019s simply no time to try every new thing. Aren\u2019t we the lucky ones?\n\nThe hero image is copyright by Emi Martinez, and it\u2019s coming from his animated short film made with Blender. Check it out!\n\nWriting this article took close to 20 hours including research, testing, talking to developers, bug reporting etc.\n\nIf you enjoy the work I do, you can support me on Patreon or make a one-time donation",
            "published_at": "2021-01-21T21:53:22+05:30"
        },
        {
            "authors": [
                "John Timmer"
            ],
            "title": "New metamaterial merges magnetic memory and physical changes",
            "contents": "For applications like robotics, there's usually a clear division of labor between the processors that control the robot's body and the actuators that actually control the physical changes of that body. But a new paper being released today blurs the lines between the two, using a magnetic switch in a way that both stores a bit representing the hardware's state and alters the physical conformation of the hardware. In essence, it merges memory and physical changes.\n\nThis particular implementation doesn't seem to be especially useful\u2014it's much too big to be a practical form of memory, and the physical changes are fairly limited. But the concept is intriguing, and it's possible that someone more adept at creative thinking can find ways of modifying the concept to create a useful device.\n\nA magnetic metamaterial?\n\nA metamaterial is generally defined as a material that is structured so that it has properties that aren't found in bulk mixes of its raw materials. A broad reading of that definition, however, would mean that a car is a metamaterial, which makes the definition near meaningless. The researchers behind the new device, based at Switzerland's \u00c9cole Polytechnique Fede\u0155ale de Lausanne, claim their creation is a metamaterial, but it's fairly large (roughly a cube three centimeters on a side) and has a number of distinct parts. I'd tend to call that a device rather than a material and will use that terminology here.\n\nSo what is the device? The part that changes its configuration is a platform supported by a set of four legs that are bent inward (v and iii in the image below). The two different states of the system are read out by registering the amount of force needed to push the platform down. The force needed is raised by pushing a wedge (iv) between the bend of the legs, forcing them outward and dropped again by sliding the wedge back out. The wedge is indirectly attached to a flexible base that pops between two stable states (i), a bit like the tops of twist-off jar lids that pop out to indicate the jar has been opened.\n\nThe whole thing is controlled by part ii, which links the flexible base to the wedges. In this device, it's made of a polymer that has had magnetic particles embedded in it. This allows the state of the device to be controlled using an external magnetic field. Pull the central magnetic component up and the base pops upward, driving the wedge between the legs and raising the force needed to deform the device. Push the magnetic hardware back down and the wedge drops out of the way, and the force required to push the platform down drops with it.\n\nAdvertisement\n\nOn the grid\n\nThe researchers built a six-by-six array of these devices and showed that the devices could be individually addressed using other magnetic devices positioned above and below it. The process isn't quick; it takes nearly a second for the device to change state, and the system has to cool down for four seconds after each of these. But the devices could be switched back and forth over 1,000 times without losing performance.\n\nThe six-by-six array allowed 37 different combinations of on/off states in individual devices, and the researchers tested the force required to flatten the platform in each of these states. As expected, that force varied based on the configuration, showing that the devices could collectively change the properties of the hardware they were part of.\n\nBut in addition to measuring the state of the devices via force, however, the researchers found they could also read their magnetic state, much like bits on a hard drive. Because of the change in the location of the magnetic part of the device, they found a five-fold difference in its magnetic properties when measured at the hardware's surface.\n\nIn total, the grid of devices enabled three distinct measurements to be made: an overall change in the deformation properties of the surface they supported, a difference in the force required to deform individual elements, and a change in the magnetic properties of individual elements.\n\nCollectively, all of that is pretty neat. It's not, however, obviously useful. That's in part due to the device's size, but it's also in part because there's no obvious immediate need for a surface with fine-tuned compressibility or to read the state of the parts magnetically instead of simply remembering how they were set. But the researchers say that there should be a variety of means to shrink the device down. And they argue that the concept could be extended in a lot of ways now that it has been demonstrated. We'll have to reserve judgement on utility until we see what the rest of the research community does with the concept.\n\nNature, 2021. DOI: 10.1038/s41586-020-03123-5 (About DOIs)."
        },
        {
            "authors": [
                "January"
            ],
            "title": "Twitter refused to remove child porn because it didn\u2019t \u2018violate policies\u2019: lawsuit",
            "contents": "Twitter refused to take down widely shared pornographic images and videos of a teenage sex trafficking victim because an investigation \u201cdidn\u2019t find a violation\u201d of the company\u2019s \u201cpolicies,\u201d a scathing lawsuit alleges.\n\nThe federal suit, filed Wednesday by the victim and his mother in the Northern District of California, alleges Twitter made money off the clips, which showed a 13-year-old engaged in sex acts and are a form of child sexual abuse material, or child porn, the suit states.\n\nThe teen \u2014 who is now 17 and lives in Florida \u2014 is identified only as John Doe and was between 13 and 14 years old when sex traffickers, posing as a 16-year-old female classmate, started chatting with him on Snapchat, the suit alleges.\n\nDoe and the traffickers allegedly exchanged nude photos before the conversation turned to blackmail: If the teen didn\u2019t share more sexually graphic photos and videos, the explicit material he\u2019d already sent would be shared with his \u201cparents, coach, pastor\u201d and others, the suit states.\n\nDoe, acting under duress, initially complied and sent videos of himself performing sex acts and was also told to include another child in his videos, which he did, the suit claims.\n\nEventually, Doe blocked the traffickers and they stopped harassing him, but at some point in 2019, the videos surfaced on Twitter under two accounts that were known to share child sexual abuse material, court papers allege.\n\nOver the next month, the videos would be reported to Twitter at least three times \u2014 first on Dec. 25, 2019 \u2014 but the tech giant failed to do anything about it until a federal law enforcement officer got involved, the suit states.\n\nDoe became aware of the tweets in January 2020 because they\u2019d been viewed widely by his classmates, which subjected him to \u201cteasing, harassment, vicious bullying\u201d and led him to become \u201csuicidal,\u201d court records show.\n\nWhile Doe\u2019s parents contacted the school and made police reports, he filed a complaint with Twitter, saying there were two tweets depicting child pornography of himself and they needed to be removed because they were illegal, harmful and were in violation of the site\u2019s policies.\n\nA support agent followed up and asked for a copy of Doe\u2019s ID so they could prove it was him and after the teen complied, there was no response for a week, the family claims.\n\nAround the same time, Doe\u2019s mother filed two complaints to Twitter reporting the same material and for a week, she also received no response, the suit states.\n\nFinally on Jan. 28, Twitter replied to Doe and said they wouldn\u2019t be taking down the material, which had already racked up over 167,000 views and 2,223 retweets, the suit states.\n\n\u201cThanks for reaching out. We\u2019ve reviewed the content, and didn\u2019t find a violation of our policies, so no action will be taken at this time,\u201d the response reads, according to the lawsuit.\n\n\u201cIf you believe there\u2019s a potential copyright infringement, please start a new report. If the content is hosted on a third-party website, you\u2019ll need to contact that website\u2019s support team to report it. Your safety is the most important thing, and if you believe you are in danger, we encourage you to contact your local authorities.\u201d\n\nIn his response, published in the complaint, Doe appeared shocked.\n\n\u201cWhat do you mean you don\u2019t see a problem? We both are minors right now and were minors at the time these videos were taken. We both were 13 years of age. We were baited, harassed, and threatened to take these videos that are now being posted without our permission. We did not authorize these videos AT ALL and they need to be taken down,\u201d the teen wrote back to Twitter.\n\nHe even included his case number from a local law enforcement agency, but still the tech giant allegedly ignored him and refused to do anything about the illegal child sexual abuse material \u2014 as it continued to rack up more and more views.\n\nTwo days later, Doe\u2019s mom was connected with an agent from the Department of Homeland Security through a mutual contact who successfully had the videos removed on Jan. 30, the suit states.\n\n\u201cOnly after this take-down demand from a federal agent did Twitter suspend the user accounts that were distributing the CSAM and report the CSAM to the National Center on Missing and Exploited Children,\u201d states the suit, filed by the National Center on Sexual Exploitation and two law firms.\n\n\u201cThis is directly in contrast to what their automated reply message and User Agreement state they will do to protect children.\u201d\n\nThe disturbing lawsuit goes on to allege Twitter knowingly hosts creeps who use the platform to exchange child porn material and profits from it by including ads interspersed between tweets advertising or requesting the material.\n\nEarly Thursday, Twitter declined comment to The Post but later in the day, reversed course and sent a statement by email.\n\n\u201cTwitter has zero-tolerance for any material that features or promotes child sexual exploitation. We aggressively fight online child sexual abuse and have heavily invested in technology and tools to enforce our policy, a Twitter spokesperson wrote.\n\n\u201cOur dedicated teams work to stay ahead of bad-faith actors and to ensure we\u2019re doing everything we can to remove content, facilitate investigations, and protect minors from harm \u2014 both on and offline.\u201d",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [],
            "title": "Steganography: hiding messages in images",
            "contents": "Hiding messages in images: steganography with Python and Repl.it\n\nIn this tutorial, we'll build a steganography tool in Python. Steganography is the practice of hiding information within other data. Unlike encryption, where the goal is to secure the contents of communication between two parties, steganography aims to obscure the fact that the parties are communicating at all.\n\nOur tool will enable the user to hide secret text within a normal-looking .png image file. The receiver of the image will use the same tool to reveal the hidden message.\n\nWe'll use Python to build the tool. The most popular Python image processing libraries are Pillow and OpenCV, but these are heavy libraries with many dependencies. We'll avoid these and instead use the lightweight PyPNG library which is written in pure Python, and therefore easier to run on various platforms.\n\nA quick background on steganography\n\nLet's imagine three people: Alice, Bob and Eve. Alice wants to send a private message to Bob, while Eve wants to intercept this message. While modern-day encryption can help Alice and Bob ensure that Eve doesn't know the contents of their message, Eve can possibly still deduce interesting information just from knowing that Alice and Bob are communicating at all, and how frequently they communicate.\n\nTo obscure the communication channel completely, Alice and Bob can exploit the fact that hundreds of millions of photos are uploaded and shared across the internet daily. Instead of communicating directly, Alice can leave her message hidden in an image at a pre-agreed location and Bob can access this message. From Eve's perspective, there is now no direct communication between the two.\n\nA single image is made up of millions of pixels. While many formats exist, a pixel is most simply represented by a group of three numbers between 0 and 255, one number each for the red, blue, and green values of that pixel. Using this Red-Green-Blue scheme we can represent any colour in the RGB color model.\n\nDigital text, like images, is also represented internally by numbers, so the differences between a text file and an image file are not as large as you might assume. Any digital data can be represented as a binary string, a bunch of 1s and 0s, and we can make tiny modifications to an image to encode a binary string within it. As an example, consider the following:\n\nimage = [ , , ]\n\nThis is a representation of an image with three pixels: one red, one green, and one blue. If we encode this as an image and open it in an image viewer, we'll see the three pixel image, but if we read this data with Python, it is simply a list of tuples, each containing three integers.\n\nWe could also look at each value making up each pixel and calculate whether it is odd or even. We could encode odd numbers as 1 and even values as 0 . This would give us the binary string \"100 010 001\" (as the 255 values are odd and the 0s are even).\n\nIf we made a small modification to the image as follows:\n\nimage = [ , , ]\n\nThe image would look almost identical in any image viewer (we have just added or subtracted a minuscule amount of color from some values), but the binary string -- using our odd/even method -- would look completely different: \"011 111 100\".\n\nUsing this technique but extending it over an entire image (millions of pixels), we can hide a large amount of text data in any image.\n\nCreating the project on Repl.it\n\nIf you were serious about keeping your messages as secret as possible, you'd want to do all of these steps on an offline computer that you fully control. As a learning exercise though, we'll set the project up on repl.it. Navigate to their site and sign up for an account if you don't have one.\n\nCreate a new project, choosing \"Python\" as the language, and give your project a name.\n\nThe first piece we need to build is a function to encode any text message as a binary string.\n\nEncoding a text message as a binary string\n\nOpen the main.py file and add the following code\n\nimport base64 def encode_message_as_bytestring( ): b64 = .encode( \"utf8\" ) bytes_ = base64.encodebytes(b64) bytestring = \"\" .jo [ \"{:08b}\" . ( ) for bytes_]) bytestring\n\nThis first encodes our text as base64 and then as a binary string. You can add some print statements to see how the message is transformed in the different steps, as shown below.\n\nThe base64 step is not strictly necessary, but it is useful as any file or data can be encoded as base64. This opens our project up to future extensions such as hiding other kinds of files within image files instead of just text strings.\n\nAdding an 'end of message' delimeter\n\nWe'll assume that our message will always 'fit' in our image. We can fit three binary digits per pixel (one for each of the RGB values), so our resulting binary string should be shorter than the the number of pixels in the image multiplied by three.\n\nWe'll also need to know when the message ends. The message will only be encoded in the beginning of the image file, but if we don't know how long the message is, we'll keep looking at normal pixels and trying to encode them as text data. Let's add an \"end of string\" delimiter to the end of our message: this should be something that wouldn't appear half way through our actual message by chance. We'll use the binary representation of '!ENDOFMESSAGE!' for this.\n\nModify your function to look as follows, which adds this delimeter at the end.\n\nimport base64 ENDOFMESSAGE = \"0100100101010101010101100100111101010010010001010011100101000111010101000101010101010110010101000101010100110000010001100100100001010010010100110100010100111101\" def encode_message_as_bytestring(message): b64 = message.encode( \"utf8\" ) bytes_ = base64.encodebytes(b64) bytestring = \"\" . join([\"{:08b}\".format(x) for x in bytes_]) bytestring += ENDOFMESSAGE return bytestring\n\nNow that we can handle some basic text encoding, let's look at images.\n\nGetting pixels from an image\n\nFind a PNG image somewhere - either one you've taken yourself or from a site like unsplash. You can use any online JPG to PNG converter if you only have .jpg files available.\n\nUpload your PNG file by clicking on the three dot menu in the repl sidebar, in the top right corner of the files pane to the left, and selecting upload file or by simply dragging and dropping your file within the files pane.\n\nWe're going to write a function that extracts the raw pixel data from this image file. Add an import to the top of the file.\n\nimport png\n\nAnd then add a new function to the bottom of main.py :\n\ndef get_pixels_from_image (fname) : img = png.Reader(fname).read() pixels = img[ 2 ] return pixels\n\nThe read() method returns a 4\u2011tuple consisting of:\n\nwidth: Width of PNG image in pixels\n\nheight: Height of PNG image in pixels\n\nrows: A sequence or iterator for the row data\n\ninfo: An info dictionary containing some meta data\n\nWe are primarily interested in the third item, \"rows\", which is an iterator containing all the pixels of the image, row by row. If you're not familiar with Python generators take a look at this guide, but they are essentially memory-efficient lists.\n\nEncoding the image with the message\n\nNow that we have the encoded message and pixels of the image ready we can combine them to form our secret encoded image.\n\nAdd the following function to the bottom of the main.py file. This function takes in the outputs from the previous functions (our raw pixels and our message encoded as a binary string), and combines them.\n\ndef encode_pixels_with_message ( pixels, bytestring ): '''modifies pixels to encode the contents from bytestring''' enc_pixels = [] string_i = 0 for row in pixels: enc_row = [] for i, char in enumerate (row): if string_i >= len (bytestring): pixel = row[i] else : if row[i] % 2 != int (bytestring[string_i]): if row[i] == 0 : pixel = 1 else : pixel = row[i] - 1 else : pixel = row[i] enc_row.append(pixel) string_i += 1 enc_pixels.append(enc_row) return enc_pixels\n\nThis is the most complicated part of our project, but most of the code is there to handle edge cases. The important insight is that we want to control whether each pixel has an odd value (representing a 1 in our binary string) or an even one (to represent a 0). By chance, half of the pixel values will already have the correct value.\n\nWe simply loop through the binary string and the pixel and 'bump' each value that isn't correct by one. That is, we subtract 1 from the value if we need to change it from odd to even or vice versa. We don't want any negative numbers, so if we need to change any of the 0 values, we add 1 instead.\n\nWriting our modified pixels back to an image\n\nWe now have all the image data, including the encoded message but it is still just a list of pixels. Let's add a function that will compile our pixels back into a PNG image.\n\nAdd the following function to the bottom of the main.py file.\n\ndef write _pixels_to_image( pixels , fname ) : png.from _array( pixels , 'RGB') .save(fname)\n\nThe above function takes the array pixels and uses the png module to write these to a brand new .png file.\n\nPlay around with these functions to make sure you understand how they work. Before we write some wrapper code to actually use these, we're going to do everything backwards so that we can also extract hidden messages from previously encoded PNG files.\n\nDecoding messages from image files\n\nFirst we need a function that can turn a binary string back into readable text. As before, we'll go via base64 for better compatability. Add the following function to the bottom of the main.py file.\n\ndef decode _message_from_bytestring( bytestring ) : bytestring = bytestring.split(ENDOFMESSAGE) [ 0 ] message = int (bytestring, 2 ). to _bytes( len ( bytestring ) message = base64.decodebytes(message).decode( \"utf8\" ) return message\n\nRemember how we added a special ENDOFMESSAGE delimiter above? Here we first split our string on that so we don't look for text in random data (pixels from the unmodified part of the image) and then go backwards through our encoding pipe: first to base64 and then to text.\n\nWe also need a way to extract the bytestring from an image. Add the following function to main.py to do this.\n\ndef decode_pixels( pixels ): bytestring = [] for row in pixels : for c in row: bytestring. append ( str (c % 2 )) bytestring = '' . join (bytestring) message = decode_message_from_bytestring(bytestring) return message\n\nOnce again, this is just the reverse of what we did before. We grab the remainder of each value to get 1 for each odd value and 0 for each even one and keep them in a string. We then call our decode function to get the plaintext.\n\nThat's it for our encoding and decoding functions; next we'll put everything together in our main() function.\n\nAdding a command line wrapper script\n\nAt this point, we could create a web application with a UI for people to add text to their images. Given the fact that people who want to do steganography probably won't trust a web application with their data, we'll rather create a command line application that people can run on their own machines.\n\nAdd the following to the top of your main.py file, right below the imports.\n\nPROMPT = \"\"\" Welcome to basic steganography. Please choose: 1. To encode a message into an image 2. To decode an image into a message q. To exit \"\"\"\n\nNow let's write the main() function that puts it all together. Add the following to the end of the main.py file.\n\ndef main () : print(PROMPT) user_inp = \"\" while user_inp not in ( \"1\" , \"2\" , \"q\" ): user_inp = input( \"Your choice: \" ) if user_inp == \"1\" : in_image = input( \"Please enter filename of existing PNG image: \" ) in_message = input( \"Please enter the message to encode: \" ) print( \"-ENCODING-\" ) pixels = get _pixels_from_image( in_image ) bytestring = encode _message_as_bytestring( in_message ) epixels = encode _pixels_with_message( pixels , bytestring ) write _pixels_to_image( epixels , in_image + \"-enc.png\" ) elif user_inp == \"2\" : in_image = input( \"Please enter the filename of an existing PNG image: \" ) print( \"-DECODING-\" ) pixels = get _pixels_from_image( in_image ) print(decode _pixels( pixels ) ) if __name__ == \"__main__\" : main ()\n\nThe main() function above creates a prompt flow for the user to interact with the program. Depending on the input from the user, the program will call the relevant functions in order to either encode or decode a message. We also included a q for the user to close the program.\n\nWhere next?\n\nIf you have followed along you'll have your own repl to expand; if not you can fork our repl and work from there or test it out below."
        },
        {
            "authors": [
                "Tim Fitzsimons",
                "Tim Fitzsimons Is A Reporter For Nbc News."
            ],
            "title": "FBI ups reward to $75,000 for suspect who placed pipe bombs during Capitol riot",
            "contents": "Federal investigators are increasing the public reward for information that leads to the \"location, arrest & conviction of the person(s) responsible for the pipe bombs found in DC on Jan. 6\"\n\nThe reward, originally set at $50,000 one day after the pipe bombs were first discovered, is now $75,000, according to a tweet published Thursday by the FBI's Washington Field Office.\n\nADDITIONAL REWARD: @ATFWashington & #FBIWFO are now offering a reward of up to $75K for info about the person(s) responsible for the placement of suspected pipe bombs in DC on January 6th. Call 1800CALLFBI with info or submit to https://t.co/NNj84wkNJP. pic.twitter.com/f77EHkVNND \u2014 FBI Washington Field (@FBIWFO) January 21, 2021\n\nAccording to the notice, both the FBI and the Bureau of Alcohol, Tobacco, Firearms and Explosives are searching for at least one suspect who is responsible for placing suspected pipe bombs in Washington, D.C., on Jan. 6.\n\nBetween 1:00 and 1:15 p.m. on that date, law enforcement agencies received reports of two suspected pipe bombs, the notice said.\n\nThe two improvised explosive devices, which were defused by experts and did not detonate, were found outside the headquarters of the Republican National Committee and Democratic National Committee during the mayhem of the riot at the U.S. Capitol Building.",
            "published_at": "2021-01-21T17:34:49+00:00"
        },
        {
            "authors": [
                "Lisa Visentin"
            ],
            "title": "Google threatens to disable search in Australia if media code becomes law",
            "contents": "Google says it will stop making its search function available in Australia if Parliament passes the Morrison government's proposed laws to force it and Facebook to pay news businesses for their journalism.\n\nGoogle Australia managing director Mel Silva told a Senate hearing on Friday the proposed news media bargaining code remained \"unworkable\", and the company was prepared to exit the Australian market.\n\nGoogle Australia managing director Mel Silva told a Senate hearing the company would disable Google Search in Australia if the parliament proceeded to pass the Morrison government's proposed media bargaining code. Credit:Louie Douvis\n\n\"If this version of the code were to become law, it would give us no real choice but to stop making Google Search available in Australia,\" Ms Silva told the inquiry.\n\nIt is the first time the digital giant has made the threat to disable its primary search function to all Australians in its response to the proposed laws.",
            "published_at": "2021-01-22T00:00:00"
        },
        {
            "authors": [
                "Martin Ueding"
            ],
            "title": "C++ Anti-patterns",
            "contents": "This is a list of C++ anti-patterns that I have seen in various codes.\n\nPreprocessor\n\nLowercase preprocessor constants\n\nPreprocessor constant should always be in capital letters. Otherwise you will get the weirdest of bugs that take hours to days to track down. Say there is some feature that you want to switch on/off during compile time. One way to do it would be using #ifdef like this:\n\nint make_query ( Query const & q ) { // Do some query stuff here. #ifdef VERIFY_RESULT // Perform some verification. #endif // Some other tasks and return statement. }\n\nWhen compiling, you can give the option -DVERIFY_RESULT and the preprocessor will put in the verification code. In one project I have seen this same thing, but it was with #ifdef verify_result . That is legal C++ and also works with the -Dverify_result command line option to the compiler.\n\nOne member of the project who just recently joined and did not know about the many compilation flags just created a new function bool verify_result(Result const &result) . The compiler did not say that the name was not in use. But rather, the compiler saw bool (Result const &result) when the option was given because that flag did not get any value. GCC complained that it did not expect the ( there.\n\nI do not know how long they tried to track that down, but it was long enough to be a real pain and certainly a big waste of time. This would not have happened if the preprocessor constants had been uppercase all the time.\n\nPreprocessor over scopes\n\nAnother project make a lot of use of preprocessor flags to change the way the code works. This in itself is not a problem. However, there are pieces of code like this:\n\n#ifdef FLAG } #endif\n\nWith multiple such flags, I do not see how you can sensible reason about this code.\n\nIncluding source files\n\nThis was in one of the header files:\n\n#include \"other.cpp\"\n\nIncluding source files in header files usually is just wrong. One edge case are unity builds or template specializations. Said project just did it for no apparent reason.\n\nUnnamed #if 0 branches\n\nThe pair #if 0 and #endif can be used to quickly comment out large chunks of code. Contrary to /* and */ , they have the advantage that they can be nested and also serve as a sort of super-comment.\n\nWhen there are multiple of those blocks around, it becomes a bit hard to understand why some are enabled and others are not enabled:\n\n#if 0 // ... #endif // ... #if 1 // ... #endif // ... #if 0 // ... #endif\n\nIs the middle one the negation of the first one? Is the first and third one actually the same; do they have to be enabled at the same time?\n\nHere I like it better to introduce some named constant for that. The above could look like the following:\n\n#ifdef POLISH_WIDGETS // ... #endif // ... #ifndef POLISH_WIDGETS // ... #endif // ... #ifdef POLISH_WIDGETS // ... #endif\n\nThen it would be easy to understand what is going on. If somebody wanted to change the code, a single #define POLISH_WIDGETS would be enough and everything would be consistent and readable.\n\nHeader files\n\nStandalone header files\n\nA header file should include all the other headers it needs to be compiled. Something I have seen a couple times is this here:\n\nf.hpp :\n\n#pragma once void f ( MyClass x );\n\nNotice that the type MyClass is not defined here. It is defined in this header, but that is not included in f.hpp at all.\n\nIt is defined in myclass.hpp :\n\n#pragma once class MyClass { public : int member ; };\n\nThe implementation of the function f is in f.cpp . There the header for MyClass is included:\n\n#include \"myclass.hpp\" #include \"f.hpp\" void f ( MyClass x ) { x . member = 0 ; }\n\nAnd that actually compiles because the compiler only works on the .cpp files. And given the ordering of the #include statements, the C++ compiler sees this:\n\nclass MyClass { public : int member ; }; void f ( MyClass x ); void f ( MyClass x ) { x . member = 0 ; }\n\nThis will work as long as that header f.hpp is always included after myclass.hpp . It will start to fail when you use f.hpp somewhere else. There should be an #include \"myclass.hpp\" in f.hpp .\n\nWrong order of header files\n\nThe order of the header files in f.cpp in the above example made it possible to hide the missing include inside f.hpp such that it still compiles. One can make this fail earlier by having the following order of includes in the .cpp files:\n\n\"Own\" header file, for X.cpp that would be X.hpp Project header files Third-party library headers Standard library Headers\n\nThis way, missing include statements in the header will become directly apparent. You might find a bug in a third-party library this way.\n\nCyclic dependencies\n\nI saw a case where the file A.hpp provides the classes A1 and A2 . The file B.hpp provided B1 . For some reasons, the dependencies were A1 \u2192 B1 \u2192 A2 . That is not directly a cyclic dependency of the types but of the header files. This was \"solved\" in the project like this in file A.hpp :\n\nclass A1 { ... }; #include \"B.hpp\" class A2 { ... };\n\nSure, that compiled just fine. But again, header B.hpp is not a standalone header and can only be used in this sandwich. I have resolved this by creating the files A1.hpp and A2.hpp and properly including them inside each other to break the cyclic dependency of the files.\n\nMixing up system and local headers\n\nThere are two distinct include paths, the one that gets searched when you use #include <\u2026> and another for #include \"\u2026\" . The former is for system and third-party library headers that are installed globally. You can add to that path using -I . The latter is for your project and can be amended with -i . One should not mix the two and use -I. in order to include the local headers with #include <\u2026> .\n\nusing namespace inside headers\n\nOne can argue about the usage of using namespace . In most cases I will not use it to keep the global namespace somewhat clean. Using using namespace std; in a .cpp file is okay because the namespace std is just dumped out in a single compilation unit. I do not have to worry about it in other files.\n\nIt becomes a totally different story once you put that using namespace std; into a header file. Then every other header or source file that includes it will have that namespace dumped out. Even other projects using that library will suffer from the attempt to save some typing of the library programming.\n\nMultiple files with same include guard\n\nI do not like the #ifndef X , #define X , #endif style include guards because they are so error prone. They are hard to read but even worse, one can choose the same include guard twice in a project. One could even have the same include guard that some other library already has used.\n\nIf that is the case, the errors will be painfully subtle. Depending on the order if inclusion, the last header file will just not be included. Then you get errors about undefined types and undefined functions and might pull out your hair before you realize what is going on.\n\n#pragma once is a good alternative except if you are working with IBM XL 12. But that compiler cannot do C++11, so it is not that interesting for me anyway.\n\nInclude guards do not match the filename\n\nIf you have #ifndef FOO_H in bar.h , bad things will happen when you create another foo.h . Therefore the include guard should always be derived from the path, or use #pragma once .\n\nC library headers\n\nUsing C functions in C++ might be a reasonable thing to do. If you do that, do not include X.h but rather cX . So instead of <stdint.h> use <cstdint> . This way the C functions will be properly in the std namespace.\n\nFunctions like printf , scanf , atoi , fopen , malloc , and a bunch of others should not be used in C++. There are so much better ways.\n\ninclude inside namespaces\n\nFor reason unknown to me, there are a few includes of standard library headers inside of namespaces. The only reason I can think of is that writing ::std:: instead of std:: seemed too much work.\n\nGCC 6.0 has changed the standard headers such that they do not work when you include them inside a namespace. This way one can catch this anti-pattern.\n\nCorrectness\n\nNULL , 0 , and nullptr\n\nIn C++ (and C) there are 64-bit unsigned integer number ( uint64_t or perhaps unsigned long ) and pointers ( void * ). In the machine, they are exactly the same thing, 64-bit unsigned integer numbers. For the type system, it still makes a great difference.\n\nAlthough they are both just numbers to the computer, one should help the programmers read the source code and write\n\nuint64_t number = 0 ; void * pointer = NULL ;\n\nThis way, it is clear that one is number-zero and the other is pointer-null. The preprocessor constant NULL is just 0 or 0u , so the compiler always sees a plain 0 there. Still it is helpful for the programmer to read. In C++11 there even is nullptr which has the correct type, nullptr_t .\n\nIn some code you see horrible things like these:\n\nuint64_t number = NULL ; void * pointer = 0x0 ;\n\nCasting pointers to integers\n\nFor some reason in one project, pointers are casted to integers. And they are converted to a special type of integer that you have to choose as a flag to configure such that the length of that integer is the exact same as the pointer length, otherwise the compiler would give you an error.\n\nI still do not understand why one would do this sort of brittle code.\n\nSubtle dependence on operator precedence\n\nWhat is the value of x ?\n\nuint32_t x = 1 << 3 - 1 ;\n\nI would read it as $2^3 - 1$ and that is seven. The value actually is four. When compiling with Clang, it conveniently says this:\n\nprecedence.cpp:7:25: warning: operator '<<' has lower precedence than '-'; '-' will be evaluated first [-Wshift-op-parentheses] uint32_t x = 1 << 3 - 1; ~~ ~~^~~ precedence.cpp:7:25: note: place parentheses around the '-' expression to silence this warning uint32_t x = 1 << 3 - 1; ^ ( )\n\nWhat did the original author of that line really mean? Was he aware that the - operation binds stronger than << ? Can I trust that code without completely understand where all those magic numbers come from?\n\nNot using RAII\n\nI have seen a couple persons program Java in C++ syntax. With that I mean they wrote stuff like this:\n\nT * t = new T (); // Some code with t-> everywhere. // No delete.\n\nThey did not dispose of the allocated memory again. If they were doing this to hand out the memory or something like that, a heap allocation is needed. But just for local variables, this is nonsense. I have asked why he did not just write T t; and be done with it. He said that he is used to the T *t = new T(); syntax. Then I pointed out that he has a memory leak. He replied that the runtime will take care of that. Too bad there isn't a runtime in C++ ...\n\nAnother person doing this style of programming was actually supposed to teach us students C++.\n\nSince C++11 there are the standard std::unique_ptr and str::shared_ptr with std::make_shared and std::make_unique (last since C++14 only) that will eliminate virtually every case where you need a pointer.\n\nNot using const in C++\n\nconst is an awesome thing. Perhaps Rust with its mut approaches the problem even better although I am a fan of the minimal annotation concept that seems to prevail in C++. Whenever I program in C++, I try to put const wherever I can. It helps me to reason about my code (less moving parts) and helps the compiler to elide some of the variables.\n\nIf you do not even use const for your global variable pi , you are just begging for trouble. I mean I would just add the following somewhere to implement the Indiana Pi Bill:\n\nvoid indiana () { pi = 4 ; }\n\nAnd then the whole program would change its behavior and it would be a pain to debug.\n\nAnother funny example that I found in code:\n\ndouble a = ( double )( 1.25 ); // always 1.25\n\nIt is good to give this magic number a name, but why isn't there a const ?\n\nGlobal variables everywhere\n\nThe const keyword is there for a reason. It is a pity that it is not enforced in C. But that's why we got C++. The project I have in mind consists of almost only global variables and routines. One cannot even call them functions, they only have side effects.\n\nThis needlessly increases the number of things the programmer has to juggle in his head simultaneously. Unit testing is impossible with such a monolithic code. How do you reason about such code?\n\nUnreachable code\n\nSome code is just plain unreachable. Say you have a contraption like this:\n\nif ( condition x ) { // something } else { // something else if ( condition x ) { // What? } }\n\nThere are cases when you have multiple threads and you use locks or atomic variables. Then this code might be reached. Otherwise it is just bloat.\n\nFaulty buffer implementation\n\nUsing other people's tested code is beautiful. So when the standard library of your programming language supports something like a buffer ( std::vector perhaps), use it!\n\nOne project contained its own buffer implementation which had a serious bug: When you call clear() , it will not reset the internal cursor to the beginning and run into other memory eventually. That might corrupt the data or (hopefully) crash the program. Just use the standard library!\n\nC-style casts\n\nC++ has the static_cast , the const_cast , the dynamic_cast , and the reinterpret_cast . They only cast a particular attribute, not everything.\n\nOften I see a C-Style cast:\n\nint i , j ; double x = (( double ) i ) / j ;\n\nIt would be better to use a C++ cast instead:\n\nint i , j ; double x = static_cast < double > ( i ) / j ;\n\nThen it is clear that it is about the type ( int vs. double ) and not about removing a const or something like that.\n\nMixing signed and unsigned integers\n\nThere are signed ( int , long , signed char ) and unsigned ( unsigned int , unsigned long , char ) integer types. For a long time I thought that their major difference is in the ability to store negative numbers. But that is not their key difference. What distinguishes the types is that only unsigned integers have defined overflow behavior.\n\nMost programs use these types for the wrong reasons. Using an unsigned integer forces the caller of the function to give you a positive number. But actually you are specifying (to the compiler) that you want well-defined overflow behavior. Most of the time you do not want that, so using a signed integer is actually the correct thing regarding to the compiler.\n\nSee this talk by Chandler Carruth where he explains the issue with a concrete code example.\n\nAlways using 0 exit status\n\nThe exit status (return value of main ) signals whether an error has occurred to the caller. This is important when the program is called from a test script. Without a proper exit code there is no way of reliably determining the outcome of the called program.\n\nOften I see the following construct:\n\nstd :: cout << \"ERROR: The widgets could not be frobnicated!!!\" << std :: endl ; exit ( 0 );\n\nThere might be the word \"error\" on the screen for the user to see, but a calling script cannot infer that something went wrong. Therefore use exit(1) or some other non-zero number to give the appropriate signal to the outside.\n\nI like to throw an exception even more. So I suggest to do the following instead:\n\nthrow std :: runtime_error ( \"The widgets could not be frobnicated!\" );\n\nThis has the following advantages:\n\nThe exit status is automatically set to a non-zero value.\n\nThe error message on the console will be emitted on standard error ( std::cerr )\n\n) In contrast to abort() this will call the destructors of the current scopes.\n\nthis will call the destructors of the current scopes. The keyword throw stands out in the editor to signal that this code path is about something special. No need for the word \"error\" in all-caps, multiple exclamation marks or similar.\n\nstands out in the editor to signal that this code path is about something special. No need for the word \"error\" in all-caps, multiple exclamation marks or similar. The error message has a standard format: $ ./a.out terminate called after throwing an instance of 'std::runtime_error' what(): The widgets could not be frobnicated! fish: './a.out' terminated by signal SIGABRT (Abbruch)\n\nThe user can load the program in the debugger and work with the stack trace to see the origin of the error. For a simple example (exception thrown in f ) this looks like this: (gdb) bt #0 0x00007ffff716d660 in raise () from /lib64/libc.so.6 #1 0x00007ffff716ec41 in abort () from /lib64/libc.so.6 #2 0x00007ffff7ae3025 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6 #3 0x00007ffff7ae0c16 in __cxxabiv1::__terminate(void (*)()) () from /lib64/libstdc++.so.6 #4 0x00007ffff7ae0c61 in std::terminate() () from /lib64/libstdc++.so.6 #5 0x00007ffff7ae0ea4 in __cxa_throw () from /lib64/libstdc++.so.6 #6 0x000000000040093a in f () at cpp.cpp:7 #7 0x0000000000400964 in main (argc=1, argv=0x7fffffffe588) at cpp.cpp:11 Also I can move around the stack and take a look at all the variables in the program.\n\nNumerics\n\nInsufficient serialization precision\n\nBinary data formats are harder to understand than a simple text file that is human readable. However, storing arrays of numbers in binary has many advantages:\n\nThe file size is just the size of the array in memory. This also means that the IO bandwidth is used more efficiently.\n\nThere is no need to parse the file, it can be loaded directly into the memory.\n\nAs every number has the exact same size, one can parallelize reading and writing of chunks of the file, say with MPI.\n\nThere is no precision loss.\n\nThe only disadvantages are that you might have a hard time changing the architecture. With IEEE floating point this should be fine for all usual computers. There are architectures like PowerPC which work in big endian (though PowerPC supports both), whereas the usual x64 CPUs work in little endian.\n\nIf you have a complicated binary format which various different elements, you might have a hard time reading the data without the actual program. I would suggest not to do this and instead use something structured like HDF5 to store data and metadata in a binary format.\n\nIn projects I have seen people writing out floating point numbers with a simple fprintf(fp, \"%f\", number) . This looks great at first as you can read the generated text file with a text editor. But IO will be slower than necessary. Also you only get a few digits (five by default?). This means that writing and reading your data will introduce rounding errors.\n\nEven though I would strongly recommend binary IO for this, use at least std::numeric_limits<T>::max_digits10 (cppreference page) digits when writing numbers to a text file. This way it will be exact in the conversion from binary to decimal and back.\n\nObject oriented programming\n\nToo large classes\n\nA class should have a number of invariants. These are statements about the consistency of the thing that the class represents. All member functions need to take the class from one valid state to another one. Within a member function you may temporarily break an invariant if you restore if before the end of the function.\n\nThe set of member functions should be as small as possible. Everything else should be implemented as free functions. I often see code where a bunch of convenience functions are crammed into the class.\n\nFor an implementation of the Ising model with the Metropolis algorithm one needs some sort of 2D data structure. A simple quadratic lattice can be implemented using this class:\n\nclass Lattice { public : using Spin = int8_t ; Lattice(int const size) : size_(size), data_(size * size) { } Spin & operator ()( int const x , int const y ) { return data_ [ y * size_ + x ] ; } Spin const & operator ()( int const x , int const y ) const { return data_ [ y * size_ + x ] ; } int size () const { return size_ ; } private : int size_ ; std :: vector < Spin > data_ ; } ;\n\nIn a lot of solutions I have seen member functions like double Lattice::energy() const , double Lattice::magnetization() const and void Lattice::do_update() . They do not belong in this class! The invariants of this class are:\n\nThere is enough memory for the whole lattice.\n\nEach lattice point contains a valid spin value.\n\nThe function energy should rather be implemented as a free double energy(Lattice const &lattice) which can then use the Lattice::operator() const to compute the total energy of the lattice.\n\nMissing virtual destructor\n\nWhen a class is meant to be derived from, the destructor should be virtual. Otherwise there can be memory leaks. Take this very simple example where we have a base class that has no members and no virtual functions. A derived class (over which the author of Base has no control) now introduces manual resource management with new and delete . In the user code we use dynamic polymorphism. When the unique_ptr gets destructed it will call delete on its internal pointer. And that will then call Base::~Base which is a non-virtual empty function supplied by the compiler.\n\n#include <iostream> #include <memory> class Base { public : void hello () { std :: cout << \"Hello, World\" << std :: endl ; } // virtual ~Base() {} }; class Derived : public Base { public : Derived () : p ( new int [ 1 ]) {} ~ Derived () { delete p ; } private : int * p ; }; int main () { std :: unique_ptr < Base > base ( new Derived ()); }\n\nThe lack of the virtual constructor means that Base does not have a virtual function table and that deriving classes have no chance of overriding the destructor. In this example this introduces a memory leak which is directly found with the leak sanitizer ( -fsanitize=leak ):\n\n== 31756 == ERROR : LeakSanitizer : detected memory leaks Direct leak of 4 byte ( s ) in 1 object ( s ) allocated from : # 0 0 x7f520ce6d605 in operator new []( unsigned long ) ( / lib64 / liblsan . so . 0 + 0 xe605 ) # 1 0 x400975 in Derived :: Derived () ( / home / mu / Projekte / eigene - webseite / articles / cpp - antipatterns / a . out + 0 x400975 ) # 2 0 x4008c6 in main ( / home / mu / Projekte / eigene - webseite / articles / cpp - antipatterns / a . out + 0 x4008c6 ) # 3 0 x7f520c1e0f29 in __libc_start_main ( / lib64 / libc . so . 6 + 0 x20f29 ) SUMMARY : LeakSanitizer : 4 byte ( s ) leaked in 1 allocation ( s ).\n\nWhen the virtual destructor is commented in, the memory leak disappears.\n\nFormatting & organization\n\nLeading or double underscores\n\nOne should not use leading underscores for definitions in normal code. Those are reserved for the standard library or the compiler. When programming with intrinsics like AVX, then one has to use functions and types that start with underscores. But one is not allowed to defined new types with leading underscores.\n\nTwo underscores anywhere in the name is also reserved.\n\nA pattern I often see is the usage of leading underscores in include guards:\n\n#ifndef __MY_AWESOME_HEADER_H__ #define __MY_AWESOME_HEADER_H__ // Code #endif\n\nThere is no need for the __ , really. Just having MY_AWESOME_HEADER_H would be fine as well. Or perhaps just #pragma once , but that is a different debate.\n\nInconsistent indentation\n\nFor several years now, there is clang-format. If you do not use that, you are seriously missing out. It will format your whole codebase perfectly and also comes with an editor plugin. You do not have to worry about indentation, it will just be perfect.\n\nUnfortunately it is very hard to apply that to a codebase which has a lot of branches in the git repository. Reformatting all the code will lead to merge conflicts and resolving them will not be any fun. So start your project right away with it or attempt to introduce it at some point.\n\nInconsistent formatting will make the code harder to read and reason about, something you do not want with code that is already complex.\n\nActually, I think that having no comments is great to some point. Compare\n\ndouble f(double x) { double c = 235; double z = c / x; return z; }\n\nwith\n\n// Convert {miles per gallon} to {liters per 100 km}. double f(double x) { // Magic conversion factor. double c = 235; // Do the conversion. double z = c / x; // Return the result. return z; }\n\nand with\n\ndouble mpg_to_l100km(double const mpg) { double const conversion_factor = 235; double const l100km = conversion_factor / mpg; return l100km; }\n\nI think that good variable and function names can make a huge difference and eliminating the need for comments. If you can write your code so expressive that you do not need the comments, that is better than commenting bad code.\n\nNo formal docstrings\n\nPython had docstrings, Java has JavaDoc and C++ has CppDoc you can set with Sphinx, Epidoc, JavaDoc and Doxygen. Not having any of those makes some tricky functions harder to use than needed in case the function is not as trivial as the name might suggest.\n\nEven worse than no comments are comments that do not match the code. Then you are left to figure out what happens. Hint: The compiler only reads the code.\n\nComplex variable names\n\nWhat does the variable nyHP1 contain? Isn't it obvious that it is \"number of elements in y-direction halved plus one\"?\n\nBugtracking and changelog in the source itself\n\nIn the (to me) ancient days when one did not have version control or a bug tracking system, writing those in text files next to the source seemed to make sense. Today there is no excuse to track your bugs in top of the one source file.\n\n3400 lines in a single file\n\nWhen a file is too long, you cannot keep its structure in your mind. After a couple hundred lines, you should start a new file. That is also a good opportunity to refactor the code into smaller pieces.\n\nOf course this might increase compilation time. Then something like a unity build might be appropriate again. However, this is a distinct step and makes the file more readable.\n\nSingle function with 800 lines\n\nHaving a single function with 800 lines is just impossible to reason about. You cannot even remember the variables you have declared. It is almost as bad as if they were global.\n\nSuperfluous blank lines\n\nOccasionally I see lots and lots of blank lines in code. One or perhaps even two blank lines can help grouping various parts of a file. When there are five or mode blank lines in a row, it dilutes the information density on screen, I have to scroll to read it and cannot have a whole function on my screen. Getting rid of those superfluous blank lines makes the code easier to read.\n\nDuplicating source code\n\nSay there is a function that does a particular task. Then there is a need for a second task which is almost like the first task, but different at a couple of different points. It might be tempting to just duplicate all the code and change the few bits.\n\nIn the long run, the parts which used to be the same will diverge, though. And then the real cost of duplicating the code will show up: Do you have to copy the changes? It would be better to abstract the first task such that there is a generic task and the first task is just a specialization of that. Then the second task could be added as another specialization without duplicating any code.\n\nIn the real world, this is not that simple. It might be worthwhile to start duplicating the code and get the feature implemented. Then later on, one can find a good abstraction for the two tasks. Finding the abstraction beforehand might now be feasible.\n\nHard to read for loops\n\nSometimes a loop has to be done over a few elements only. For instance 1 and -1 . Then writing a loop like this would work:\n\nfor (int i = 1; i >= -1; i -= 2)\n\nHowever, it takes a while to get that it iterates over 1 and -1 . A more succinct way of writing would be this:\n\nfor (int i : {1, -1})\n\nThat needs C++11, but that should be available.\n\nMiscellaneous C++ items\n\nDuplicate explicit instantiation\n\nTemplates can drive up the compilation time a lot. One way around this is to define the template functions in a source file and explicitly instantiate them there. This works if you know all the template parameters already, say you want to template on float and double only. But if you do it, just instantiate once in the whole project. Otherwise it will become a compilation error.\n\nNeedless flushing\n\nA std::endl is a combined \"\n\n\" and std::flush . So when one wants to output a multi-line string, it is better to use \"\n\n\" instead of std::endl each time. Also supplying std::endl << std::flush will just make the program a tad slower, it output to the terminal is the limiting factor already.\n\nUseless cast\n\nOnce I saw the line\n\ndouble x = (double)(1);\n\nThis is not really needed. It would be sufficient to do double x = 1; and trust that the assignment does the proper cast. Also it would do the same do double x = 1.0 . Also one could just do auto x = 1.0 for double , 1.0f for float and 1.0l for long double . That would not even use a single type identifier once.\n\nOverloaded functions with different tasks\n\nThe C++ overload is very handy do have things like void foo(char const *) and void foo(std::string const &) . You can pass both things and the correct function will be used.\n\nSometimes there are overloads which do vastly different things. There is a void do_stuff(int) which does something completely different from void do_stuff(double, size_t) . How is the user supposed do understand this?\n\nVersion control\n\nNot using version control\n\nIf you are not using version control, you are probably having a harder time to collaborate or develop new features than it would need to be.\n\nI have seen one group of four people that have send around tar archives and used the diff tool manually. That was in 2015, git has been 10 years old and there is no excuse, really.\n\nUnstable \"master\"\n\nThe master branch is supposed to be the branch that you can checkout and compile, always. If you want to develop a new feature and it is not stable yet, have in a development branch or some feature branch.\n\nI expect that a simple git clone (which checkouts master ) will give me working code.\n\nDoing all the work on develop and never merging back into master keeps that branch stable, but it is also not really cool. In one project the master was outdated and unstable. Great combination.\n\nAbandoned branches\n\nA lot of git repositories have a lot of branches that seem somewhat abandoned. The commit messages promise some great feature that the develop branch lacks. However, the branches have not seen a commit in over a year. So are they still relevant?\n\nChecking in build files\n\nWay too often build files like .o files are checked into git. This means that every time you just compile the code, git will tell you about changes to the files. Now you can just commit them or leave them uncommitted. Usually they get committed and then your git log is just polluted with changes to files that should not have been tracked in the first place.\n\nWith git submodules that is really nasty because a simple compilation of the project will leave all the submodules in the dirty state. You will have to go through the submodules recursively and do a git reset HEAD --hard in order to get rid of the changes. But that resets your compilation again.\n\nThe only solution is to do git rm --cached on those files and put them into a .gitignore .\n\nThere are many versions to create tags for versions: Leading v , periods or hyphens or underscores between the digits, leading zeros. But please be consistent, otherwise it will be a nightmare to find the latest version.\n\nGit submodules via GitHub SSH\n\nWhen repositories are hosted on GitHub, one can either use them as submodules over SSH or HTTPS. The advantage of HTTPS is that it does not need any sort of authentication in order to read. For SSH, you need to have an SSH key registered with GitHub. If you do that on your personal workstation where you have your SSH key configured with GitHub, that is not a problem at all.\n\nOn the clusters and supercomputers, I do no have my personal SSH key. And I am not going to copy my personal key to a machine which I do not control. I could create a SSH key on the cluster and add that to my GitHub account. But then you could use that SSH key to make changes to all the repositories on my GitHub account. Not cool.\n\nCreating a whole second GitHub account is possible, but you cannot have more than one free account. So I would do get a second paid account for my master thesis work. Also something I would like to avoid.\n\nThe easiest solution is to convert the submodules over to HTTPS. Then you can just download them everywhere. Pushing changes over HTTPS is not that nice because you need to enter your password every single time. So I can understand why SSH is appealing. Also there are apparently some setups where HTTPS is blocked by SSH is not.\n\nSwitching it over to HTTPS would it make easier for people to clone the repository. Then if you want to push something, just add an SSH based remote to the repository and you can push. That way, both sides have it usable.\n\nUnmergable forks\n\nSay one has a project that uses the double data type everywhere. One wants to let it run with float because that is a bit faster and one does not need the precision. The easy way would be to fork the project, create a new branch and just replace every double with float . Problem solved.\n\nThat cannot be sensibly merged, though. The upstream author probably still wants to have double in their version of the code. The other person wants float . Now the two forks are divergent and there is no sensible way to ever get them together again. Even worse, further changes cannot be shared without manually replacing all the double with float or the other way around.\n\nIt is better to solve this via abstraction. Work with the original author to abstract away the actual floating point data type used. Something like this, perhaps:\n\n#ifdef SINGLE_PRECISION typedef float Float ; #else typedef double Float ; #endif\n\nThen you change every double to Float in the codebase once and you can easily share the code back and forth. Actually, there is no need for a long running fork any more, the two people can just use the same code all the time and compile it how they like.\n\nOne should be careful not do overdo this, though. Exposing an option for everything in your program will make it a nightmare to actually compile and the number of combinations increases exponentially. In this particular case, where two people have actual needs, it makes more sense to solve it with abstraction than a hard fork of the whole code.\n\nDead code\n\nCode might not be needed any more at some point. In order to quickly get rid of the code, one can comment it out. On the long run, one should just delete the code and trust the version control to remember it. Otherwise the codebase is littered with old code that has been removed and does not serve any purpose any more.\n\nBuild System\n\nHardcoding paths\n\nSome parts of the build systems might have hardcoded paths in them. It might work well on the developer's machine but will lead to unresolvable paths on every other machine.\n\nOn one supercomputer, I have GNU Autotools 1.11 and 1.14 available. The project that I have downloaded ships with some files that can easily be generated (see above). Those files explicitly refer to GNU Autotools 1.13 and therefore fail to build for me.\n\nI had to remove files like configure and aclocal.m4 and recreate those with some calls to automake --add-missing and autoreconf . After that I was able to run configure .\n\nPatch the code to build / compile branches\n\nOne project has to run on different architectures and with different MPI implementations and versions of compilers. Since the MPI implementations differ by some const in functions like MPI_Send , you cannot use const -aware code with an implementation which is unaware of it. The code had to be patched before building on different architectures. There were a lot of branches in git just for compiling.\n\nThis makes bisecting impossible and also packaging this for a distribution is hard. Debian and especially Fedora frown about patching the source for reasons like this. The build system should expose all the options needed to build it on the target platforms.",
            "published_at": "2016-12-04T00:00:00+01:00"
        },
        {
            "authors": [],
            "title": "Support for the IPFS protocol",
            "contents": "Please enable JavaScript in your browser to use all the features on this site."
        },
        {
            "authors": [],
            "title": "TIOBE - The Software Quality Company",
            "contents": "TIOBE Index for January 2021\n\nJanuary Headline: Python is TIOBE's Programming Language of 2020!\n\nPython has won the TIOBE programming language of the year award! This is for the fourth time in the history, which is a record! The title is awarded to the programming language that has gained most popularity in one year. Python made a positive jump of 2.01% in 2020. Programming language C++ is a very close runner up with an increase of 1.99%. Other winners are C (+1.66%), Groovy (+1.23%) and R (+1.10%).\n\nIt has been stated before: Python is popping up everywhere. It started as a competitor of Perl to write scripts for system administrators a long time ago. Nowadays it is the favorite language in fields such as data science and machine learning, but it is also used for web development and back-end programming and growing into the mobile application domain and even in (larger) embedded systems. The main reasons for this massive adoption are the ease of learning the language and its high productivity. These two qualities are key in a world that is craving for more developers in all kinds of fields. Python already tested the second position some months ago and it will for sure swap places with Java permanently soon. Will Python also beat C? Well, C has still one trump card to play: its performance, and this will remain the case for some time to come. So I guess it will certainly take some years for Python to become the new number 1 in the TIOBE index.\n\nWhat else happened in the TIOBE index in 2020? C has become number 1 again, beating Java. Java lost almost 5% in only 1 year. Other interesting moves in the top 20 are the statistical language R (from position 18 to 9), and Groovy, which is mainly used for scripting for CI/CD tool Jenkins, from position 23 to 10. Are there any new top 20 entries to be expected in 2021? Top candidate is without doubt Julia, which jumped from position 47 to position 23 in the last 12 months. Dart and Rust are other candidates, but both of them touched the top 20 already without being able to stay for a longer time. - Paul Jansen CEO TIOBE Software\n\nThe TIOBE Programming Community index is an indicator of the popularity of programming languages. The index is updated once a month. The ratings are based on the number of skilled engineers world-wide, courses and third party vendors. Popular search engines such as Google, Bing, Yahoo!, Wikipedia, Amazon, YouTube and Baidu are used to calculate the ratings. It is important to note that the TIOBE index is not about the best programming language or the language in which most lines of code have been written.\n\nThe index can be used to check whether your programming skills are still up to date or to make a strategic decision about what programming language should be adopted when starting to build a new software system. The definition of the TIOBE index can be found here.\n\nJan 2021 Jan 2020 Change Programming Language Ratings Change 1 2 C 17.38% +1.61% 2 1 Java 11.96% -4.93% 3 3 Python 11.72% +2.01% 4 4 C++ 7.56% +1.99% 5 5 C# 3.95% -1.40% 6 6 Visual Basic 3.84% -1.44% 7 7 JavaScript 2.20% -0.25% 8 8 PHP 1.99% -0.41% 9 18 R 1.90% +1.10% 10 23 Groovy 1.84% +1.23% 11 15 Assembly language 1.64% +0.76% 12 10 SQL 1.61% +0.10% 13 9 Swift 1.43% -0.36% 14 14 Go 1.41% +0.51% 15 11 Ruby 1.30% +0.24% 16 20 MATLAB 1.15% +0.41% 17 19 Perl 1.02% +0.27% 18 13 Objective-C 1.00% +0.07% 19 12 Delphi/Object Pascal 0.79% -0.20% 20 16 Classic Visual Basic 0.79% -0.04%\n\nOther programming languages\n\nThe complete top 50 of programming languages is listed below. This overview is published unofficially, because it could be the case that we missed a language. If you have the impression there is a programming language lacking, please notify us at tpci@tiobe.com. Please also check the overview of all programming languages that we monitor.\n\nPosition Programming Language Ratings 21 SAS 0.77% 22 PL/SQL 0.74% 23 Julia 0.73% 24 Scratch 0.71% 25 Dart 0.62% 26 Rust 0.61% 27 ABAP 0.57% 28 D 0.54% 29 Prolog 0.48% 30 Fortran 0.46% 31 COBOL 0.42% 32 Ada 0.40% 33 Transact-SQL 0.40% 34 Scala 0.39% 35 VHDL 0.38% 36 Lisp 0.37% 37 Lua 0.37% 38 PowerShell 0.35% 39 (Visual) FoxPro 0.35% 40 Kotlin 0.34% 41 Logo 0.33% 42 TypeScript 0.33% 43 LabVIEW 0.31% 44 Haskell 0.31% 45 Apex 0.29% 46 Bash 0.29% 47 Awk 0.28% 48 Solidity 0.27% 49 Scheme 0.27% 50 VBScript 0.26%\n\nThe Next 50 Programming Languages\n\nThe following list of languages denotes #51 to #100. Since the differences are relatively small, the programming languages are only listed (in alphabetical order).\n\n4th Dimension/4D, ABC, ActionScript, Alice, Applescript, AutoLISP, B4X, bc, Bourne shell, CIL, CL (OS/400), Clojure, CoffeeScript, Common Lisp, Crystal, cT, Elixir, Emacs Lisp, Erlang, F#, Factor, Hack, Icon, Inform, Io, J, Korn shell, Ladder Logic, LiveCode, Maple, ML, MQL4, NATURAL, Nim, OpenCL, OpenEdge ABL, PILOT, PL/I, PostScript, Q, Ring, RPG, S, Simulink, Small Basic, SPARK, SPSS, Stata, Tcl, Verilog\n\nThis Month's Changes in the Index\n\nThis month the following changes have been made to the definition of the index:\n\nMalgorzata Slota did some research and proposed to add \"-healing\" for Crystal programming. This has been accepted. Thanks for your suggestion Malgorzata!\n\nThere are lots of mails that still need to be processed. As soon as there is more time available your mail will be answered. Please be patient.\n\nVery Long Term History\n\nTo see the bigger picture, please find below the positions of the top 10 programming languages of many years back. Please note that these are average positions for a period of 12 months.\n\nProgramming Language 2021 2016 2011 2006 2001 1996 1991 1986 C 1 2 2 1 1 1 1 1 Java 2 1 1 2 3 28 - - Python 3 5 6 7 23 13 - - C++ 4 3 3 3 2 2 2 8 C# 5 4 5 6 8 - - - JavaScript 6 7 10 9 6 30 - - PHP 7 6 4 4 18 - - - R 8 16 41 - - - - - SQL 9 - - - - - - - Swift 10 15 - - - - - - Perl 14 9 7 5 4 3 17 - Lisp 29 26 14 13 17 6 3 2 Ada 33 24 21 16 15 4 9 3\n\nProgramming Language Hall of Fame\n\nThe hall of fame listing all \"Programming Language of the Year\" award winners is shown below. The award is given to the programming language that has the highest rise in ratings in a year.\n\n\n\n\n\nYear Winner 2019 C 2018 Python 2017 C 2016 Go 2015 Java 2014 JavaScript 2013 Transact-SQL 2012 Objective-C 2011 Objective-C 2010 Python 2009 Go 2008 C 2007 Python 2006 Ruby 2005 Java 2004 PHP 2003 C++\n\nBugs & Change Requests\n\nThis is the top 5 of most requested changes and bugs. If you have any suggestions how to improve the index don't hesitate to send an e-mail to tpci@tiobe.com.\n\nApart from \"<language> programming\", also other queries such as \"programming with <language>\", \"<language> development\" and \"<language> coding\" should be tried out. Add queries for other natural languages (apart from English). The idea is to start with the Chinese search engine Baidu. This has been implemented partially and will be completed the next few months. Add a list of all search term requests that have been rejected. This is to minimize the number of recurring mails about Rails, JQuery, JSP, etc. Start a TIOBE index for databases, software configuration management systems and application frameworks. Some search engines allow to query pages that have been added last year. The TIOBE index should only track those recently added pages.\n\nFrequently Asked Questions (FAQ)"
        },
        {
            "authors": [],
            "title": "California's Clearest COVID-19 Vaccine Appointment Dashboard Is Run by Volunteers",
            "contents": "If you've been looking for facts about getting your COVID-19 vaccine, you might have discovered that just finding clear information on how to schedule an appointment for an eligible person can be a difficult, time-consuming process.\n\nA week ago, a site called VaccinateCA launched that not only lists vaccination sites around California, but details their current vaccine availability. The information is gathered manually by a team who compiled a list of medical centers, pharmacies and hospitals around the state, and now regularly contacts those locations to confirm their vaccine inventory \u2014 as well as what groups they're now accepting, and how to make an appointment yourself.\n\nBut the thing about VaccinateCA? It's completely staffed by volunteers who make those calls, maintain the website and coordinate efforts across the state. And this crowdsourcing is filling a need in California that as yet, health officials don't seem to have addressed themselves: the need for residents to simply schedule a COVID-19 vaccine appointment near them.\n\nConfusion and Low Availability\n\nCalifornia has begun by prioritizing health care workers and long-term care residents (Phase 1A) for vaccination, and is now moving into its next phases. But even if you're eligible for the state's current vaccination phase, you'll need to find a vaccination site near you that has inventory enough to be able to serve you."
        },
        {
            "authors": [
                "Caleb Scharf",
                "Lee Billings",
                "John Hainze",
                "Ron Cowen",
                "Kitty Ferguson"
            ],
            "title": "Who Said Nobody Read Isaac Newton?",
            "contents": "The central university library at Cambridge, in the United Kingdom, is an imposing, towered building known affectionately for being called a \u201cmagnificent erection\u201d by, before he became prime minister, Neville Chamberlain.1 When I was a graduate student there, studying astronomy, rumors circulated within my cohort that if you went to the library and asked nicely, you would be allowed to examine, under supervision, a first edition of Isaac Newton\u2019s Principia, first published in 1687, complete with his own handwritten notes in the margins and inserted sheets.\n\nAll of which suggested a marvelous adventure. The Principia, or Philosophae Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), has a mythological status. It was in fact three books: the first two covering the propositions and laws of motion, along with the innate attraction of all bodies to each other (gravitation), and the third being On the system of the world, in which Newton applied all of his insights to the detailed motions of the planets and their satellites, including our moon, the tidal motions, and even comets. These books laid out the quantitative foundations of classical mechanics, established the idea of gravity as a force sculpting the cosmos, and set up space and time as absolutes.\n\nSupposedly the Principia was such an incomprehensible monster of that few could make much sense of it.\n\nSadly, I never quite got around to testing the rumor out for myself on the days I rode my bicycle past the library, hurrying to get out of the drizzle or morning fog. But the copy most certainly exists,2 as do at least one or two others in Cambridge that have Newton\u2019s own post-publication additions. Part of the mythology surrounding Newton\u2019s magnificent thesis has been the notion that neither very many copies were printed for its first edition, nor was it at first very well read, or used. That latter supposition seems to relate to the received wisdom that the Principia, written in Latin, was such an incomprehensible monster of technical detail that very few would have been able to make much sense of it. Instead, the story goes, there was a slow burn\u2014until the early 1700s, at which point Newton\u2019s corrected and improved later editions, along with a growing interest among scientists, began to finally have an impact.\n\n\n\nLike many things in history, the precise origins of this myth are a little hard to pin down. But a recently published study, the result of more than a decade of work by the historians Mordechai Feingold, of Caltech, and Andrej Svoren\u010d\u00edk, of the University of Mannheim, looks to shed light on this tale, and presents a dramatically updated narrative.3 What seems to be emerging from this new census is a picture in which Newton\u2019s great work was actually widely, and immediately, read and discussed, perhaps even enjoyed, by both his intellectual peers and a larger population. It adds a new element to the story of how modern science developed, with more likely to come as the researchers dig deeper.\n\nAlso in Literature The Book No One Read By Lee Billings I remember well the first time my certainty of a bright future evaporated, when my confidence in the panacea of technological progress was shaken. It was in 2007, on a warm September evening in San Francisco, where I was relaxing...READ MORE\n\nYet the idea that some of the central texts in the development of western science were either little distributed, or little read, has a certain strange draw. This same tale of scant readership was for a long time also applied to Nicolaus Copernicus\u2019s De revolutionibus (On the Revolutions of the Heavenly Spheres), the volume that was famously published more or less while he was on his deathbed, in the spring of 1543. It changed our view of existence by firmly removing Earth from the center of the cosmos, and by applying an inductive approach to science, where evidence came first, and hypothesis after.\n\nThe astronomer and historian of science Owen Gingerich, in his magnificent exploration The Book Nobody Read, published in 2004, went in search of answers to this puzzling contradiction: the idea that Copernicus\u2019s work was little read and little understood, yet so influential. What he found was not only 276 first editions (a perfectly respectable number for the time) but that a wealth of copies came with the handwritten notes of their readers, many of whom represented the astronomical and intellectual movers-and-shakers of the period. Far from being a 16th-century coffee table obscurity, De revolutionibus and the proposals it contained had clearly been a critical, and well-read, reference point.\n\nWhen new insights to the nature emerge, they are stress-tested by their correspondence to reality.\n\nThis kind of narrative, of the unread masterpiece, doesn\u2019t go away though. Even with more modern texts, aimed at quite general audiences, the same sort of tales or myths seem to develop. For a while it was (and perhaps still is4) positively fashionable to admit to having a copy of Stephen Hawking\u2019s A Brief History of Time but to have \u201cnot really read all of it.\u201d As if that somehow conferred greater status to a book, one that explored the origins and evolution of the universe, as well as to your fortitude for at least having given the story a shot.\n\n\n\nIn Newton\u2019s case, some of our modern interpretation of the Principia\u2019s dispersal appears to stem from a 1953 study by Henry Macomber5 (curiously then at the Babson Institute in Massachusetts, a school focused on business and finance) that made a census of who owned the first editions. That study sided with estimates made in the late 1800s to claim that fewer than 250 copies (based on 189 actually identified ones) had ever existed. Given the well-known birthing pains of Principia itself, which took the gritted teeth and persistence of astronomer and comet-finder Edmond Halley to squeeze action out of Newton, and a variety of elitist 18th- and 19th-century anecdotes about the material\u2019s impenetrability, it\u2019s not hard to see how the idea of a \u201cdifficult,\u201d little-read book took root.\n\nYet there had to be a rather larger print run, others have suggested, based on the obvious incompleteness that comes from only counting the copies that still exist today. To try to get to the bottom of this, Feingold and Svoren\u010d\u00edk undertook an astonishingly systematic and exhaustive preliminary survey spanning some 27 countries and their libraries, private owners, and booksellers. The survey\u2019s discoveries add up to 387 identified copies of the first edition of Principia, indicating a print run that was likely more in the range of 600 to 650 copies\u2014a far cry from a couple of hundred. But the most wonderful results and insights promise to come from future analyses of who owned the books, how often they exchanged hands, and what people\u2019s reactions were from the notes written in the pages.\n\nIt was (and perhaps still is) fashionable to admit to having a copy of Stephen Hawking\u2019s A Brief History of Time.\n\nThe reality in all of these cases has to be that, in the end, important works do most certainly get read, or else how would we ever consider them important? But the avid readers, the marginalia scribblers, the thinkers, don\u2019t go about telling their gossipy cocktail partners that they\u2019re really relishing Newton, or Copernicus. Instead they\u2019re simply busy stretching their minds, seeking insight and inspiration for their own works that will, in turn, help dilute and propagate the original breakthroughs that were\u2014by the nature of being pioneering\u2014awkward, dense, tricky,6 and extremely challenging.\n\n\n\nBy contrast, it\u2019s the complainers and (to use the modern term) trolls who perhaps make the most quotable and long-lasting statements about their confusions. In Feingold and Svoren\u010d\u00edk\u2019s examples they describe incidents like that of the elderly mathematician and natural philosopher Gilbert Clarke, who wrote to Newton asking him to assist with understanding Principia\u2019s contents, and complaining about why it wasn\u2019t easier to understand; stating \u201cyou masters doe(sic) not consider the infirmities of your readers, except you intend to write only to professours or intended to have your books lie, moulding in libraries or other men to get the credit of your inventions.\u201d7\n\nIt\u2019s an amusing, if perhaps horrifying, thought experiment to imagine Principia showing up on Amazon today and running the gauntlet of the comment system. \u201cWhy is this thing written in Latin?\u201d \u201cIt would help if it wasn\u2019t so long.\u201d \u201cI can\u2019t understand this, why are scientists so bad at explaining things?\u201d \u201cI don\u2019t buy this idea of gravity at all, what nonsense!\u201d Four centuries from now the historians of science would be examining terabytes of Facebook and Twitter records to try to understand whether or not anyone actually read this monumental work.\n\nThere is a deeper, and far more optimistic lesson in all of this. We are a species of information and ideas, propagating these things through time and space in our genes, brains, and externally instantiated data.8 When new insights to the nature of the world emerge, they are stress-tested by their correspondence to reality, and their reception by our spindly neurons. Science of the past 400 years is a particularly efficient and demanding sieve for what works and what doesn\u2019t. Science also doesn\u2019t stand isolated from the societies that have generated it. Scurrilous rumors about nobody reading certain books, or ideas that are too hard to come to grips with, are a part of that social element, and they can be seen as part of the sieve, helping distill and disperse the best of our insights.\n\nIndeed, if you follow up on the tale of crabby old Gilbert Clarke in the 1600s, it turns out that Newton took his complaints to heart and made corrections in the second edition of Principia, based on those prompts. But we tend not to hear much about these improvements to readability. The myth of his \u201cimpossible\u201d book actually reinforced its importance; after all, it must be really important if it\u2019s that tough to understand.\n\nIn that sense, there is a kind of natural selection taking place for human information itself, like a bottleneck or founder effect in biological evolution. The most useful or important information tends to propagate forward, but in order to do so it may have to be squeezed and recast into better-digested pieces that can go on to populate human minds. Consequently, we feel compelled to, at first, convince ourselves that Principia or De revolutionibus were too much for most people\u2014yet somehow the ideas stuck because they were so extraordinarily powerful.\n\nIn retrospect, my not parking my bicycle outside of the Cambridge library and going in to look at Newton\u2019s handwritten notes wasn\u2019t actually a gross personal failure. I already had knowledge of the Principia\u2019s essential contents. I was a perfect example of the astonishing success of the mythology of this impossible, little-read book, its actual wide dispersal and translation, and the scientific revolution that it set in motion. At least that\u2019s my story.\n\n\n\n\n\nCaleb Scharf is an astrophysicist and the director of astrobiology at Columbia University in New York. His latest book is The Ascent of Information: Books, Bits, Genes, Machines, and Life\u2019s Unending Algorithm, coming in June 2021. Follow him on Twitter @caleb_scharf.\n\n\n\n\n\nReferences\n\n1. In 1934 upon the library\u2019s completion, prior to his becoming prime minister.\n\n2. Newton, I. Philosophi\u00e6 Naturalis Principia Mathematica (1687). University of Cambridge Digital Library. Retrieved from: https://cudl.lib.cam.ac.uk/view/PR-ADV-B-00039-00001/9\n\n3. Feingold, M. & Svoren\u010d\u00edk, A. A preliminary census of copies of the first edition of Newton\u2019s Principia (1687). Annals of Science 77, 253-348 (2020).\n\n4. The New York Times Books. Just like you, Claire Messud never read A Brief History of Time. The New York Times (2020).\n\n5. Macomber, H.P. A census of the owners of copies of 1687 first edition of Newton\u2019s Principia. Papers of the Bibliographical Society of America 47, 269\u2013300 (1953).\n\n6. Sylla, E. Compounding ratios: Bradwardine, Oresme, and the first edition of Newton\u2019s Principia. In Mendelsohn, E. (ed.) Transformation and Tradition in the Sciences: Essays in Honour of I. Bernard Cohen, Cambridge University Press, Cambridge (1984).\n\n7. Newton, I. Correspondence 2:492, 501; 3:7.\n\n8. Scharf, C. The Ascent of Information: Books, Bits, Genes, Machines, and Life\u2019s Unending Algorithm Riverhead/Penguin Random House, New York, NY (2021).\n\n\n\n\n\nLead photocollage: Tasnuva Elahi. Original images: Natata / Shutterstock; Public Domain"
        },
        {
            "authors": [
                "National Research Council Of Science"
            ],
            "title": "Cancer can be precisely diagnosed using a urine test with artificial intelligence",
            "contents": "The set of sensing signals collected for each patient were then analyzed using ML to screen the patient for PCa. Seventy-six urine samples were measured three times, thereby generating 912 biomarker signals or 228 sets of sensing signals. We used RF and NN algorithms to analyze the multimarker signals. Both algorithms provided an increased accuracy, and the AUROC increased in size as the number of biomarkers was increased. Credit: Korea Institute of Science and Technology (KIST)\n\nProstate cancer is one of the most common cancers among men. Patients are determined to have prostate cancer primarily based on PSA, a cancer factor in blood. However, as diagnostic accuracy is as low as 30%, a considerable number of patients undergo additional invasive biopsy and thus suffer from resultant side effects, such as bleeding and pain.\n\nThe Korea Institute of Science and Technology (KIST) announced that the collaborative research team led by Dr. Kwan Hyi Lee from the Biomaterials Research Center and Professor In Gab Jeong from Asan Medical Center developed a technique for diagnosing prostate cancer from urine within only 20 minutes with almost 100% accuracy. The research team developed this technique by introducing a smart AI analysis method to an electrical-signal-based ultrasensitive biosensor.\n\nAs a noninvasive method, a diagnostic test using urine is convenient for patients and does not need invasive biopsy, thereby diagnosing cancer without side effects. However, as the concentration of cancer factors is low in urine, urine-based biosensors are only used for classifying risk groups rather than for precise diagnosis thus far.\n\nDr. Lee's team at the KIST has been working toward developing a technique for diagnosing disease from urine with an electrical-signal-based ultrasensitive biosensor. An approach using a single cancer factor associated with a cancer diagnosis was limited in increasing the diagnostic accuracy to over 90%. However, to overcome this limitation, the team simultaneously used different kinds of cancer factors instead of using only one to enhance the diagnostic accuracy innovatively.\n\nThe team developed an ultrasensitive semiconductor sensor system capable of simultaneously measuring trace amounts of four selected cancer factors in urine for diagnosing prostate cancer. They trained AI by using the correlation between the four cancer factors, which were obtained from the developed sensor. The trained AI algorithm was then used to identify those with prostate cancer by analyzing complex patterns of the detected signals. The diagnosis of prostate cancer by utilizing the AI analysis successfully detected 76 urinary samples with almost 100 percent accuracy.\n\n\"For patients who need surgery and/or treatments, cancer will be diagnosed with high accuracy by using urine to minimize unnecessary biopsy and treatments, which can dramatically reduce medical costs and medical staff's fatigue,\" Professor Jeong at Asan Medical Center said. \"This research developed a smart biosensor that can rapidly diagnose prostate cancer with almost 100 percent accuracy only through a urine test, and it can be further used in the precise diagnoses of other cancers via a urine test,\" Dr. Lee at the KIST said.\n\nExplore further Quit smoking, your bladder will thank you\n\nMore information: Hojun Kim et al, Noninvasive Precision Screening of Prostate Cancer by Urinary Multimarker Sensor and Artificial Intelligence Analysis, ACS Nano (2020). Journal information: ACS Nano Hojun Kim et al, Noninvasive Precision Screening of Prostate Cancer by Urinary Multimarker Sensor and Artificial Intelligence Analysis,(2020). DOI: 10.1021/acsnano.0c06946\n\nProvided by National Research Council of Science & Technology"
        },
        {
            "authors": [
                "Lee Mcintyre",
                "Posted On"
            ],
            "title": "How Cognitive Bias Can Explain Post-Truth",
            "contents": "How Cognitive Bias Can Explain Post-Truth\n\nOur built-in biases help explain our post-truth era, when \u201calternative facts\u201d replace actual facts, and feelings have more weight than evidence.\n\nStill from one of Leon Festinger's classic experiments on cognitive dissonance.\n\nBy: Lee McIntyre\n\nTo say that facts are less important than feelings in shaping our beliefs about empirical matters seems new, at least in American politics. In the past we have faced serious challenges \u2014 even to the notion of truth itself \u2014 but never before have such challenges been so openly embraced as a strategy for the political subordination of reality, which is how I define \u201cpost-truth.\u201d Here, \u201cpost\u201d is meant to indicate not so much the idea that we are \u201cpast\u201d truth in a temporal sense (as in \u201cpostwar\u201d) but in the sense that truth has been eclipsed by less important matters like ideology.\n\nOne of the deepest roots of post-truth has been with us the longest, for it has been wired into our brains over the history of human evolution: cognitive bias. Psychologists for decades have been performing experiments that show that we are not quite as rational as we think. Some of this work bears directly on how we react in the face of unexpected or uncomfortable truths.\n\nThis article is adapted from Lee McIntyre\u2019s book \u201cPost-Truth\u201c\n\nA central concept of human psychology is that we strive to avoid psychic discomfort. It is not a pleasant thing to think badly of oneself. Some psychologists call this \u201cego defense\u201d (after Freudian theory), but whether we frame it within this paradigm or not, the concept is clear. It just feels better for us to think that we are smart, well-informed, capable people than that we are not. What happens when we are confronted with information that suggests that something we believe is untrue? It creates psychological tension. How could I be an intelligent person yet believe a falsehood? Only the strongest egos can stand up very long under a withering assault of self-criticism: \u201cWhat a fool I was! The answer was right there in front of me the whole time, but I never bothered to look. I must be an idiot.\u201d So the tension is often resolved by changing one of one\u2019s beliefs.\n\nIt matters a great deal, however, which beliefs change. One would like to think that it should always be the belief that was shown to be mistaken. If we are wrong about a question of empirical reality \u2014 and we are finally confronted by the evidence \u2014 it would seem easiest to bring our beliefs back into harmony by changing the one that we now have good reason to doubt. But this is not always what happens. There are many ways to adjust a belief set, some rational and some not.\n\nThree Classic Findings from Social Psychology\n\nIn 1957, Leon Festinger published his pioneering book \u201cA Theory of Cognitive Dissonance,\u201d in which he offered the idea that we seek harmony between our beliefs, attitudes, and behavior, and experience psychic discomfort when they are out of balance. In seeking resolution, our primary goal is to preserve our sense of self-value.\n\nIn a typical experiment, Festinger gave subjects an extremely boring task, for which some were paid $1 and some were paid $20. After completing the task, subjects were requested to tell the person who would perform the task after them that it was enjoyable. Festinger found that subjects who had been paid $1 reported the task to be much more enjoyable than those who had been paid $20. Why? Because their ego was at stake. What kind of person would do a meaningless, useless task for just a dollar unless it was actually enjoyable? To reduce the dissonance, they altered their belief that the task had been boring (whereas those who were paid $20 were under no illusion as to why they had done it). In another experiment, Festinger had subjects hold protest signs for causes they did not actually believe in. Surprise! After doing so, subjects began to feel that the cause was actually a bit more worthy than they had initially thought.\n\nTo one degree or another, all of us suffer from cognitive dissonance.\n\nBut what happens when we have much more invested than just performing a boring task or holding a sign? What if we have taken a public stand on something, or even devoted our life to it, only to find out later that we\u2019ve been duped? Festinger analyzed just this phenomenon in a book called \u201cThe Doomsday Cult,\u201d in which he reported on the activities of a group called The Seekers, who believed that their leader, Dorothy Martin, could transcribe messages from space aliens who were coming to rescue them before the world ended on December 21, 1954. After selling all of their possessions, they waited on top of a mountain, only to find that the aliens never showed up (and of course the world never ended). The cognitive dissonance must have been tremendous. How did they resolve it? Dorothy Martin soon greeted them with a new message: Their faith and prayers had been so powerful that the aliens had decided to call off their plans. The Seekers had saved the world!\n\nFrom the outside, it is easy to dismiss these as the beliefs of gullible fools, yet in further experimental work by Festinger and others it was shown that \u2014 to one degree or another \u2014 all of us suffer from cognitive dissonance. When we join a health club that is too far away, we may justify the purchase by telling our friends that the workouts are so intense we only need to go once a week; when we fail to get the grade we\u2019d like in organic chemistry, we tell ourselves that we didn\u2019t really didn\u2019t want to go to medical school anyway. But there is another aspect of cognitive dissonance that should not be underestimated, which is that such \u201cirrational\u201d tendencies tend to be reinforced when we are surrounded by others who believe the same thing we do. If just one person had believed in the \u201cdoomsday cult\u201d perhaps he or she would have committed suicide or gone into hiding. But when a mistaken belief is shared by others, sometimes even the most incredible errors can be rationalized.\n\nIn his path-breaking 1955 paper \u201cOpinions and Social Pressure,\u201d Solomon Asch demonstrated that there is a social aspect to belief, such that we may discount even the evidence of our own senses if we think that our beliefs are not in harmony with those around us. In short, peer pressure works. Just as we seek to have harmony within our own beliefs, we also seek harmony with the beliefs of those around us.\n\nIn his experiment, Asch assembled seven to nine subjects, all of whom but one were \u201cconfederates\u201d (i.e., they were \u201cin on\u201d the deception that would occur in the experiment). The one who was not \u201cin on it\u201d was the sole experimental subject, who was always placed at the last seat at the table. The experiment involved showing the subjects a card with a line on it, then another card with three lines on it, one of which was identical in length to the one on the other card. The other two lines on the second card were \u201csubstantially different\u201d in length. The experimenter then went around the group and asked each subject to report aloud which of the three lines on the second card were equal in length to the line on the first card. For the first few trials, the confederates reported accurately and the experimental subject of course agreed with them. But then things got interesting. The confederates began to unanimously report that one of the obviously false choices was in fact equal to the length of the line on the first card. By the time the question came to the experimental subject, there was obvious psychic tension. As Asch describes it:\n\nHe is placed in a position in which, while he is actually giving the correct answers, he finds himself unexpectedly in a minority of one, opposed by a unanimous and arbitrary majority with respect to a clear and simple fact. Upon him we have brought to bear two opposed forces: the evidence of his senses and the unanimous opinion of a group of his peers.\n\nBefore announcing their answer, virtually all dissonance-primed subjects looked surprised, even incredulous. But then a funny thing happened. Thirty-seven percent of them yielded to the majority opinion. They discounted what they could see right in front of them in order to remain in conformity with the group.\n\nAnother piece of key experimental work on human irrationality was done in 1960 by Peter Cathcart Wason. In his paper \u201cOn the Failure to Eliminate Hypotheses in a Conceptual Task,\u201d Wason took the first in a number of steps to identify logical and other conceptual mistakes that humans routinely make in reasoning. In this first paper, he introduced (and later named) an idea that nearly everyone in the post-truth debate has likely heard of: confirmation bias.\n\nWason\u2019s experimental design was elegant. He gave 29 college students a cognitive task whereby they would be called on to \u201cdiscover a rule\u201d based on empirical evidence. Wason presented the subjects with a three-number series such as 2, 4, 6, and said that their task would be to try to discover the rule that had been used in generating it. Subjects were requested to write down their own set of three numbers, after which the experimenter would say whether their numbers conformed to the rule or not. Subjects could repeat this task as many times as they wished, but were instructed to try to discover the rule in as few trials as possible. No restrictions were placed on the sorts of numbers that could be proposed. When they felt ready, subjects could propose their rule.\n\nThe results were shocking. Out of 29 very intelligent subjects, only six of them proposed the correct rule without any previous incorrect guesses. Thirteen proposed one incorrect rule and nine proposed two or more incorrect rules. One subject was unable to propose any rule at all. What happened?\n\nAs Wason reports, the subjects who failed at the task seemed unwilling to propose any set of numbers that tested the accuracy of their hypothesized rule and instead proposed only those that would confirm it. For instance, given the series 2, 4, 6, many subjects first wrote down 8, 10, 12, and were told \u201cyes, this follows the rule.\u201d But then some just kept going with even numbers in ascending order by two. Rather than use their chance to see whether their intuitive rule of \u201cincrease by intervals of two\u201d was incorrect, they continued to propose only confirming instances. When these subjects announced their rule they were shocked to learn that it was incorrect, even though they had never tested it with any disconfirming instances.\n\nWhen a mistaken belief is shared by others, sometimes even the most incredible errors can be rationalized.\n\nAfter this, 13 subjects began to test their hypotheses and eventually arrived at the correct answer, which was \u201cany three numbers in ascending order.\u201d Once they had broken out of their \u201cconfirming\u201d mindset, they were willing to entertain the idea that there might be more than one way to get the original series of numbers. This cannot explain, however, the nine subjects who gave two or more incorrect rules, for they were given ample evidence that their proposal was incorrect, but still could not find the right answer. Why didn\u2019t they guess 9, 7, 5? Here Wason speculates that \u201cthey might not have known how to attempt to falsify a rule by themselves; or they might have known how to do it, but still found it simpler, more certain or more reassuring to get a straight answer from the experimenter.\u201d In other words, at this point their cognitive bias had a firm hold on them, and they could only flail for the right answer.\n\nAll three of these experimental results \u2014 (1) cognitive dissonance, (2) social conformity, and (3) confirmation bias \u2014 are obviously relevant to post-truth, whereby so many people seem prone to form their beliefs outside the norms of reason and good standards of evidence, in favor of accommodating their own intuitions or those of their peers.\n\nYet post-truth did not arise in the 1950s or even the 1960s. It awaited the perfect storm of a few other factors like extreme partisan bias and social media \u201csilos\u201d that arose in the early 2000s. And in the meantime, further stunning evidence of cognitive bias \u2014 in particular the \u201cbackfire effect\u201d and the \u201cDunning\u2013Kruger effect,\u201d both of which are rooted in the idea that what we hope to be true may color our perception of what actually is true \u2014 continued to come to light.\n\nImplications for Post-Truth\n\nIn the past, perhaps our cognitive biases were ameliorated by our interactions with others. It is ironic to think that in today\u2019s media deluge, we could perhaps be more isolated from contrary opinion than when our ancestors were forced to live and work among other members of their tribe, village, or community, who had to interact with one another to get information. When we are talking to one another, we can\u2019t help but be exposed to a diversity of views. And there is even empirical work that shows the value that this can have for our reasoning.\n\nIn his book \u201cInfotopia,\u201d Cass Sunstein has discussed the idea that when individuals interact they can sometimes reach a result that would have eluded them if each had acted alone. Call this the \u201cwhole is more than the sum of its parts\u201d effect. Sunstein calls it the \u201cinteractive group effect.\u201d\n\nWhen we open our ideas up to group scrutiny, this affords us the best chance of finding the right answer.\n\nIn one study, J. C. Wason and colleagues brought a group of subjects together to solve a logic puzzle. It was a hard one, and few of the subjects could do it on their own. But when the problem was later turned over to a group to solve, an interesting thing happened. People began to question one another\u2019s reasoning and think of things that were wrong with their hypotheses, to a degree they seemed incapable of doing with their own ideas. As a result, researchers found that in a significant number of cases a group could solve the problem even when none of its members alone could do so. (It is important to note that this was not due to the \u201csmartest person in the room\u201d phenomenon, where one person figured it out and told the group the answer. Also, it was not the mere \u201cwisdom of crowds\u201d effect, which relies on passive majority opinion. The effect was found only when group members interacted with one another.)\n\nFor Sunstein, this is key. Groups outperform individuals. And interactive, deliberative groups outperform passive ones. When we open our ideas up to group scrutiny, this affords us the best chance of finding the right answer. And when we are looking for the truth, critical thinking, skepticism, and subjecting our ideas to the scrutiny of others works better than anything else.\n\nYet these days we have the luxury of choosing our own selective interactions. Whatever our political persuasion, we can live in a \u201cnews silo\u201d if we care to. If we don\u2019t like someone\u2019s comments, we can unfriend him or hide him on Facebook. If we want to gorge on conspiracy theories, there is probably a radio station for us. These days more than ever, we can surround ourselves with people who already agree with us. And once we have done this, isn\u2019t there going to be further pressure to trim our opinions to fit the group?\n\nSolomon Asch\u2019s work has already shown that this is possible. If we are a liberal we will probably feel uncomfortable if we agree with most of our friends on immigration, gay marriage, and taxes, but are not so sure about gun control. If so, we will probably pay a social price that may alter our opinions. To the extent that this occurs not as a result of critical interaction but rather a desire not to offend our friends, this is likely not to be a good thing. Call it the dark side of the interactive group effect, which any of us who has ever served on a jury can probably describe: we just feel more comfortable when our views are in step with those of our compatriots. But what happens when our compatriots are wrong? Whether liberal or conservative, none of us has a monopoly on the truth.\n\nI am not here suggesting that we embrace false equivalence, or that the truth probably lies somewhere between political ideologies. The halfway point between truth and error is still error. But I am suggesting that at some level all ideologies are an enemy of the process by which truth is discovered. Perhaps researchers are right that liberals have a greater \u201cneed for cognition\u201d than conservatives, but that does not mean liberals should be smug or believe that their political instincts are a proxy for factual evidence. In the work of Festinger, Asch, and others, we can see the dangers of ideological conformity. The result is that we all have a built-in cognitive bias to agree with what others around us believe, even if the evidence before our eyes tells us otherwise. At some level we all value group acceptance, sometimes even over reality itself. But if we care about truth, we must fight against this. Why? Because cognitive biases are the perfect precursor for post-truth.\n\nIf we are already motivated to want to believe certain things, it doesn\u2019t take much to tip us over to believing them, especially if others we care about already do so. Our inherent cognitive biases make us ripe for manipulation and exploitation by those who have an agenda to push, especially if they can discredit all other sources of information. Just as there is no escape from cognitive bias, a news silo is no defense against post-truth. For the danger is that at some level they are connected. We are all beholden to our sources of information. But we are especially vulnerable when they tell us exactly what we want to hear.\n\nLee McIntyre is a Research Fellow at the Center for Philosophy and History of Science at Boston University. He is the author of several books, including \u201cDark Ages: The Case for a Science of Human Behavior,\u201d \u201cThe Scientific Attitude: Defending Science from Denial, Fraud, and Pseudoscience,\u201d and \u201cPost-Truth,\u201d from which this article is adapted.",
            "published_at": "2021-01-21T11:28:00+00:00"
        },
        {
            "authors": [
                "The Economist"
            ],
            "title": "Palace intrigue - The world must not accept the jailing of Alexei Navalny",
            "contents": "I N A DEMOCRACY the battle for power involves elections, media skirmishes and the occasional metaphorical stab in the back. In Russia it is literally a matter of life and death. To oppose President Vladimir Putin requires not only charisma and clear vision but also physical stamina and courage. Alexei Navalny possesses these qualities in abundance.\n\nListen to this story Your browser does not support the <audio> element. Enjoy more audio and podcasts on iOS or Android.\n\nThe Kremlin has tried hard to neutralise him. Prosecutors have levelled a series of trumped-up criminal charges against him. State propagandists have amplified them, and added imaginary calumnies to the mix. Last year Russia\u2019s security services slipped him a nerve agent in a botched attempt to murder him. Mr Putin no doubt hoped that after all this Mr Navalny would be scared into permanent exile. Instead, on January 17th, five months after falling into a coma and being evacuated to Germany on a stretcher, Mr Navalny boarded a low-cost airline called Pobeda (Victory) and flew back to Moscow.\n\nHe was grabbed at the border, spirited off to a police station and put on trial there at one minute\u2019s notice. The charge was violating parole\u2014while lying in a German hospital, he had been unable to check into a police station in Russia. He was found guilty, of course, and sentenced to 30 days\u2019 imprisonment. He awaits a second trial scheduled for February 2nd that could see him locked up for three and a half years and possibly much longer.\n\nYet still he torments his tormentor. On January 19th he released a two-hour film about Mr Putin\u2019s billion-dollar secret palace on the Black Sea, set on an estate 39 times larger than Monaco, with an underground ice-hockey rink, a casino and a red-velvet hookah lounge and dance pole. This was bought with \u201cthe biggest bribe in history\u201d, Mr Navalny\u2019s team claimed. (The Kremlin denies that the palace belongs to Mr Putin.) In less than a day the video had clocked up 20m views on YouTube.\n\nMr Navalny\u2019s conviction has plunged Russia to a new nadir of lawlessness. In the room where he was tried, there was even a picture on the wall of the head of Stalin\u2019s secret police. The violence unleashed by the Kremlin on its opponents is a threat not only to Mr Navalny, but also to ordinary Russians. A kleptocracy and repressive regime cannot go into reverse and requires new fodder to keep itself in power. What happens next depends largely on how the population and the elite respond. A nationwide protest called by Mr Navalny on January 23rd, ahead of his new trial, will be a critical test.\n\nA lawless Russia is a threat to the outside world, too. Repression at home is rarely isolated from aggression abroad, as Mr Putin has repeatedly shown. His arrest of Mr Navalny, who was treated as Angela Merkel\u2019s personal guest while he was in Germany, is a slap in the face for the German chancellor and the West. It also presents a challenge to the incoming administration of President Joe Biden who, unlike his predecessor, sees Mr Putin\u2019s Russia as one of the biggest threats to American security.\n\nMr Biden\u2019s incoming national security adviser, Jake Sullivan, set the right tone by issuing a sharply worded demand for Mr Navalny\u2019s release within hours of his arrest. But words are not enough. Mr Biden should lead a coalition to grapple with Russia\u2019s corruption and its Western enablers (see article). It should impose personal sanctions not only on Mr Putin\u2019s cronies and those responsible for poisoning and jailing Mr Navalny, but also on the much larger number of corrupt officials and politicians who have laundered or spent their ill-gotten wealth in the West in the past two decades of Mr Putin\u2019s rule.\n\nMr Navalny is risking his freedom and his life to stand up to a brutal, crooked regime. Mr Putin may command an army, the security services and a nuclear arsenal. But he is still afraid of the truth. Mr Navalny\u2019s courage has captured the world\u2019s imagination and put the Kremlin on the defensive. He deserves support. What becomes of him matters not only to Russia\u2014a vast, talented country captured by rapacious ex-spooks\u2014but to the world. \u25a0",
            "published_at": "2021-01-23T00:00:00"
        },
        {
            "authors": [
                "Reuters Staff"
            ],
            "title": "Elon Musk to offer $100 million prize for 'best' carbon capture tech",
            "contents": "FILE PHOTO: SpaceX founder and chief engineer Elon Musk speaks at a post-launch news conference to discuss the SpaceX Crew Dragon astronaut capsule in-flight abort test at the Kennedy Space Center in Cape Canaveral, Florida, U.S. January 19, 2020. REUTERS/Joe Skipper/File Photo\n\n(Reuters) - Tesla Inc chief and billionaire entrepreneur Elon Musk on Thursday took to Twitter to promise a $100 million prize for development of the \u201cbest\u201d technology to capture carbon dioxide emissions.\n\nCapturing planet-warming emissions is becoming a critical part of many plans to keep climate change in check, but very little progress has been made on the technology to date, with efforts focused on cutting emissions rather than taking carbon out of the air.\n\nThe International Energy Agency said late last year that a sharp rise in the deployment of carbon capture technology was needed if countries are to meet net-zero emissions targets.\n\n\u201cAm donating $100M towards a prize for best carbon capture technology,\u201d Musk wrote in a tweet, followed by a second tweet that promised \u201cDetails next week.\u201d\n\nTesla officials did not immediately respond to a request for additional information.\n\nMusk, who co-founded and sold Internet payments company PayPal Holdings Inc, now leads some of the most futuristic companies in the world.\n\nBesides Tesla, he heads rocket company SpaceX and Neuralink, a startup that is developing ultra-high bandwidth brain-machine interfaces to connect the human brain to computers.\n\nNewly-sworn-in U.S. President Joe Biden has pledged to accelerate the development of carbon capture technology as part of his sweeping plan to tackle climate change. On Thursday, he named Jennifer Wilcox, an expert in carbon removal technologies, as the principal deputy assistant secretary for fossil energy at the U.S. Department of Energy.",
            "published_at": "2021-01-22T00:39:09+00:00"
        },
        {
            "authors": [
                "Reshma Khilnani"
            ],
            "title": "How to Start a Biotech Company on a Budget",
            "contents": "Running a biotech company is capital intensive, and this can make it intimidating to get started. Founders sometimes face a chicken and egg problem: how do you make progress without millions of dollars in funding, and how do you raise millions of dollars in funding without having made progress?\n\nHowever, it is quite possible today to start a biotech company on a shoestring budget ($0-$200k starting costs). Many successful biotech companies started in just this way.\n\nThis post walks through three such stories, covering three biotech companies that are doing extremely well. Each example explains how they managed to get started and hit initial milestones with minimal funding or without any funding at all.\n\nSpecifically, these case studies will demonstrate answers to the following questions:\n\n1) Can you run initial experiments showing technical feasibility of your idea, like GeneWEAVE?\n\n2) If your target market will be very expensive to enter, can you repurpose your product to go after a smaller, more tractable market initially, like BillionToOne?\n\n3) Can you generate revenue upfront by providing services to other companies that are enabled by your technology, like Arpeggio?\n\nGeneWEAVE\n\nToday GeneWEAVE is a division of Roche, the world\u2019s largest biotech company and leading provider of in vitro diagnostics. GeneWEAVE\u2019s infectious disease diagnostic products are authorized by the FDA and based on technology that was acquired by Roche in 2015 for $425M. They have a unique diagnostic approach: while most use PCR based techniques to diagnose bacterial infections, GeneWEAVE uses phages (viruses that infect bacteria) as the detection mechanism. The main advantage of this approach is faster, easier and cheaper sample preparation that enables rapid results.\n\nThe founding of GeneWEAVE, however, is a case study on focus and scrappiness. Diego Rey, co-founder and CTO of GeneWEAVE needed to show that his phage based diagnostics could work. Intensely focused on demonstrating technical feasibility, he and his co-founders found $75K in grant money and hired a postdoc at Cornell to do the first proof-of-principle study. The study needed to demonstrate that the phage-based bacterial detection technique could work. The initial data took a year to collect, and the results were good. However, this initial data used a lab strain of bacteria and the team had yet to show that the technology could work with bacteria recovered from actual infections \u2014 or in their case, drug resistant staph (MRSA).\n\nAt this point, GeneWEAVE set up a chain of experiments, and used each experiment to raise another small round of funding. Their first investor check was for $150K and they used this to generate more data and convince another investor to also invest $150K. This led to the next experiment which got them another $300K and a final proof point that landed them another $300K. By now the team showed that the tech could work with bacteria found in the real world and directly from patient specimens. This was enough to convince Series A investors to fund the company with $12M.\n\nWhat\u2019s notable about GeneWEAVE is not only what they did, but what they did not do. They focused on their proof of principle, and were not distracted by setting up a facility or starting their FDA 510(k). They defined a first specific product (MRSA screening) and focused on producing key data that de-risked the technology and product one step at a time.\n\nBillionToOne (YC S17)\n\nBillionToOne makes diagnostic blood tests. They have raised over $30M and are working on a product to detect cancer using a blood test.\n\nThe BillionToOne blood test works using cell free DNA \u2014that is the DNA that is released into the bloodstream when cells die. Cancer cells have detectable mutations that shed into the bloodstream when the cells die. Theoretically, it would be possible to detect those mutations early via blood draw to determine the correct, personalized treatment for every cancer patient.\n\nWhen getting started, BillionToOne decided not to pursue cancer diagnostics as their first product. Instead, they used their core cell free DNA technology for non-invasive prenatal screening (i.e. determining disease in a fetus from a maternal blood sample), a market much easier to enter due to existing reimbursement rules. Similar to cancer cells, placental cells die and release fetal DNA into the mom\u2019s bloodstream. The technology that would look for cancer mutations instead would look for chromosomal abnormalities, as well as other disorders such as sickle cell anemia, cystic fibrosis, and spinal muscular atrophy mutations in the prenatal use case.\n\nCo-founder and CEO Oguzhan Atay and team joined YC in summer 2017 focused on building a prenatal test. Their prenatal focus was one of the key elements to launching on a budget because prenatal testing can be recommended by doctors and billed to insurance. Though cell free DNA technology can be used to diagnose cancer, the incentives are not yet aligned for a product of this type. For example, there isn\u2019t a CPT code to bill against for a \u201ccancer blood test\u201d for early detection or treatment monitoring, or an agreed upon criteria for testing patients with symptoms for an early diagnosis. Today, cancer screening is intriguing, but a company would have to raise a ton of money to prove diagnostic capability at the level needed to get reimbursed.\n\nBy focusing on prenatal diagnosis, a category with established care patterns and reimbursable by insurance, they could develop their technology and get paid in a year instead of a decade. To ship their first version, BillionToOne recruited physicians and scientists who were interested in sickle cell prenatal diagnosis, and worked with those physicians to run a clinical study to develop their laboratory developed test (LDT). Because those scientists were excited about the promise of the product, they were willing to do the study on a budget, and all-in-all BillionToOne was able to develop their product and complete their clinical studies for around $2M before raising their $15M Series A to set up their CLIA laboratory and commercialize their test.\n\nThe speed and savvy of the team was one of the factors that helped them raise their Series A in 2019.\n\nThere are many lessons in this early life of BillionToOne that is useful for those starting a bio company on a budget:\n\nFinding an initial product that\u2019s reimbursable and then taking the shortest path to get there.\n\nThinking carefully about your regulatory strategy; in this case pursuing CLIA/CAP first before FDA 510(k) clearance.\n\nIdentifying and exciting researchers and physicians who are passionate about a disease (in this case sickle cell disease), and working with them to get a study done in a timely and cost effective way.\n\nArpeggio Bio (YC S19)\n\nArpeggio Bio is a therapeutics company that is developing cancer treatments. They are designing oncology drugs that activate specific drug targets \u2014 increasing effectiveness and reducing side effects for a drug regimen. Their client list includes some of the biggest pharma companies, including Novartis.\n\nThe early story of Arpeggio is unique in that it was a bootstrapped biotech company. In the beginning, founder Joey Azofeifa was a PhD student who had a few years of pharma work under his belt. He built a platform (hardware, reagents and software) that gave pharma companies the ability to see what targets their drug compound hits and which ones it doesn\u2019t. Before, pharma companies would spend years and lots of money determining that 10% of patients don\u2019t respond to a treatment; with Arppeggio\u2019s tool, they could determine the response to treatments before human testing.\n\nArpeggio\u2019s first customer was one of Joey\u2019s former colleagues. The initial contract wasn\u2019t for technology, but rather a fee-for-service contract where Arpeggio was shipped drug compounds, and in the lab they profiled the compound and the changes it induced in 21k genes using their own tool. From there, the Arpeggio team cold messaged on LinkedIn and asked former colleagues for intros. Through this strategy, they reach close to $1M annual run rate, completely bootstrapped, before applying to YC in 2019.\n\nArpeggio used their own technology to do drug discovery, and sold it as \u201cconsulting\u201d to build credibility with investors and trust with a few initial clients.\n\nIn conclusion\n\nContrary to popular belief, it\u2019s possible to start a bio company on a budget. We hope that a few illustrations will encourage more founders to take their first steps."
        },
        {
            "authors": [],
            "title": "Ask HN: Is there a quick summary of how much tax various corporations pay?",
            "contents": "I think it would be insightful to see how much tax various corporations pay versus revenue. Especially when they provide their opinions on public policy and expect action / exemptions / allowances / subsidies. Anyone have any pointers?"
        },
        {
            "authors": [],
            "title": "zalo/CascadeStudio: A Full Live-Scripted CAD Kernel in the Browser",
            "contents": "A Full Live-Scripted CAD Kernel and IDE in the Browser.\n\nUse code to create 3D Models with features ranging from simple primitives + CSG to complex revolves, sweeps, and fillets. Cascade Studio exposes the full power of the OpenCascade kernel, while providing a concise standard library for simple operations.\n\nSave your completed models to .step, .stl. or .obj, or copy the url and share it with the community.\n\nFeatures\n\nA Powerful Standard Library to Simplify Model Construction\n\nIntellisense Autocomplete/AutoSuggest and Documentation\n\nAccess to the Full OpenCASCADE Kernel (via the oc. namespace)\n\nnamespace) Automatic Caching Acceleration of Standard Library Operations\n\n.STEP / .IGES / .STL Import - .STEP / .STL / .OBJ Export\n\n/ / Import - / / Export URL Serialization of code for easy sharing and ownership\n\nSave/Load Projects to preserve Code, Layout, and Imported Files\n\nIntegrated GUI System for Simple Customization\n\nEasily Installable for Offline-use as a Progressive Web App\n\nFree and Open Source under the MIT License\n\nExamples\n\nModel code is saved to the URL upon every successful evaluation, so you can copy and paste that link to others to view your model. Github Discussions is out of beta! So feel free to share your creations and examples here.\n\nContributing\n\nCascade Studio is entirely static assets and vanilla javascript, so running it locally is as simple as running a server from the root directory (such as the VS Code Live Server, Python live-server, or Node live-server ).\n\nPull Requests to this repo are automatically hosted to Vercel instances, so other users will be able to test and benefit from your modifications as soon as the PR is submitted.\n\nCredits\n\nCascade Studio uses:\n\nCascade Studio is maintained by Johnathon Selstad @zalo"
        },
        {
            "authors": [
                "Michael Andreuzza"
            ],
            "title": "Wicked Blocks",
            "contents": "12 Blocks\n\nHEADERS\n\nRight Headers.\n\nThis headers have text on the right side of the screen with one or two call to action, on the left side there's an image or other assets. You can use them as a hero or within sections."
        },
        {
            "authors": [],
            "title": "MuscleWiki",
            "contents": ""
        },
        {
            "authors": [
                "Us Department Of Commerce",
                "National Oceanic",
                "Atmospheric Administration"
            ],
            "title": "How far does sound travel in the ocean?",
            "contents": "Water temperature and pressure determine how far sound travels in the ocean.\n\nWhile sound moves at a much faster speed in the water than in air , the distance that sound waves travel is primarily dependent upon ocean temperature and pressure. While pressure continues to increase as ocean depth increases, the temperature of the ocean only decreases up to a certain point, after which it remains relatively stable. These factors have a curious effect on how (and how far) sound waves travel.\n\nImagine a whale is swimming through the ocean and calls out to its pod. The whale produces sound waves that move like ripples in the water. As the whale\u2019s sound waves travel through the water, their speed decreases with increasing depth (as the temperature drops), causing the sound waves to refract downward. Once the sound waves reach the bottom of what is known as the thermocline layer, the speed of sound reaches its minimum. The thermocline is a region characterized by rapid change in temperature and pressure which occurs at different depths around the world. Below the thermocline \"layer,\" the temperature remains constant, but pressure continues to increase. This causes the speed of sound to increase and makes the sound waves refract upward.\n\nThe area in the ocean where sound waves refract up and down is known as the \"sound channel.\" The channeling of sound waves allows sound to travel thousands of miles without the signal losing considerable energy. In fact, hydrophones, or underwater microphones, if placed at the proper depth, can pick up whale songs and manmade noises from many kilometers away."
        },
        {
            "authors": [],
            "title": "Stepping up for a truly open source Elasticsearch",
            "contents": "Last week, Elastic announced they will change their software licensing strategy, and will not release new versions of Elasticsearch and Kibana under the Apache License, Version 2.0 (ALv2). Instead, new versions of the software will be offered under the Elastic License (which limits how it can be used) or the Server Side Public License (which has requirements that make it unacceptable to many in the open source community). This means that Elasticsearch and Kibana will no longer be open source software. In order to ensure open source versions of both packages remain available and well supported, including in our own offerings, we are announcing today that AWS will step up to create and maintain a ALv2-licensed fork of open source Elasticsearch and Kibana.\n\nWhat this means for the Open Distro for Elasticsearch community\n\nWe launched Open Distro for Elasticsearch in 2019 to provide customers and developers with a fully featured Elasticsearch distribution that provides all of the freedoms of ALv2-licensed software. Open Distro for Elasticsearch is a 100% open source distribution that delivers functionality practically every Elasticsearch user or developer needs, including support for network encryption and access controls. In building Open Distro, we followed the recommended open source development practice of \u201cupstream first.\u201d All changes to Elasticsearch were sent as upstream pull requests (#42066, #42658, #43284, #43839, #53643, #57271, #59563, #61400, #64513), and we then included the \u201coss\u201d builds offered by Elastic in our distribution. This ensured that we were collaborating with the upstream developers and maintainers, and not creating a \u201cfork\u201d of the software.\n\nChoosing to fork a project is not a decision to be taken lightly, but it can be the right path forward when the needs of a community diverge\u2014as they have here. An important benefit of open source software is that when something like this happens, developers already have all the rights they need to pick up the work themselves, if they are sufficiently motivated. There are many success stories here, like Grafana emerging from a fork of Kibana 3.\n\nWhen AWS decides to offer a service based on an open source project, we ensure that we are equipped and prepared to maintain it ourselves if necessary. AWS brings years of experience working with these codebases, as well as making upstream code contributions to both Elasticsearch and Apache Lucene, the core search library that Elasticsearch is built on\u2014with more than 230 Lucene contributions in 2020 alone.\n\nOur forks of Elasticsearch and Kibana will be based on the latest ALv2-licensed codebases, version 7.10. We will publish new GitHub repositories in the next few weeks. In time, both will be included in the existing Open Distro distributions, replacing the ALv2 builds provided by Elastic. We\u2019re in this for the long haul, and will work in a way that fosters healthy and sustainable open source practices\u2014including implementing shared project governance with a community of contributors.\n\nWhat this means for Amazon Elasticsearch Service customers\n\nYou can rest assured that neither Elastic\u2019s license change, nor our decision to fork, will have any negative impact on the Amazon Elasticsearch Service (Amazon ES) you currently enjoy. Today, we offer 18 versions of Elasticsearch on Amazon ES, and none of these are affected by the license change.\n\nIn the future, Amazon ES will be powered by the new fork of Elasticsearch and Kibana. We will continue to deliver new features, fixes, and enhancements. We are committed to providing compatibility to eliminate any need to update your client or application code. Just as we do today, we will provide you with a seamless upgrade path to new versions of the software.\n\nThis change will not slow the velocity of enhancements we offer to our customers. If anything, a community-owned Elasticsearch codebase presents new opportunities for us to move faster in improving stability, scalability, resiliency, and performance.\n\nWhat this means for the open source community\n\nDevelopers embrace open source software for many reasons, perhaps the most important being the freedom to use that software where and how they wish.\n\nThe term \u201copen source\u201d has had a specific meaning since it was coined in 1998. Elastic\u2019s assertions that the SSPL is \u201cfree and open\u201d are misleading and wrong. They\u2019re trying to claim the benefits of open source, while chipping away at the very definition of open source itself. Their choice of SSPL belies this. SSPL is a non-open source license designed to look like an open source license, blurring the lines between the two. As the Fedora community states, \u201c[to] consider the SSPL to be \u2018Free\u2019 or \u2018Open Source\u2019 causes [a] shadow to be cast across all other licenses in the FOSS ecosystem.\u201d\n\nIn April 2018, when Elastic co-mingled their proprietary licensed software with the ALv2 code, they promised in \u201cWe Opened X-Pack\u201d: \u201cWe did not change the license of any of the Apache 2.0 code of Elasticsearch, Kibana, Beats, and Logstash \u2014 and we never will.\u201d Last week, after reneging on this promise, Elastic updated that same page with a footnote that says \u201ccircumstances have changed.\u201d\n\nElastic knows what they\u2019re doing is fishy. The community has told them this (e.g., see Brasseur, Quinn, DeVault, and Jacob). It\u2019s also why they felt the need to write an additional blustery blog (on top of their initial license change blog) to try to explain their actions as \u201cAWS made us do it.\u201d Most folks aren\u2019t fooled. We didn\u2019t make them do anything. They believe that restricting their license will lock others out of offering managed Elasticsearch services, which will let Elastic build a bigger business. Elastic has a right to change their license, but they should also step up and own their own decision.\n\nIn the meantime, we\u2019re excited about the long-term journey we\u2019ve embarked on with Open Distro for Elasticsearch. We look forward to providing a truly open source option for Elasticsearch and Kibana using the ALv2 license, and building and supporting this future with the community.\n\nAn earlier version of this post incorrectly indicated that the Jenkins CI tool was a fork. We thank @abayer for the correction.",
            "published_at": "2021-01-21T14:07:09-08:00"
        },
        {
            "authors": [
                "Scott Alexander"
            ],
            "title": "Still Alive",
            "contents": "I.\n\nThis was a triumph\n\nI'm making a note here, huge success\n\nNo, seriously, it was awful. I deleted my blog of 1,557 posts. I wanted to protect my privacy, but I ended up with articles about me in New Yorker, Reason, and The Daily Beast. I wanted to protect my anonymity, but I Streisand-Effected myself, and a bunch of trolls went around posting my real name everywhere they could find. I wanted to avoid losing my day job, but ended up quitting so they wouldn't be affected by the fallout. I lost a five-digit sum in advertising and Patreon fees. I accidentally sent about three hundred emails to each of five thousand people in the process of trying to put my blog back up.\n\nI had, not to mince words about it, a really weird year.\n\n513,000 people read my blog post complaining about the New York Times' attempt to dox me (for comparison, there are 366,000 people in Iceland). So many people cancelled their subscription that the Times' exasperated customer service agents started pre-empting callers with \"Is this about that blog thing?\" A friend of a friend reports her grandmother in Slovakia heard a story about me on Slovak-language radio.\n\nI got emails from no fewer than four New York Times journalists expressing sympathy and offering to explain their paper's standards in case that helped my cause. All four of them gave totally different explanations, disagreeing about whether the reporter I dealt with was just following the rules, was flagrantly violating the rules, was unaffected by any rules, or what. Seems like a fun place to work. I was nevertheless humbled by their support.\n\nI got an email from Balaji Srinivasan, a man whose anti-corporate-media crusade straddles a previously unrecognized border between endearing and terrifying. He had some very creative suggestions for how to deal with journalists. I'm not sure any of them were especially actionable, at least not while the Geneva Convention remains in effect. But it was still a good learning experience. In particular, I learned never to make an enemy of Balaji Srinivasan. I am humbled by his support.\n\nI got emails from two different prediction aggregators saying they would show they cared by opening markets into whether the Times would end up doxxing me or not. One of them ended up with a total trade volume in the four digits. For a brief moment, I probably had more advanced decision-making technology advising me in my stupid conflict with a newspaper than the CIA uses for some wars. I am humbled by their support.\n\nI got an email from a very angry man who believed I personally wrote the entirety of Slate.com. He told me I was a hypocrite for wanting privacy even though Slate.com had apparently published some privacy-violating stories. I tried to correct him, but it seemed like his email client only accepted replies from people on his contact list. I think this might be what the Catholics call \"invincible ignorance\". But, uh, I'm sure if we got a chance to sort it out I would have been humbled by his support.\n\nI got an email from a former member of the GamerGate movement, offering advice on managing PR. It was very thorough and they had obviously put a lot of effort into it, but it was all premised on this idea that GamerGate was some kind of shining PR success, even though as I remember it they managed to take a complaint about a video game review and mishandle it so badly that they literally got condemned by the UN General Assembly. But it's the thought that counts, and I am humbled by their support.\n\nI got an email from a Russian reader, which I will quote in full: \"In Russia we witnessed similar things back in 1917. 100 years later the same situation is in your country :)\". I am not sure it really makes sense to compare my attempted doxxing to the Bolshevik Revolution, and that smiley face will haunt my dreams, but I am humbled by his support.\n\nEventually it became kind of overwhelming. 7500 people signed a petition in my favor. Russia Today wrote an article about my situation as part of their propaganda campaign against the United States. Various tech figures started a campaign to stop granting interviews to NYT in protest. All of the humbling support kind of blended together. At my character level, I can only cast the spell Summon Entire Internet once per decade or so. So as I clicked through email after email, I asked myself: did I do the right thing?\n\nII.\n\nI'm not even angry\n\nI'm being so sincere right now\n\nBefore we go any further: your conspiracy theories are false. An SSC reader admitted to telling a New York Times reporter that SSC was interesting and he should write a story about it. The reporter pursued the story on his recommendation. It wasn't an attempt by the Times to crush a competitor, it wasn't retaliation for my having written some critical things about the news business, it wasn't even a political attempt to cancel me. Someone just told a reporter I would make a cool story, and the reporter went along with it.\n\nNor do I think it was going to be a hit piece, at least not at first. I heard from most of the people who the Times interviewed. They were mostly sympathetic sources, the interviewer asked mostly sympathetic questions, and someone who knows New York Times reporters says the guy on my case was their non-hit-piece guy; they have a different reporter for hatchet jobs. After I torched the blog in protest, they seem to have briefly flirted with turning it into a hit piece, and the following week they switched to interviewing everyone who hated me and asking a lot of leading questions about potentially bad things I did. My contacts in the news industry said even this wasn't necessarily sinister. They might have assumed I had something to hide, and wanted to figure out what it was just in case it was a better story than the original. Or they might have been deliberately interviewing friendly sources first, in order to make me feel safe so I would grant them an interview, and then moved on to the unfriendly ones after they knew that wouldn't happen. I'm not sure. But the pattern doesn't match \"hit piece from the beginning\".\n\nAs much crappy political stuff as there is in both the news industry and the blogsphere these days, I don't think this was a left-right political issue. I think the New York Times wanted to write a fairly boring article about me, but some guideline said they had to reveal subjects' real identities, if they knew them, unless the subject was in one of a few predefined sympathetic categories (eg sex workers). I did get to talk to a few sympathetic people from the Times, who were pretty confused about whether such a guideline existed, and certainly it's honored more in the breach than in the observance (eg Virgil Texas). But I still think the most likely explanation for what happened was that there was a rule sort of like that on the books, some departments and editors followed it more slavishly than others, and I had the bad luck to be assigned to a department and editor that followed it a lot. That's all. Anyway, they did the right thing and decided not to publish the article, so I have no remaining beef with them.\n\n(aside from the sorts of minor complaints that Rob Rhinehart expresses so eloquently here)\n\nI also owe the Times apologies for a few things I did while fighting them. In particular, when I told them I was going to delete the blog if they didn't promise not to dox me, I gave them so little warning that it probably felt like a bizarre ultimatum. At the time I was worried if I gave them more than a day's warning, they could just publish the story while I waited; later, people convinced me the Times is incapable of acting quickly and I could have let them think about it for longer.\n\nAlso, I asked you all to email an NYT tech editor with your complaints. I assumed NYT editors, like Presidents and Senators, had unlimited flunkies sorting through their mailbags, and would not be personally affected by any email deluge. I was wrong and I actually directed a three to four digit number of emails to the personal work inbox of some normal person with a finite number of flunkies. That was probably pretty harrowing and I'm sorry.\n\nAs for the Times' mistakes: I think they just didn't expect me to care about anonymity as much as I did. In fact, most of my supporters, and most of the savvy people giving me advice, didn't expect me to care as much as I did. Maybe I should explain more of my history here: back in the early 2010s I blogged under my real name. When I interviewed for my dream job in psychiatry, the interviewer had Googled my name, found my blog, and asked me some really pointed questions about whether having a blog meant I was irresponsible and unprofessional. There wasn't even anything controversial on the blog - this was back in the early 2010s, before they invented controversy. They were just old-school pre-social-media-era people who thought having a blog was fundamentally incompatible with the dignity of being a psychiatrist. I didn't get that job, nor several others I thought I was a shoo-in for. I actually failed my entire first year of ACGME match and was pretty close to having to give up on a medical career. At the time I felt like that would mean my life was over.\n\nSo I took a bunch of steps to be in a better position for the next year's round of interviews, and one of the most important was deleting that blog, scrubbing it off the Web as best I could, and restarting my whole online presence under a pseudonym. I was never able to completely erase myself from the Internet, but I made some strategic decisions - like leaving up a bunch of older stuff that mentioned my real name so that casual searchers would find that instead of my real blog. The next year, I tried the job interview circuit again and got hired.\n\nBut I still had this really strong sense that my career hung on this thread of staying anonymous. Sure, my security was terrible, and a few trolls and malefactors found my real name online and used it to taunt me. But my attendings and my future employers couldn't just Google my name and find it immediately. Also, my patients couldn't Google my name and find me immediately, which I was increasingly realizing the psychiatric community considered important. Therapists are supposed to be blank slates, available for patients to project their conflicts and fantasies upon. Their distant father, their abusive boyfriend, their whatever. They must not know you as a person. One of my more dedicated professors told me about how he used to have a picture of his children on a shelf in his office. One of his patients asked him whether those were his children. He described suddenly realizing that he had let his desire to show off overcome his duty as a psychiatrist, mumbling a noncommital response lest his patient learn whether he had children or not, taking the picture home with him that night, and never displaying any personal items in his office ever again. That guy was kind of an extreme case, but this is something all psychiatrists think about, and better pychiatrist-bloggers than I have quit once their side gig reached a point where their patients might hear about it. There was even a very nice and nuanced article about the phenomenon in - of all places - The New York Times.\n\nAfter all that, yeah, I had a phobia of being doxxed. But psychotherapy classes also teach you to not to let past traumas control your life even after they've stopped being relevant. Was I getting too worked up over an issue that no longer mattered?\n\nThe New York Times thought so. Some people kept me abreast of their private discussions (in Soviet America, newspaper's discussions get leaked to you!) and their reporters had spirited internal debates about whether I really needed anonymity. Sure, I'd gotten some death threats, but everyone gets death threats on the Internet, and I'd provided no proof mine were credible. Sure, I might get SWATted, but realistically that's a really scary fifteen seconds before the cops apologize and go away. Sure, my job was at risk, but I was a well-off person and could probably get another. Also, hadn't I blogged under my real name before? Hadn't I published papers under my real name in ways that a clever person could use to unmask my identity? Hadn't I played fast and loose with every form of opsec other than whether the average patient or employer could Google me in five seconds?\n\nSome of the savvy people giving me advice suggested I fight back against this. Release the exact death threats I'd received and explain why I thought they were scary. Play up exactly how many people lived with me and exactly why it would be traumatic for them to get SWATted. Explain exactly how seriously it would harm my patients if I lost my job. Say why it was necessary for my career to publish those papers under my real name.\n\nWhy didn't I do this? Partly because it wasn't true. I don't think I had particularly strong arguments on any of these points. The amount I dislike death threats is basically the average amount that the average person would dislike them. The amount I would dislike losing my job...and et cetera. Realistically, my anonymity let me feel safe and comfortable. But it probably wasn't literally necessary to keep me alive. I feel bad admitting this, like I conscripted you all into a crusade on false pretenses. Am I an entitled jerk for causing such a stir just so I can feel safe and comfortable? I'm sure the New York Times customer service representatives who had to deal with all your phone calls thought so.\n\nBut the other reason I didn't do it was...well, suppose Power comes up to you and says hey, I'm gonna kick you in the balls. And when you protest, they say they don't want to make anyone unsafe, so as long as you can prove that kicking you in the balls will cause long-term irrecoverable damage, they'll hold off. And you say, well, it'll hurt quite a lot. And they say that's subjective, they'll need a doctor's note proving you have a chronic pain condition like hyperalgesia or fibromyalgia. And you say fine, I guess I don't have those, but it might be dangerous. And they ask you if you're some sort of expert who can prove there's a high risk of organ rupture, and you have to admit the risk of organ rupture isn't exactly high. But also, they add, didn't you practice taekwondo in college? Isn't that the kind of sport where you can get kicked in the balls pretty easily? Sounds like you're not really that committed to this not-getting-kicked-in-the-balls thing.\n\nNo! There's no dignified way to answer any of these questions except \"fuck you\". Just don't kick me in the balls! It isn't rocket science! Don't kick me in the fucking balls!\n\nIn the New York Times' worldview, they start with the right to dox me, and I had to earn the right to remain anonymous by proving I'm the perfect sympathetic victim who satisfies all their criteria of victimhood. But in my worldview, I start with the right to anonymity, and they need to make an affirmative case for doxxing me. I admit I am not the perfect victim. The death threats against me are all by losers who probably don't know which side of a gun you shoot someone with. If anything happened at work, it would probably inconvenience me and my patients, but probably wouldn't literally kill either of us. Still! Don't kick me in the fucking balls!\n\nI don't think anyone at the Times bore me ill will, at least not originally. But somehow that just made it even more infuriating. In Street Fighter, the hero confronts the Big Bad about the time he destroyed her village. The Big Bad has destroyed so much stuff he doesn't even remember: \"For you, the day [I burned] your village was the most important day of your life. For me, it was Tuesday.\" That was the impression I got from the Times. They weren't hostile. I wasn't a target they were desperate to take out. The main emotion I was able to pick up from them was annoyance that I was making their lives harder by making a big deal out of this. For them, it was Tuesday.\n\nIt's bad enough to get kicked in the balls because Power hates you. But it's infuriating to have it happen because Power can't bring itself to care. So sure, deleting my blog wasn't the most, shall we say, rational response to the situation. But iterated games sometimes require a strategy that deviates from apparent first-level rationality, where you let yourself consider lose-lose options in order to influence an opponent's behavior.\n\nOr, in layman's terms, sometimes you have to be a crazy bastard so people won't walk all over you.\n\nIn 2010, a corrupt policewoman demanded a bribe from impoverished pushcart vendor Mohammed Bouazizi. He couldn't afford it. She confiscated his goods, insulted him, and (according to some sources) slapped him. He was humiliated and destitute and had no hope of ever getting back at a police officer. So he made the very reasonable decision to douse himself in gasoline and set himself on fire in the public square. One thing led to another, and eventually a mostly-peaceful revolution ousted the government of Tunisia. I am very sorry for Mr. Bouazizi and his family. But he did find a way to make the offending policewoman remember the day she harassed him as something other than Tuesday. As the saying goes, \"sometimes setting yourself on fire sheds light on the situation\".\n\nIII.\n\nAs I burned it hurt because\n\nI was so happy for you\n\nBut as I was thinking about all this, I got other emails. Not just the prediction aggregators and Russians and so on; emails of a totally different sort.\n\nI got emails from other people who had deleted their blogs out of fear. Sometimes it was because of a job search. Other times it was because of *gestures expansively at everything*. These people wanted me to know they sympathized with what I was going through.\n\nI got emails from people who hadn't deleted their blogs, but wished they had. A lot of them had stories like mine - failed an interview they should have aced, and the interviewer mentioned their blog as an issue. These people sympathized too.\n\nI got emails that were like that, only it was grad students. Apparently if you have a blog about your field, that can make it harder to get or keep a job in academia. I'm not sure what we think we're gaining by ensuring the smartest and best educated people around aren't able to talk openly about the fields they're experts in, but I hope it's worth it.\n\nI got an email from a far-left blogger with a similar story, which got me thinking about socialists in particular. Imagine you're writing a socialist blog - as is 100% your right in a democratic society. Aren't employers going to freak out as soon as they Google your name, expecting you to start a union or agitate for higher wages or seize the means of production or something? This is a totally different problem from the cancel culture stories I usually hear about, but just as serious. How are you supposed to write about communism in a world where any newspaper can just figure out your real name, expose you, and lock you out of most normal jobs?\n\nI got emails from some transgender bloggers, who talked about how trans people go by something other than their legal name and have a special interest in not getting outed in the national news. I don't think the Times would deliberately out trans people - probably there's some official policy against it. But the people emailing me understood that we're all in this together, and that if oppressed people don't stand up for the rights of the privileged, no one will. Or something. Man, it's been a weird year.\n\nI got an email telling me to look into the story of Richard Horton, a police officer in the UK. He wrote a blog about his experience on the force which was by all accounts incredible - it won the Orwell Prize for being the best political writing in Britain that year. The Times (a British newspaper unrelated to NYT) hacked his email and exposed his real identity, and his chief forced him to delete the blog in order to keep his job. I wonder whether maybe if police officers were allowed to write anonymously about what was going on without getting doxxed by newspapers, people wouldn't have to be so surprised every time something happens involving the police being bad. See for example The Impact Of The Cessation Of Blogs Within The UK Police Blogosphere, a paper somebody apparently needed to write.\n\nI got an email telling me to look into the story of Naomi Wu, a Chinese woman who makes videos about engineering and DIY tech projects under the name SexyCyborg. She granted an interview to a Vice reporter under the condition that he not reveal some sensitive details of her personal life which could get her in trouble with the Chinese authorities. Vice agreed, then revealed the details anyway (who could have guessed that a webzine founded by a violent neo-fascist leader and named after the abstract concept of evil would stoop so low?) In a Medium post, Wu wrote that \"Vice would endanger me for a few clicks because in Brooklyn certain things are no big deal...I had no possible recourse against a billion dollar company who thought titillating their readers with my personal details was worth putting me in jeopardy.\" She then went on to dox the Vice reporter involved, Which Was Morally Wrong And I Do Not Condone It - but also led to some interesting revelations about how much more journalists cared when it's one of their own and not just some vulnerable woman in a dictatorship.\n\nGetting all these emails made me realize that, whatever the merits of my own case, maybe by accident, I was fighting for something important here. Who am I? I'm nobody, I'm a science blogger with some bad opinions. But these people - the trans people, the union organizers, the police whistleblowers, the sexy cyborgs - the New York Times isn't worthy to wipe the dirt off their feet. How dare they assert the right to ruin these people's lives for a couple of extra bucks.\n\n...but I was also grateful to get some emails from journalists trying to help me understand the perspective of their field. They point out that reporting is fundamentally about revealing information that wasn't previously public, and hard-hitting reporting necessarily involves disclosing things about subjects that they would rather you not know. Speculating on the identities of people like Deep Throat, or Satoshi Nakamoto, or QAnon, or that guy who wrote Primary Colors, is a long-standing journalistic tradition, one I had never before thought to question. Many of my correspondents brought up that some important people read my blog (Paul Graham was the most cited name). Isn't there a point past which you stop being that-guy-with-a-Tumblr-account who it's wrong to dox, and you become more like Satoshi Nakamoto where trying to dox you is a sort of national sport? Wouldn't it be fair to say I had passed that point?\n\nWith all due respect to these reporters, and with complete admission of my own bias, I reject this entire way of looking at things. If someone wants to report that I'm a 30-something psychiatrist who lives in Oakland, California, that's fine, I've had it in my About page for years. If some reporter wants to investigate and confirm, I have some suggestions for how they could use their time better - isn't there still a war in Yemen? - but I'm not going to complain too loudly. But I don't think whatever claim the public has on me includes a right to know my name if I don't want them to. I don't think the public needs to know the name of the cops who write cop blogs, or the deadnames of trans people, or the dating lives of sexy cyborgs. I'm not even sure the public needs to know the name of Satoshi Nakamoto. If he isn't harming anyone, let him have his anonymity! I would rather we get whatever pathologies come from people being able to invent Bitcoin scot-free, than get whatever pathologies come from anyone being allowed to dox anyone else if they can argue that person is \"influential\". Most people don't start out trying to be influential. They just have a Tumblr or a LiveJournal or something, and a few people read it, and then a few more people read it, and bam! - they're influential! If influence takes away your protection, then none of us are safe - not the random grad student with a Twitter account making fun of bad science, not the teenager with a sex Tumblr, not the aspiring fashionista with an Instagram. I've read lots of interesting discussion on how much power tech oligarchs should or shouldn't be allowed to have. But this is the first time I've seen someone suggest their powers should include a magic privacy-destroying gaze, where just by looking at someone they can transform them into a different kind of citizen with fewer rights. Is Paul Graham some weird kind of basilisk, such that anyone he stares at too long turns into fair game?\n\nAnd: a recent poll found that 62% of people feel afraid to express their political beliefs. This isn't just conservatives - it's also moderates (64%), liberals (52%) and even many strong liberals (42%). This is true even among minority groups, with more Latinos (65%) feeling afraid to speak out than whites (64%), and blacks (49%) close behind. 32% of people worry they would be fired if their political views became generally known, including 28% of Democrats and 38% of Republicans. Poor people and Hispanics were more likely to express this concern than rich people and whites, but people with post-graduate degrees have it worse than any other demographic group.\n\nAnd the kicker is that these numbers are up almost ten percentage points from the last poll three years ago. The biggest decline in feeling safe was among \"strong liberals\", who feel an entire 12 percentage points less safe expressing their opinion now than way back in the hoary old days of 2017. What happens in a world where this trend continues? Does everyone eventually feel so unsafe that we completely abandon the public square to professional-opinion-havers, talking heads allowed to pontificate because they have the backing of giant institutions? What biases does that introduce to the discussion? And if we want to avoid that, is there any better way then a firm stance that people's online pseudonymity is a basic right, not to be challenged without one hell of a compelling public interest? Not just \"they got kinda big, so now we can destroy them guilt-free\", but an actual public interest?\n\nI'm not trying to convince the New York Times - obviously it would very much fit their business plan if we came to rely on professional-opinion-havers backed by big institutions. I'm trying to convince you, the average Internet person. For the first ten or twenty years of its history, the Internet had a robust norm against doxxing. You could troll people, you could Goatse or Rickroll them, but doxxing was beyond the pale. One of the veterans of this era is Lawrence Lessig, who I was delighted to see coming to my defense. We've lost a lot of that old Internet, sold our birthright to social media companies and content providers for a few spurts of dopamine, but I think this norm is still worth protecting.\n\nIf me setting myself on fire got the New York Times to rethink some of its policies, and accidentally helped some of these people win their own fights, it was totally worth it.\n\nIV.\n\nNow these points of data make a beautiful line\n\nAnd we're out of beta, we're releasing on time\n\nSo I'm glad I got burned\n\nThink of all the things we learned\n\nFor the people who are still alive\n\nThere's a scene in Tom Sawyer where Tom runs away from town and is presumed dead. He returns just as they're holding his funeral, and gets to listen to everyone praise his life and talk about how much they loved him. Seems like a good deal. Likewise, Garrison Keillor said that - since they say such nice things at people's funerals - it was a shame he was going to miss his own by just a few days.\n\nAfter deleting the blog I felt like I was attending my own funeral. I asked people to send the Times emails asking them not to publish the article. Some people ccd me on them. These weren't just \"Dear NYT, please do not dox this blogger, yours, John\". Some of them were a bit over-the-top. I believe a few of them may have used the words \"national treasure\". I can only hope the people at my real funeral are as kind.\n\nOther people just sent me the over-the-top emails directly. I got emails from people in far-away, very poor countries, telling me that there was nothing at all like a rationalist movement in their countries and my blog was how they kept up with the intellectual currents of a part of the world they might never see. I am humbled to be able to help them.\n\nI got emails from medical interns and residents, telling me they enjoyed hearing about my experiences in medicine. You guys only have like three minutes of free time a week, and I am humbled that you would spend some of it reading me.\n\nI got emails from people saying I was one of their inspirations for going into science academia. I am so, so, sorry. I am humbled by their continued support even after I ruined their lives.\n\nI got emails from people in a host of weird and difficult situations, telling me about how reading my blog was the only thing that kept them sane through difficult times. One woman insisted that I start blogging before she got pregnant again because I was her postpartum coping strategy. I hope I've made it in time - but in any case I am humbled by their support.\n\nI got emails from couples, saying that reading my blog together once a week was their romantic bonding activity. Again, I hope I've restarted in time, before anyone's had to divorce. They are very cute and I am humbled by their support.\n\nAnd more along the same lines, and some even more humbling than these. I want to grab some of you by the shoulders and shake you and shout \"IT'S JUST A BLOG, GET A LIFE\". But of course I would be a hypocrite. I remember back to when I was a new college graduate, desperately trying to make sense of the world. I remember the sheer relief when I came across a few bloggers - I most clearly remember Eliezer Yudkowsky - who seemed to be tuned exactly to my wavelength, people who were making sense when the entire rest of the world was saying vague fuzzy things that almost but not quite connected with the millions of questions I had about everything. These people weren't perfect, and they didn't have all the answers, but their existence reassured me that I wasn't crazy and I wasn't alone. I was an embarrassing fanboy of theirs for many years - I kind of still am - and if my punishment is to have embarassing fanboys of my own then I accept it as part of the circle of life.\n\nAnd also - I am maybe the worst person possible to argue that this doesn't matter. Almost everything good in my life I've gotten because of you. I met most of my friends through blogging. I met my housemates, who are basically my family right now, through blogging. I got introduced to my girlfriend by someone I know through blogging. My patients are doing better than they could be - some of them vastly better - because of things I learned from all of you in the process of blogging. Most of the intellectual progress I've made over the past ten years has been following up on leads people sent me because of my blogging. To the degree that the world makes sense to me, to the degree that I've been able to untie some of the thornier knots and be rewarded with the relief of mental clarity, a lot of it has been because of things I learned while blogging. However many over-the-top dubious claims you want to make about how much I have improved your life, I will one-up you with how much you have improved mine. And after reading a few hundred of your emails, I've realized, crystal-clear, that I am going to be spending the rest of my life trying to deserve even one percent of the love you've shown and the gifts you've given me.\n\nSo I've taken the steps I need to in order to feel comfortable revealing my real name online. I talked to an aggressively unhelpful police officer about my personal security. I got advice from people who are more famous than I am, who have allayed some fears and offered some suggestions. Some of the steps they take seem extreme - the Internet is a scarier place than I thought - but I've taken some of what they said to heart, rejected the rest in a calculated way, and realized realistically I was never that protected anyhow. So here we are.\n\nAnd I left my job. They were very nice about it, they were tentatively willing to try to make it work. But I just don't think I can do psychotherapy very well while I'm also a public figure, plus people were already calling them trying to get me fired and I didn't want to make them deal with more of that.\n\nAs I was trying to figure out how this was going to work financially, Substack convinced me that I could make decent money here. With that in place, I felt like I could also take a chance on starting my dream business. You guys have had to listen to me write ad nauseum about cost disease - why does health care cost 4x times more per capita than it did just a generation ago? I have a lot of theories about why that happened and how to fix it. But as Feynman put it, \"what I cannot create I cannot understand\". So I'm going to try to start a medical practice that provides great health care to uninsured people for 4x less than what anyone else charges. If it works, I plan to be insufferable about it. If it doesn't, I can at least have a fun conversation with Alex Tabarrok about where our theories went wrong. Since I'm no longer protecting my anonymity, I can advertise it here - Lorien Psychiatry - though I'm not currently accepting blog readers as patients, sorry.\n\nThat's taken up most of my time over the past six months. Going back to blog posts like this is a strange feeling. I wondered if I'd enjoy the break. I didn't particularly; it felt at least as much like trying to resist an addiction as it did resting from a difficult task. There's so much left to say! I never got the chance to tell you whether the SSC Survey found birth order effects to be biologically or socially mediated! And the predictive processing community is starting to really chip away at the question of why psychotherapies work - I need to explain this to someone else before I can be sure I understand it! I only discovered taxometrics a few months ago and I haven't talked your ears off about it yet - that will change! I made predictions about Trump - now that he's come and gone I need to grade them publicly so you can raise or lower your opinion of me as appropriate! And there's the book review contest! We are absolutely going to do the book review contest!\n\nSo here goes. With malice towards none, with charity towards all, with firmness in the \u1e5bta as reflective equilibrium gives us to see the \u1e5bta, let us restart our mutual explorations, begin anew the joyful reduction of uncertainty wherever it may lead us.\n\nMy name is Scott Siskind, and I love all of you so, so much.\n\n\n\nBut look at me, still talking when there's Science to do\n\nWhen I look out there it makes me glad I've got you\n\nI've experiments to run, there is research to be done\n\nOn the people who are still alive\n\nAnd believe me I am still alive\n\nI'm doing science and I'm still alive\n\nI feel fantastic and I'm still alive\n\nStill alive"
        },
        {
            "authors": [
                "Abc News",
                "Olga R. Rodriguez Associated Press",
                "January"
            ],
            "title": "Monarch butterfly population moves closer to extinction",
            "contents": "Monarch butterfly population moves closer to extinction Researchers say the number of western monarch butterflies wintering along the California coast has plummeted to a record low, putting the orange-and-black insects closer to extinction\n\nSAN FRANCISCO -- The number of western monarch butterflies wintering along the California coast has plummeted precipitously to a record low, putting the orange-and-black insects closer to extinction, researchers announced Tuesday.\n\nAn annual winter count by the Xerces Society recorded fewer than 2,000 butterflies, a massive decline from the tens of thousands tallied in recent years and the millions that clustered in trees from Northern California's Marin County to San Diego County in the south in the 1980s.\n\nWestern monarch butterflies head south from the Pacific Northwest to California each winter, returning to the same places and even the same trees, where they cluster to keep warm. The monarchs generally arrive in California at the beginning of November and spread across the country once warmer weather arrives in March.\n\nOn the eastern side of the Rocky Mountains, another monarch population travels from southern Canada and the northeastern United States across thousands of miles to spend the winter in central Mexico. Scientists estimate the monarch population in the eastern U.S. has fallen about 80% since the mid-1990s, but the drop-off in the western U.S. has been even steeper.\n\nThe Xerces Society, a nonprofit environmental organization that focuses on the conservation of invertebrates, recorded about 29,000 butterflies in its annual survey last winter. That was not much different than the tally the winter before, when an all-time low of 27,000 monarchs were counted.\n\nBut the count this year is dismal. At iconic monarch wintering sites in the city of Pacific Grove, volunteers didn\u2019t see a single butterfly this winter. Other well-known locations, such as Pismo State Beach Monarch Butterfly Grove and Natural Bridges State Park, only hosted a few hundred butterflies, researchers said.\n\n\u201cThese sites normally host thousands of butterflies, and their absence this year was heartbreaking for volunteers and visitors flocking to these locales hoping to catch a glimpse of the awe-inspiring clusters of monarch butterflies,\u201d said Sarina Jepsen, director of endangered species at the Xerces Society.\n\nScientists say the butterflies are at critically low levels in western states because of destruction to their milkweed habitat along their migratory route as housing expands into their territory and use of pesticides and herbicides increases.\n\nResearchers also have noted the effect of climate change. Along with farming, climate change is one of the main drivers of the monarch\u2019s threatened extinction, disrupting an annual 3,000-mile (4,828-kilometer) migration synched to springtime and the blossoming of wildflowers. Massive wildfires throughout the U.S. West last year may have influenced their breeding and migration, researchers said.\n\nA 2017 study by Washington State University researchers predicted that if the monarch population dropped below 30,000, the species would likely go extinct in the next few decades if nothing is done to save them.\n\nMonarch butterflies lack state and federal legal protection to keep their habitat from being destroyed or degraded. In December, federal officials declared the monarch butterfly \u201ca candidate\u201d for threatened or endangered status but said no action would be taken for several years because of the many other species awaiting that designation.\n\nThe Xerces Society said it will keep pursuing protection for the monarch and work with a wide variety of partners \u201cto implement science-based conservation actions urgently needed to help the iconic and beloved western monarch butterfly migration.\u201d\n\nPeople can help the colorful insects by planting early-blooming flowers and milkweed to fuel migrating monarchs on their paths to other states, the Xerces Society said."
        },
        {
            "authors": [
                "Astro Teller"
            ],
            "title": "Loon\u2019s final flight",
            "contents": "Loon\u2019s final flight\n\nWhen we unveiled Loon in June 2013, we meant everything in its name. It was a way-out-there and risky venture. Not just fragile-balloons-on-the-edge-of-space risky, but risky at the core of the question it was asking. Could this be the radical idea that might finally bring abundant, affordable Internet access, not just to the next billion, but to the last billion? To the last unconnected communities and those least able to pay?\n\nSadly, despite the team\u2019s groundbreaking technical achievements over the last 9 years \u2014 doing many things previously thought impossible, like precisely navigating balloons in the stratosphere, creating a mesh network in the sky, or developing balloons that can withstand the harsh conditions of the stratosphere for more than a year \u2014 the road to commercial viability has proven much longer and riskier than hoped. So we\u2019ve made the difficult decision to close down Loon. In the coming months, we\u2019ll begin winding down operations and it will no longer be an Other Bet within Alphabet.\n\nFrom the farmers in New Zealand who let us attach a balloon communications hub to their house in 2013, to our partners who made it possible to deliver essential connectivity to people following natural disasters in Puerto Rico and Peru, to our first commercial partners in Kenya, to the diverse organizations working tirelessly to find new ways to deliver connectivity from the stratosphere \u2014 thank you deeply. Loon wouldn\u2019t have been possible without a community of innovators and risk-takers who are as passionate as we are about connecting the unconnected. And we hope we have reason to work together again before long.",
            "published_at": "2021-01-22T00:12:50.112000+00:00"
        },
        {
            "authors": [],
            "title": "NHTSA releases final Low-Volume Manufacturing Rules",
            "contents": "National Highway Traffic Safety Administration (NHTSA) has completed a regulation permitting low volume motor vehicle manufacturers to begin selling replica cars that resemble vehicles produced at least 25 years ago. Congress enacted a DeLorean Motor Company-backed bill backed by the Specialty Equipment Market Association (SEMA) DeLorean Motor Company, and others into law in 2015, which streamlined requirements for small automakers, but implementation was delayed while awaiting the NHTSA regulations. Companies like DeLorean will now be able to apply for authorization to produce and sell vehicles under this program.\n\nThe recent release of the final rule document was unexpected, and we\u2019re very pleased to see it finally happen. Still, four years overdue with no clear idea of when (or if!) these would ever be released did certainly keep us from putting too many eggs in that stainless steel basket, so to speak.\n\nSome previous suppliers that we had lined up have gone out of business during the pandemic, others have been absorbed by larger companies that have made it clear low volume component production is not something they\u2019re interested in pursuing. In that regard there will be a fair amount of work to be re-done. Perhaps worse, some \u201cchampions\u201d we had at various suppliers have retired or moved on. In some cases this has left a void, where before there was a DeLorean fan, who rallied for us within their company and management.\n\nAdditionally, certain staffing candidates that were on our short-list have long since moved on in and while unemployment has increased during 2020, many of the specialized roles that we require are still hard to fill.\n\nAs mentioned before, in 2015 our planned engine had a life-cycle of emissions compliance through 2022. We had hoped to get into production by 2017 and get 3-4 years out of it before having to take on the engineering for a new powertrain. It\u2019s believed that this engine has been extended through perhaps 2024 now, but it doesn\u2019t seem like a good idea to plan around an engine so near its end-of-life.\n\nThat said, with EV\u2019s becoming more mainstream, we\u2019ve been considering switching to an all-electric as the future. It certainly makes for an easier path through emissions maze which still looms large over any internal combustion engine. While an electric Cobra or Morgan may be a little extreme for their potential market, we\u2019ve already seen that an EV DeLorean \u2013 as we displayed at the 2012 New York International Auto Show \u2013 is not such an \u201cout there\u201d idea.\n\nMost critically, financial markets have changed, and will change even more as the world navigates the continuing COVID crisis during the Biden administration. Will the financial support that we had lined up a few years ago to carry us through the final development and into production still be available?\n\nAs the automotive brand with likely the highest name recognition across all demographics in spite of not having a new product in 40 years, we still believe that none of the above is insurmountable and believe that others will see value in it, as well.\n\nStay tuned\u2026"
        },
        {
            "authors": [],
            "title": "Retiring Tucows Downloads",
            "contents": "A note to Tucows Downloads visitors:\n\nAll good things\u2026\n\nWe have made the difficult decision to retire the Tucows Downloads site. We\u2019re pleased to say that much of the software and other assets that made up the Tucows Downloads library have been transferred to our friends at the Internet Archive for posterity.\n\nThe shareware downloads bulletin board system (BBS) that would become Tucows Downloads was founded back in 1993 on a library computer in Flint, MI. What started as a place for people in the know to download software became the place to download software on the burgeoning Internet. Far more quickly than anyone could have imagined.\n\nA lot has changed since those early years. Tucows has grown and evolved as a business. It\u2019s been a long time since Tucows has been TUCOWS, which stood for The Ultimate Collection of Winsock Software.\n\nToday, Tucows is the second-largest domain name registrar in the world behind Go Daddy and the largest wholesaler of domain names in the world with customers like Shopify and other global website builder platforms. Hover offers domain names and email at retail to help people brand their life online. OpenSRS (and along the way our acquisitions of Enom, Ascio and EPAG) are the SaaS platforms upon which tens of thousands of customers have built their own domain registration businesses, registering tens of millions of domains on behalf of their customers. Ting Internet is building fiber-optic networks all over the U.S. At the same time, we\u2019re building the Mobile Services Enabler SaaS platform that is powering DISH\u2019s entry into the US mobile market.\n\nPoint is, we\u2019re keeping busy.\n\nFor the past several years, history, well sentimentality, has been the only reason to keep Tucows Downloads around. We talked about shutting the site down before. Most seriously in 2016 when instead, we decided to go ad-free, keeping the site up as a public service.\n\nToday is different. Tucows Downloads is old. Old sites are a maintenance challenge and therefore a risk. Maintaining the Tucows Downloads site pulls people away from the work that moves our businesses forward.\n\nTucows Downloads has had an incredible run. Retiring it is the right move but that doesn\u2019t alter the fact that it will always hold a special place in hearts and our story. We\u2019re thankful to the thousands of software developers who used Tucows Downloads to get their software in front of millions of people, driving billions of downloads over more than 25 years.\n\nThank you.\n\n\n\nSincerely,\n\nElliot Noss\n\nCEO, Tucows\n\nA note to Tucows Downloads Authors/Developers\n\nIf you\u2019re a developer who used the Tucows Author Resource Center (ARC) as part of your software dissemination, to buy code signing or other services, we\u2019re happy to help with the transition.\n\nAny certificates purchased through ARC remain valid. If you\u2019re looking to buy or renew code signing certificates, we invite you to go straight to the source; Sectigo was our supplier and will be happy to be yours too.\n\nFeel free to reach out to us at help@tucows.com if we can help with anything at all."
        },
        {
            "authors": [],
            "title": "r/wallstreetbets set to private after increased media coverage",
            "contents": "https://old.reddit.com/r/wallstreetbets/ Some unverified Twitter chatter talking about unwanted legal attention after pumping up GameStop stock: https://mobile.twitter.com/wsbmod"
        },
        {
            "authors": [],
            "title": "Bitwarden Help & Support",
            "contents": "On this page:\n\nBitwarden believes source code transparency is an absolute requirement for security solutions like ours. View full, detailed Release Notes in GitHub using any of the following links:\n\nRelease Announcements\n\nNote Dates on this page represent Cloud Server and Web releases. Bitwarden incrementally updates each client application (Desktop, Browser Extension, Mobile, etc.) following the initial release date to ensure feature efficacy and stability. As a result, client applications should expect listed features following the initial release.\n\n2021-01-19\n\nFor the first major release of 2021, the Bitwarden team combined multiple major enhancements to address the critical needs of all users, including:\n\nEmergency Access : Bitwarden\u2019s new Emergency Access feature enables users to designate and manage trusted emergency contacts, who may request access to their Vault in a Zero Knowledge/Zero Trust environment (see here for details).\n\n: Bitwarden\u2019s new Emergency Access feature enables users to designate and manage trusted emergency contacts, who may request access to their Vault in a Zero Knowledge/Zero Trust environment (see here for details). Encrypted Exports : Personal users and Organizations can now export Vault data in an encrypted .json file (see here for details).\n\n: Personal users and Organizations can now export Vault data in an encrypted file (see here for details). New Role : A Custom role is now available to allow for granular control over user permissions (see here for details).\n\n: A Custom role is now available to allow for granular control over user permissions (see here for details). New Enterprise Policy : The Personal Ownership policy is now available for use by Enterprise Organization (see here for details).\n\n: The Personal Ownership policy is now available for use by Enterprise Organization (see here for details). Biometric Unlock for Browser Extensions: Using an integration with a native Desktop application, you can now use Biometric input to unlock Firefox and Chromium-based Browser Extensions (see here for details).\n\n2020-11-12\n\nThe latest release of Bitwarden adds SSO-related enhancements to all client applications, including:\n\nNew Enterprise Policies: The Single Organization and Single Sign-On Authentication polices are now available for use by Enterprise Organizations (see here for details).\n\nThe Single Organization and Single Sign-On Authentication polices are now available for use by Enterprise Organizations (see here for details). API Key for CLI: Authenticate into the Bitwarden CLI using an API Key newly available from your Web Vault (see here for details).\n\nAuthenticate into the Bitwarden CLI using an API Key newly available from your Web Vault (see here for details). Improvements to SSO Onboarding: We\u2019ve made some improvements to the way users are onboarded via SSO to prevent potential security risks (see here for details).\n\nWe\u2019ve made some improvements to the way users are onboarded via SSO to prevent potential security risks (see here for details). GDPR Acknowledgement: From now on, new users of Bitwarden will be asked to acknowledge a Privacy Policy on registration.\n\nFrom now on, new users of Bitwarden will be asked to acknowledge a Privacy Policy on registration. Android 11 Inline Auto-fill: For devices using Android 11+, enabling the Auto-fill Service will display suggestions inline for IMEs that also support this feature (see here for details).\n\n2020-9-30\n\nThe latest release of Bitwarden adds much-anticipated Login with SSO functionality for all client applications, and the Business Portal for Web Vaults. Read this blog post for more information about Login with SSO, and refer to our documentation.\n\nEarly 2020 releases\n\nThe following items were released between March and September of 2020.",
            "published_at": "2021-01-19T00:00:00"
        },
        {
            "authors": [],
            "title": "The Next Gen Database Servers Powering Let's Encrypt",
            "contents": "Let\u2019s Encrypt helps to protect a huge portion of the Web by providing TLS certificates to more than 235 million websites. A database is at the heart of how Let\u2019s Encrypt manages certificate issuance. If this database isn\u2019t performing well enough, it can cause API errors and timeouts for our subscribers. Database performance is the single most critical factor in our ability to scale while meeting service level objectives. In late 2020, we upgraded our database servers and we\u2019ve been very happy with the results.\n\nWhat exactly are we doing with these servers?\n\nOur CA software, Boulder, uses MySQL-style schemas and queries to manage subscriber accounts and the entire certificate issuance process. It\u2019s designed to work with a single MySQL, MariaDB, or Percona database. We currently use MariaDB, with the InnoDB database engine.\n\nWe run the CA against a single database in order to minimize complexity. Minimizing complexity is good for security, reliability, and reducing maintenance burden. We have a number of replicas of the database active at any given time, and we direct some read operations to replica database servers to reduce load on the primary.\n\nOne consequence of this design is that our database machines need to be pretty powerful. Eventually we may need to shard or break the single database into multiple databases, but hardware advancements have allowed us to avoid that so far.\n\nHardware Specifications\n\nThe previous generation of database hardware was powerful but it was regularly being pushed to its limits. For the next generation, we wanted to more than double almost every performance metric in the same 2U form factor. In order to pull that off, we needed AMD EPYC chips and Dell\u2019s PowerEdge R7525 was ideal. Here are the specifications:\n\nPrevious Generation Next Generation CPU 2x Intel Xeon E5-2650\n\nTotal 24 cores / 48 threads 2x AMD EPYC 7542\n\nTotal 64 cores / 128 threads Memory 1TB 2400MT/s 2TB 3200MT/s Storage 24x 3.8TB Samsung PM883\n\nSATA SSD\n\n560/540 MB/s read/write 24x 6.4TB Intel P4610\n\nNVMe SSD\n\n3200/3200 MB/s read/write\n\nDell PowerEdge R7525 internals. The two silver rectangles in the middle are the CPUs. The RAM sticks, each 64GB, are above and below the CPUs. The 24x NVMe drives are in the front of the server, on the far left.\n\nBy going with AMD EPYC, we were able to get 64 physical CPU cores while keeping clock speeds high: 2.9GHz base with 3.4GHz boost. More importantly, EPYC provides 128 PCIe v4.0 lanes, which allows us to put 24 NVMe drives in a single machine. NVMe is incredibly fast (~5.7x faster than the SATA SSDs in our previous-gen database servers) because it uses PCIe instead of SATA. However, PCIe lanes are typically very limited: modern consumer chips typically have only 16 lanes, and Intel\u2019s Xeon chips have 48. By providing 128 PCI lanes per chip (v4.0, no less), AMD EPYC has made it possible to pack large numbers of NVMe drives into a single machine. We\u2019ll talk more about NVMe later.\n\nPerformance Impact\n\nWe\u2019ll start by looking at our median time to process a request because it best reflects subscribers\u2019 experience. Before the upgrade, we turned around the median API request in ~90 ms. The upgrade decimated that metric to ~9 ms!\n\nWe can clearly see how our old CPUs were reaching their limit. In the week before we upgraded our primary database server, its CPU usage (from /proc/stat) averaged over 90%:\n\nThe new AMD EPYC CPUs sit at about 25%. You can see in this graph where we promoted the new database server from replica (read-only) to primary (read/write) on September 15.\n\nThe upgrade greatly reduced our overall database latency. The average query response time (from INFORMATION_SCHEMA) used to be ~0.45ms.\n\nQueries now average three times faster, about 0.15ms.\n\nOpenZFS and NVMe\n\nNVMe drives are becoming increasingly popular because of their incredible performance. Up until recently, though, it was nearly impossible to get many of them in a single machine because NVMe uses PCIe lanes. Those were very limited: Intel\u2019s Xeon processors come with just 48 PCIe v3 lanes, and a number of those are used up by the chipset and add-on cards such as network adapters and GPUs. You can\u2019t fit many NVMe drives in the remaining lanes.\n\nAMD\u2019s latest generation of EPYC processors come with 128 PCIe lanes - more than double what Intel offers - and they\u2019re PCIe v4! This is enough to pack a 2U server full of NVMe drives (24 in our case).\n\nOnce you have a server full of NVMe drives, you have to decide how to manage them. Our previous generation of database servers used hardware RAID in a RAID-10 configuration, but there is no effective hardware RAID for NVMe, so we needed another solution. One option was software RAID (Linux mdraid), but we got several recommendations for OpenZFS and decided to give it a shot. We\u2019ve been very happy with it!\n\nThere wasn\u2019t a lot of information out there about how best to set up and optimize OpenZFS for a pool of NVMe drives and a database workload, so we want to share what we learned. You can find detailed information about our setup in this GitHub repository.\n\nConclusion\n\nThis database upgrade was necessary as more people rely on Let\u2019s Encrypt for the security and privacy that TLS/SSL provides. The equipment is quite expensive and it was a sizable undertaking for our SRE team to plan and execute the transition, but we gained a lot through the process.\n\nSupport Let\u2019s Encrypt\n\nWe depend on contributions from our supporters in order to provide our services. If your company or organization would like to sponsor Let\u2019s Encrypt please email us at sponsor@letsencrypt.org. We ask that you make an individual contribution if it is within your means.",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [
                "Posted On"
            ],
            "title": "Over 700,000 paintings from the Rijksmuseum online copyright free",
            "contents": "Amsterdam\u2019s Rijksmuseum has put over 700,000 digitised copies of its huge art collection online, and is making them available to reuse as public domain\n\nIt\u2019s not a new feature, but it\u2019s not that well known, and it was revamped last November. The images are being released under Creative Commons 1.0 Universal (CC0 1.0) Public Domain Dedication \u2013 which is essentially copyright and royalty free.\n\nThe Rijksstudio, as the online gallery is called was funded by the BankGiro Lottery, the Netherlands culture lottery that provides long-term support for institutions.\n\nYou can browse and search the Rijksstudio by genrea, dates or artists, and even if you\u2019re just browsing for pleasure, the website photos are of a high resolution quality.\n\nThe collection contains more than 2,000 paintings from the Dutch Golden Age by notable painters such as Jacob van Ruisdael, Frans Hals, Johannes Vermeer, Jan Steen, Rembrandt, and Rembrandt\u2019s pupils.\n\nEach of the paintings, photographs, and drawings they\u2019ve scanned has detailed information about the subject and the artist, along with some history such as when and where it was acquired.\n\nThe Rijksmuseum requires you to open an account on their website to download anything, but in exchange, the downloaded graphics are high resolution jpegs. You can even see brush strokes in some of the images I downloaded to test this.\n\nIn addition, professionals have an option to request a free TIFF file with colour reference and tailored advice.\n\nThe Rijksstudio, in English, is here.\n\nThe British Museum also released nearly 2 million images from their archive online last year.",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [
                "Kenny Kerr"
            ],
            "title": "microsoft/windows-rs: Rust for Windows",
            "contents": "Rust for Windows\n\nThe windows crate lets you call any Windows API past, present, and future using code generated on the fly directly from the metadata describing the API and right into your Rust package where you can call them as if they were just another Rust module.\n\nThe Rust language projection follows in the tradition established by C++/WinRT of building language projections for Windows using standard languages and compilers, providing a natural and idiomatic way for Rust developers to call Windows APIs.\n\nGetting started\n\nStart by adding the following to your Cargo.toml file:\n\n[ dependencies ] windows = \" 0.2.1 \" [ build-dependencies ] windows = \" 0.2.1 \"\n\nThis will allow Cargo to download, build, and cache Windows support as a package. Next, specify which types you need inside of a build.rs build script and the windows crate will generate the necessary bindings:\n\nfn main () { windows :: build! ( windows :: data :: xml :: dom :: * windows :: win32 :: system_services :: {CreateEventW, SetEvent, WaitForSingleObject} windows :: win32 :: windows_programming :: CloseHandle ); }\n\nFinally, make use of any Windows APIs as needed.\n\nmod bindings { :: windows :: include_bindings! (); } use bindings :: { windows :: data :: xml :: dom :: * , windows :: win32 :: system_services :: {CreateEventW, SetEvent, WaitForSingleObject}, windows :: win32 :: windows_programming :: CloseHandle, }; fn main () -> windows:: Result <()> { let doc = XmlDocument :: new ()?; doc. load_xml ( \"<html>hello world</html>\" )?; let root = doc. document_element ()?; assert! (root. node_name ()? == \"html\" ); assert! (root. inner_text ()? == \"hello world\" ); unsafe { let event = CreateEventW ( std :: ptr :: null_mut (), true . into (), false . into (), std :: ptr :: null (), ); SetEvent (event). ok ()?; WaitForSingleObject (event, 0 ); CloseHandle (event). ok ()?; } Ok (()) }\n\nTo reduce build time, use a bindings crate rather simply a module. This will allow Cargo to cache the results and build your project far more quickly.\n\nThere is an experimental documentation generator for the Windows API. The documentation is published here. This can be useful to figure out how the various Windows APIs map to Rust modules and which use paths you need to use from within the build macro.\n\nFor a more complete example, take a look at Robert Mikhayelyan's Minesweeper. More simple examples can be found here."
        },
        {
            "authors": [],
            "title": "Launch HN: Finmark (YC S20) \u2013 Financial planning software for startups",
            "contents": "Hi HN! I\u2019m Rami, cofounder of Finmark (https://finmark.com/). We make it simple for companies to manage their runway, hiring, and cash. Three years ago a bad financial model almost killed my last company. A miscalculation in our behemoth of a financial model led us to overestimate the amount of cash we had coming in, and in turn, to overspend. When we finally discovered the error after 5 months, we were forced to shift from aggressive hiring to aggressive cost cutting and had to lay off employees. We were able to course-correct and sold the company successfully in 2019, but I kept coming back to how difficult and time-consuming financial modeling is for founders. So after taking some time off to recharge post acquisition, I pulled my old team together to start Finmark. We built Finmark so all founders, not just finance pros, can easily create, update, and share custom financial models without having to spend weeks laboring over complex spreadsheets. While the flexibility spreadsheets afford is great, they are also susceptible to human error, challenging for collaboration and version control, and time-consuming to create and maintain. Finmark takes complex financial concepts and calculations, distills them down into a simple dashboard, and makes it easy for startups to create and share their financial plans, manage burn rate and cash, forecast revenue and expenses, and plan for fundraising Finmark is modular, so rather than using a template that might not be a great fit for your startup, you\u2019re building a completely custom model based on your individual business needs. Finmark integrates with your entire stack to reconcile your actuals without manual data entry. It is also very easy to create, compare, and share as many different scenarios as you\u2019d like, e.g. a base plan, upside plan, and downside plan. Since launching in Fall 2020 we\u2019ve had over 1,000 companies sign up for our waitlist, and have onboarded over 400 companies so far. Subscriptions start at $25/month and increase based on a company\u2019s monthly revenue. You can check out our pricing calculator here: https://finmark.com/pricing/ We are skipping the waitlist for anyone signing up from HN. Follow this link for a 30 day free trial: https://hubs.li/H0Fc7Ss0. We're looking forward to your comments and are eager to hear your feedback on Finmark and your experiences with financial modeling in general!"
        },
        {
            "authors": [
                "Itchy N Scratchy"
            ],
            "title": "Valetudo is a cloud-free web interface for robot vacuum cleaners",
            "contents": "In my review of Kyvol Cybovac S31 LDS smart robot vacuum cleaner, I noted that app permissions meant both the map of your house and GPS coordinates may be sent to the cloud. But it was pointed out to me that earlier LDS robot vacuum cleaners from Roborock/Xiaomi were supported by Valetudo project that removes the need to connect to the cloud, and implements a mobile-friendly web interface as well as MQTT support for integration with Home Assistant or Node-RED.\n\nThe project is not a custom firmware for the robots, and instead, the stock firmware is patched with Valetudo which developers describe as an \u201calternative App implementation + mock cloud which runs on the robot itself.\u201d\n\nThe good news is that you don\u2019t necessarily need to teardown your vacuum cleaner to root it and install Valetudo, but it will depend on the model, and manufacturing date/firmware installed. The less good news is that the process can still be complicated, and only works on a few models including:\n\nRoborock V1 (Xiaomi Mi SDJQR02RR) with pre-March 2020 and post-March 2020 models requiring different installation methods.\n\nRoborock S4, S5, S6\n\n3irobotix CRL-200S (Mijia STYJ02YM) also sold under the Viomi or Cecotec brand\n\nIf you\u2019re lucky you can update the firmware over OTA (WiFi) using some commands in a Linux terminal, or a web tool called Dustbuilder. In other cases, getting root access via the USB port, and for some models\u2026 well a full teardown is in order:\n\nOnce you\u2019ve done the update the Xiaomi app will not work anymore, and you\u2019d only access the robot vacuum cleaner via its web interface which, in most cases, comes with the same features as the mobile app minus cloud connectivity. However, if you change your mind, you can simply factory reset the device to remove Valetudo and continue with the Xiaomi app, at least on Roborock models.\n\nYou can find the full details and source code on Valetudo.cloud website and Github.",
            "published_at": "2021-01-20T00:00:00"
        },
        {
            "authors": [
                "Lennart Poettering"
            ],
            "title": "Unlocking LUKS2 volumes with TPM2, FIDO2, PKCS#11 Security Hardware on systemd 248",
            "contents": "TL;DR: It's now easy to unlock your LUKS2 volume with a FIDO2 security token (e.g. YubiKey or Nitrokey FIDO2). And TPM2 unlocking is easy now too.\n\nBlogging is a lot of work, and a lot less fun than hacking. I mostly focus on the latter because of that, but from time to time I guess stuff is just too interesting to not be blogged about. Hence here, finally, another blog story about exciting new features in systemd.\n\nWith the upcoming systemd v248 the systemd-cryptsetup component of systemd (which is responsible for assembling encrypted volumes during boot) gained direct support for unlocking encrypted storage with three types of security hardware:\n\nUnlocking with FIDO2 security tokens (well, at least with those which implement the hmac-secret extension, most do). i.e. your YubiKeys (series 5 and above), or Nitrokey FIDO2 and such. Unlocking with TPM2 security chips (pretty ubiquitous on non-budget PCs/laptops/\u2026) Unlocking with PKCS#11 security tokens, i.e. your smartcards and older YubiKeys (the ones that implement PIV). (Strictly speaking this was supported on older systemd already, but was a lot more \"manual\".)\n\nFor completeness' sake, let's keep in mind that the component also allows unlocking with these more traditional mechanisms:\n\nUnlocking interactively with a user-entered passphrase (i.e. the way most people probably already deploy it, supported since about forever) Unlocking via key file on disk (optionally on removable media plugged in at boot), supported since forever. Unlocking via a key acquired through trivial AF_UNIX / SOCK_STREAM socket IPC. (Also new in v248) Unlocking via recovery keys. These are pretty much the same thing as a regular passphrase (and in fact can be entered wherever a passphrase is requested) \u2014 the main difference being that they are always generated by the computer, and thus have guaranteed high entropy, typically higher than user-chosen passphrases. They are generated in a way they are easy to type, in many cases even if the local key map is misconfigured. (Also new in v248)\n\nIn this blog story, let's focus on the first three items, i.e. those that talk to specific types of hardware for implementing unlocking.\n\nTo make working with security tokens and TPM2 easy, a new, small tool was added to the systemd tool set: systemd-cryptenroll. It's only purpose is to make it easy to enroll your security token/chip of choice into an encrypted volume. It works with any LUKS2 volume, and embeds a tiny bit of meta-information into the LUKS2 header with parameters necessary for the unlock operation.\n\nUnlocking with FIDO2\n\nSo, let's see how this fits together in the FIDO2 case. Most likely this is what you want to use if you have one of these fancy FIDO2 tokens (which need to implement the hmac-secret extension, as mentioned). Let's say you already have your LUKS2 volume set up, and previously unlocked it with a simple passphrase. Plug in your token, and run:\n\n# systemd-cryptenroll --fido2-device = auto /dev/sda5\n\n(Replace /dev/sda5 with the underlying block device of your volume).\n\nThis will enroll the key as an additional way to unlock the volume, and embeds all necessary information for it in the LUKS2 volume header. Before we can unlock the volume with this at boot, we need to allow FIDO2 unlocking via /etc/crypttab . For that, find the right entry for your volume in that file, and edit it like so:\n\nmyvolume /dev/sda5 - fido2-device=auto\n\nReplace myvolume and /dev/sda5 with the right volume name, and underlying device of course. Key here is the fido2-device=auto option you need to add to the fourth column in the file. It tells systemd-cryptsetup to use the FIDO2 metadata now embedded in the LUKS2 header, wait for the FIDO2 token to be plugged in at boot (utilizing systemd-udevd , \u2026) and unlock the volume with it.\n\nAnd that's it already. Easy-peasy, no?\n\nNote that all of this doesn't modify the FIDO2 token itself in any way. Moreover you can enroll the same token in as many volumes as you like. Since all enrollment information is stored in the LUKS2 header (and not on the token) there are no bounds on any of this. (OK, well, admittedly, there's a cap on LUKS2 key slots per volume, i.e. you can't enroll more than a bunch of keys per volume.)\n\nUnlocking with PKCS#11\n\nLet's now have a closer look how the same works with a PKCS#11 compatible security token or smartcard. For this to work, you need a device that can store an RSA key pair. I figure most security tokens/smartcards that implement PIV qualify. How you actually get the keys onto the device might differ though. Here's how you do this for any YubiKey that implements the PIV feature:\n\n# ykman piv reset # ykman piv generate-key -a RSA2048 9d pubkey.pem # ykman piv generate-certificate --subject \"Knobelei\" 9d pubkey.pem # rm pubkey.pem\n\n(This chain of commands erases what was stored in PIV feature of your token before, be careful!)\n\nFor tokens/smartcards from other vendors a different series of commands might work. Once you have a key pair on it, you can enroll it with a LUKS2 volume like so:\n\n# systemd-cryptenroll --pkcs11-token-uri = auto /dev/sda5\n\nJust like the same command's invocation in the FIDO2 case this enrolls the security token as an additional way to unlock the volume, any passphrases you already have enrolled remain enrolled.\n\nFor the PKCS#11 case you need to edit your /etc/crypttab entry like this:\n\nmyvolume /dev/sda5 - pkcs11-uri=auto\n\nIf you have a security token that implements both PKCS#11 PIV and FIDO2 I'd probably enroll it as FIDO2 device, given it's the more contemporary, future-proof standard. Moreover, it requires no special preparation in order to get an RSA key onto the device: FIDO2 keys typically just work.\n\nUnlocking with TPM2\n\nMost modern (non-budget) PC hardware (and other kind of hardware too) nowadays comes with a TPM2 security chip. In many ways a TPM2 chip is a smartcard that is soldered onto the mainboard of your system. Unlike your usual USB-connected security tokens you thus cannot remove them from your PC, which means they address quite a different security scenario: they aren't immediately comparable to a physical key you can take with you that unlocks some door, but they are a key you leave at the door, but that refuses to be turned by anyone but you.\n\nEven though this sounds a lot weaker than the FIDO2/PKCS#11 model TPM2 still bring benefits for securing your systems: because the cryptographic key material stored in TPM2 devices cannot be extracted (at least that's the theory), if you bind your hard disk encryption to it, it means attackers cannot just copy your disk and analyze it offline \u2014 they always need access to the TPM2 chip too to have a chance to acquire the necessary cryptographic keys. Thus, they can still steal your whole PC and analyze it, but they cannot just copy the disk without you noticing and analyze the copy.\n\nMoreover, you can bind the ability to unlock the harddisk to specific software versions: for example you could say that only your trusted Fedora Linux can unlock the device, but not any arbitrary OS some hacker might boot from a USB stick they plugged in. Thus, if you trust your OS vendor, you can entrust storage unlocking to the vendor's OS together with your TPM2 device, and thus can be reasonably sure intruders cannot decrypt your data unless they both hack your OS vendor and steal/break your TPM2 chip.\n\nHere's how you enroll your LUKS2 volume with your TPM2 chip:\n\n# systemd-cryptenroll --tpm2-device = auto --tpm2-pcrs = 7 /dev/sda5\n\nThis looks almost as straightforward as the two earlier sytemd-cryptenroll command lines \u2014 if it wasn't for the --tpm2-pcrs= part. With that option you can specify to which TPM2 PCRs you want to bind the enrollment. TPM2 PCRs are a set of (typically 24) hash values that every TPM2 equipped system at boot calculates from all the software that is invoked during the boot sequence, in a secure, unfakable way (this is called \"measurement\"). If you bind unlocking to a specific value of a specific PCR you thus require the system has to follow the same sequence of software at boot to re-acquire the disk encryption key. Sounds complex? Well, that's because it is.\n\nFor now, let's see how we have to modify your /etc/crypttab to unlock via TPM2:\n\nmyvolume /dev/sda5 - tpm2-device=auto\n\nThis part is easy again: the tpm2-device= option is what tells systemd-cryptsetup to use the TPM2 metadata from the LUKS2 header and to wait for the TPM2 device to show up.\n\nBonus: Recovery Key Enrollment\n\nFIDO2, PKCS#11 and TPM2 security tokens and chips pair well with recovery keys: since you don't need to type in your password everyday anymore it makes sense to get rid of it, and instead enroll a high-entropy recovery key you then print out or scan off screen and store a safe, physical location. i.e. forget about good ol' passphrase-based unlocking, go for FIDO2 plus recovery key instead! Here's how you do it:\n\n# systemd-cryptenroll --recovery-key /dev/sda5\n\nThis will generate a key, enroll it in the LUKS2 volume, show it to you on screen and generate a QR code you may scan off screen if you like. The key has highest entropy, and can be entered wherever you can enter a passphrase. Because of that you don't have to modify /etc/crypttab to make the recovery key work.\n\nFuture\n\nThere's still plenty room for further improvement in all of this. In particular for the TPM2 case: what the text above doesn't really mention is that binding your encrypted volume unlocking to specific software versions (i.e. kernel + initrd + OS versions) actually sucks hard: if you naively update your system to newer versions you might lose access to your TPM2 enrolled keys (which isn't terrible, after all you did enroll a recovery key \u2014 right? \u2014 which you then can use to regain access). To solve this some more integration with distributions would be necessary: whenever they upgrade the system they'd have to make sure to enroll the TPM2 again \u2014 with the PCR hashes matching the new version. And whenever they remove an old version of the system they need to remove the old TPM2 enrollment. Alternatively TPM2 also knows a concept of signed PCR hash values. In this mode the distro could just ship a set of PCR signatures which would unlock the TPM2 keys. (But quite frankly I don't really see the point: whether you drop in a signature file on each system update, or enroll a new set of PCR hashes in the LUKS2 header doesn't make much of a difference). Either way, to make TPM2 enrollment smooth some more integration work with your distribution's system update mechanisms need to happen. And yes, because of this OS updating complexity the example above \u2014 where I referenced your trusty Fedora Linux \u2014 doesn't actually work IRL (yet? hopefully\u2026). Nothing updates the enrollment automatically after you initially enrolled it, hence after the first kernel/initrd update you have to manually re-enroll things again, and again, and again \u2026 after every update.\n\nThe TPM2 could also be used for other kinds of key policies, we might look into adding later too. For example, Windows uses TPM2 stuff to allow short (4 digits or so) \"PINs\" for unlocking the harddisk, i.e. kind of a low-entropy password you type in. The reason this is reasonably safe is that in this case the PIN is passed to the TPM2 which enforces that not more than some limited amount of unlock attempts may be made within some time frame, and that after too many attempts the PIN is invalidated altogether. Thus making dictionary attacks harder (which would normally be easier given the short length of the PINs).\n\nPostscript\n\n(BTW: Yubico sent me two YubiKeys for testing and Nitrokey a Nitrokey FIDO2, thank you! \u2014 That's why you see all those references to YubiKey/Nitrokey devices in the text above: it's the hardware I had to test this with. That said, I also tested the FIDO2 stuff with a SoloKey I bought, where it also worked fine. And yes, you!, other vendors!, who might be reading this, please send me your security tokens for free, too, and I might test things with them as well. No promises though. And I am not going to give them back, if you do, sorry. ;-))"
        },
        {
            "authors": [],
            "title": "DDoS-Guard To Forfeit Internet Space Occupied by Parler \u2014 Krebs on Security",
            "contents": "Parler, the beleaguered social network advertised as a \u201cfree speech\u201d alternative to Facebook and Twitter, has had a tough month. Apple and Google removed the Parler app from their stores, and Amazon blocked the platform from using its hosting services. Parler has since found a home in DDoS-Guard, a Russian digital infrastructure company. But now it appears DDoS-Guard is about to be relieved of more than two-thirds of the Internet address space the company leases to clients \u2014 including the Internet addresses currently occupied by Parler.\n\nThe pending disruption for DDoS-Guard and Parler comes compliments of Ron Guilmette, a researcher who has made it something of a personal mission to de-platform conspiracy theorist and far-right groups.\n\nIn October, a phone call from Guilmette to an Internet provider in Oregon was all it took to briefly sideline a vast network of sites tied to 8chan/8kun \u2014 a controversial online image board linked to several mass shootings \u2014 and QAnon, the far-right conspiracy theory which holds that a cabal of Satanic pedophiles is running a global child sex-trafficking ring and plotting against President Donald Trump. As a result, those QAnon and 8chan sites also ultimately ended up in the arms of DDoS-Guard.\n\nMuch like Internet infrastructure firm CloudFlare, DDoS-Guard typically doesn\u2019t host sites directly but instead acts as a go-between to simultaneously keep the real Internet addresses of its clients confidential and to protect them from crippling Distributed Denial-of-Service (DDoS) attacks.\n\nThe majority of DDoS-Guard\u2019s employees are based in Russia, but the company is actually incorporated in two other places: As \u201cCognitive Cloud LLP\u201d in Scotland, and as DDoS-Guard Corp. based in Belize. However, none of the company\u2019s employees are listed as based in Belize, and DDoS-Guard makes no mention of the Latin American region in its map of global operations.\n\nIn studying the more than 11,000 Internet addresses assigned to those two companies, Guilmette found that approximately 66 percent of them were doled out to the Belize entity by LACNIC, the regional Internet registry for the Latin American and Caribbean regions.\n\nSuspecting that DDoS-Guard incorporated in Belize on paper just to get huge swaths of IP addresses that are supposed to be given only to entities with a physical presence in the region, Guilmette filed a complaint with the Internet registry about his suspicions back in November.\n\nGuilmette said LACNIC told him it would investigate, and that any adjudication on the matter could take up to three months. But earlier this week, LACNIC published a notice on its website that it intends to revoke 8,192 IPv4 addresses from DDoS-Guard \u2014 including the Internet address currently assigned to Parler[.]com.\n\nLACNIC has not yet responded to requests for comment. The notice on its site says the Internet addresses are set to be revoked on Feb. 24.\n\nDDoS-Guard CEO Evgeniy Marchenko maintains the company has done nothing wrong, and that DDoS-Guard does indeed have a presence in Belize.\n\n\u201cThey were used strongly according [to] all LACNIC policies by [a] company legally substituted in LACNIC region,\u201d Marchenko said in an email to KrebsOnSecurity. \u201cThere is nothing illegal or extremist. We have employers and representatives in different countries around the world because we are global service. And Latin America region is not an exception.\u201d\n\nGuilmette said DDoS-Guard could respond by simply moving Parler and other sites sitting in those address ranges to another part of its network. But he considers it a victory nonetheless that a regional Internet registry took his concerns seriously.\n\n\u201cIt appeared to me that it was more probable than not that they got these 8,000+ IPv4 addresses by simply creating an arguably fraudulent shell company in Belize and then going cap in hand to LACNIC, claiming that they had a real presence in the Latin & South American region, and then asking for 8,000+ IPv4 addresses,\u201d he said. \u201cSo I reported my suspicions to the LACNIC authorities in early November, and as I have only just recently learned, the LACNIC authorities followed up diligently on my report and, it seems, verified my suspicions.\u201d\n\nIn October, KrebsOnSecurity covered another revelation by Guilmette about the same group of QAnon and 8chan-related sites that moved to DDoS-Guard: The companies that provided the Internet address space used by the sites were defunct businesses in the eyes of their respective U.S. state regulators. In other words, the American Registry for Internet Numbers (ARIN) \u2014 the non-profit which administers IP addresses for entities based in North America \u2014 was well within its contract rights to revoke the IP space.\n\nGuilmette brought his findings to ARIN, which declined to act on the complaint and instead referred the matter to state investigatory agencies.\n\nStill, Guilmette\u2019s gadfly efforts to stir things up in the RIR community sometimes do pay off. For example, he spent nearly three years documenting how $50 million worth of the increasingly scarce IPv4 addresses were misappropriated from African companies to dodgy Internet marketing firms.\n\nHis complaints about those findings to the African Network Information Centre (AFRINIC) resulted in an investigation that led to the termination of a top AFRINIC executive, who was found to have quietly sold many of the address blocks for personal gain to marketers based in Europe, Asia and elsewhere.\n\nAnd this week, AFRINIC took the unusual step of officially documenting the extent of the damage wrought by its former employee, and revoking discrete chunks of address space currently being used by marketing firms.\n\nIn a detailed report released today (PDF), AFRNIC said its investigation revealed more than 2.3 million IPv4 addresses were \u201cwithout any lawful authority, misappropriated from AFRINIC\u2019s pool of resources and attributed to organizations without any justification.\u201d\n\nAFRINIC said it began its inquiry in earnest back in March 2019, when it received an application by the U.S. Federal Bureau of Investigation (FBI) about \u201ccertain suspicious activities regarding several IPv4 address blocks which it held.\u201d So far, AFRNINIC said it has reclaimed roughly half of the wayward IP address blocks, with the remainder \u201cyet to be reclaimed due to ongoing due diligence.\u201d\n\nTags: 8chan, 8kun, AFRINIC, ARIN, Cognitive Cloud LLP, DDoS-Guard Corp., Evgeniy Marchenko, fbi, LACNIC, Parler, QAnon, Ron Guilmette"
        },
        {
            "authors": [],
            "title": "Careers - Relationship Hero",
            "contents": "NOTE: This site requires you to enable JavaScript."
        },
        {
            "authors": [
                "Hush Kit",
                "Written By"
            ],
            "title": "What is good and bad about the F-35 cockpit: A \u2018Panther\u2019 pilot\u2019s guide to modern cockpits",
            "contents": "The F-35 helmet: does it show too much too small?\n\nMy background \u2013 Current F-35 pilot and Weapons School graduate. I Have flown the Harrier II and F/A-18 Hornet operationally as well as instructing Tactics and Weapons training squadrons.\n\nI can\u2019t speak with much first-hand credibility about the fighters of the 50s-70s, nor can I tell you much about any twin-seat fighter aircraft. Probably the oldest cockpit I have flown in as captain was the BAe Hawk T1A in the RAF. It was totally \u2018steam driven\u2019 with no digital instrumentation, but as an advanced trainer of its generation it had everything you needed. Someone once told me that the gun/bombsight was the same as used in the Hawker Hurricane \u2013 whilst that may not actually be true it was certainly of a similar vintage! By the \u201990s it was definitely showing its age and the jump from Hawk T1 to any of the RAF\u2019s frontline aircraft was (avionics wise) too much.\n\nThe T2 (Hawk Mk 128) was introduced sometime in the 00\u2019s and was designed to mirror the Typhoon more or less exactly. It had three MFDs and a HUD and the radar simulator was pretty much an unclassified version of the Typhoon\u2019s radar. We found that students stepping from the T2 to Typhoon were coping so much better than those who had flown just the T1.\n\nHarrier II (GR7/9 AV-8B)\n\n\n\nI loved this cockpit, and to this day it remains my favourite \u2018office\u2019 of all I\u2019ve flown. Plenty of space, a huge canopy with excellent visibility and reasonably well laid out instruments. It was a bit of a crossover between analogue and digital; it had a good HUD and two MFDs with the classic 20 pushbuttons around the outside. The Up Front Controller (UFC) was easy to use and well located, it made entering co-ordinates during CAS easy. Something that has been lost in all glass cockpits is the tactile feel of pressing buttons and knowing you got a response \u2013 I found you could enter Lat/Longs by feel whilst looking out the window. This is something you definitely can\u2019t do \u2018on the glass\u2019 on current jets.\n\nThere was a lot of space taken up by the old analogue weapons control panel on the lower left, I\u2019ve got to say I never used it other than to flick switches when I was bored on a long transit. The 6-pack of analogue flight instruments were purely there as a failsafe, although I have to say I loved the standby Attitude Indicator. It didn\u2019t just tell you your attitude, but it also rotated and gave you a heading readout too \u2013 great for practice partial-panel approaches on your annual instrument rating checkride!\n\nAs for what made the jet unique, the nozzle lever. It was situated beside the throttle but was much smaller and of a different shape. We had it drilled into us during training that we had to be very sure which lever we were pulling in case we moved the wrong one. There was one crash during my time at an airshow on the South coast of England where the pilot moved the nozzles aft inadvertently when he should have moved the throttle to max. I will always remember the advice I was given by one of my instructors during the VSTOL phase of the Operational Conversion Unit \u2013 \u201cif you move something in the cockpit and the jet does something scary \u2013 move it back!\u201d\n\nA Harrier GR.9 aircraft conducts a combat patrol over Afghanistan Dec. 12, 2008. (U.S. Air Force photo by Staff Sgt. Aaron Allmon/Released)\n\n\n\nThe HOTAS was intuitive and fairly common with most F-teen series jets. I particularly liked the Throttle Designation Controller (TDC) being on the top of the throttle and operated by your thumb \u2013 all those years of practice on the Playstation controller paid off and made slewing the Sniper pod second nature!\n\nMcDonnell Douglas F/A-18A/C Hornet\n\nBeing a McDonnell-Douglas design I found this cockpit easy to convert to after the Harrier. It had an almost identical UFC and the same MFDs on the left and right, but this time with an additional larger MFD in the middle. It felt more cramped than the Harrier, certainly narrower but it was still well laid out.\n\nOf course this jet had two engines for the first time in my career but I didn\u2019t really notice a difference after a flight or two. It only got weird when you had one throttle off or at idle when dealing with an emergency; this was exacerbated if you had the right engine off and were flying on the left throttle as the push-to-talk switch was on the right throttle. It took a bit of dexterity at times to make sure you were pressing the correct switch.\n\nThe Hornet was a jet you could fly purely by feel, and indeed sound at times. The sound of the airflow over the LEX at high alpha was known as the \u2018waterfall\u2019. When flying BFM, if you were trying to out-rate your opponent you pulled back on the stick until it felt like the jet was driving over a cobblestone road. If you wanted to tighten the radius a little you would pull a little harder until you heard what sounds like \u2018standing at the top of a waterfall\u2019 \u2013 it\u2019s obvious when you hear it! When you needed to pull someone into your HUD for a guns kill you would bury the stick into your guts until it sounded like you were standing \u2018at the bottom of a waterfall\u2019. Talking of stick movements, I have never experienced such violent and aggressive movements of a control stick in an aircraft before, you would literally pull or push it to the stops as fast and as hard as you could. Definitely a jet built to take abuse.\n\nLockheed Martin F-35 Lightning II\n\nObviously I\u2019m limited in what I\u2019m allowed to tell you about this machine, but I\u2019ll stick to what is available in the public domain. First up, there\u2019s no HUD as its all integrated into the helmet. The technology of the helmet is great, but I\u2019d take a HUD any day. It all comes down to physics \u2013 you can only shrink things so much before they start to become degraded, and HUDs have bigger optics than helmets\u2026currently.\n\nThe Hush-Kit Book of Warplanes has the finest cuts from Hush-Kit along with exclusive new articles, explosive photography and gorgeous bespoke illustrations. Order The Hush-Kit Book of Warplanes here\n\n\n\nThe side-stick is something I thought would be difficult to convert to, but in all honesty it was a non-event. The rest of the cockpit is beautiful to look at \u2013 nothing analogue, all digital with about 10 actual switches in the cockpit. Notice I say beautiful to look at, not necessarily beautiful to interact with! In theory the all-glass display is great. It\u2019s touchscreen, you can set it up to show pretty much anything you want in any layout you want. Take, for example, a fuel display. You can have it in a large window that shows you everything you could possibly want to know about the aircraft\u2019s fuel system; the contents of each tank, which pumps are operating, fuel temperature, centre of gravity etc. Or you can shrink it into a smaller window that only shows more basic info. Or you don\u2019t even display it at all because the Function Access Buttons (FAB) along the top of the display always has a small fuel section with the essential info visible at all times. That\u2019s the beauty of the display \u2013 size and customisation. The drawback is in the complete lack of tactile response. It can be challenging to press the correct \u2018button\u2019 on the display whenever the jet is in motion as it is quite a bumpy ride at times. At present I am pressing the wrong part of the screen about 20% of the time in flight due to either mis-identification, or more commonly by my finger getting jostled around in turbulence or under G. One of the biggest drawbacks is that you can\u2019t brace your hand against anything whilst typing \u2013 think how much easier it is to type on a smartphone with your thumbs versus trying to stab at a virtual keyboard on a large tablet with just your index finger.\n\n\n\nVoice input is another feature of the jet, but not one I have found to be useful. It may work well on the ground in a test rig, but under G in flight it\u2019s not something I have found to work consistently enough to rely on. I haven\u2019t met anyone who uses it.\n\nAn F-35B Lightning II assigned to the United Kingdom\u2019s 617 Squadron taxis into position on the flight deck of HMS Queen Elizabeth at sea on 23 September, 2020. Marine Fighter Attack Squadron (VMFA) 211 \u201cThe Wake Island Avengers\u201d joined the United Kingdom\u2019s 617 Squadron \u201cThe Dambusters\u201d onboard the 65,000-ton carrier as she sailed for exercises with NATO allies in the North Sea.\n\n\n\nHaving bashed the interface, the way this jet displays information to you is incredible. The sheer amount of situational awareness I gain from this aircraft and its displays is like nothing I\u2019ve experienced before. The off-boresight helmet is much more accurate than legacy JHMCS systems and I find it clearer to read (although I still want a wide-angle HUD for flight and fight-critical data!). About the only thing missing from the whole cockpit is the lack of \u2018feel\u2019.\n\nSave the Hush-Kit blog. If you\u2019ve enjoyed an article you can donate here. Your donations keep this going. Thank you. Order The Hush-Kit Book of Warplanes here\n\nI was the first foreign pilot to fly the Mach 2.8 MiG-31 interceptor, here\u2019s my story\n\n\n\nHere\u2019s a new thing! An exclusive Hush-Kit newsletter delivered straight to your inbox. Hot aviation gossip, opinion, warplane technology updates, madcap history and other insights from the world of aviation by @Hush_Kit Sign up here\n\nYou can buy our t-shirts here",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [
                "Pablo Brudnick"
            ],
            "title": "rebar3_hank: The Erlang Dead Code Cleaner",
            "contents": "rebar3_hank: The Erlang Dead Code Cleaner Written by Pablo Brudnick , January 06, 2021 Written by, January 06, 2021\n\nFrom the creators of rebar3_format, here comes\u2026 rebar3_hank, a powerful but simple tool to detect dead code around your Erlang codebase (and kill it with fire!).\n\nDevelopers can use this rebar plugin in addition to a linter (Elvis), Xref, and Dialyzer; they complement each other perfectly.\n\n10 minute read\n\n\n\nHank Scorpio - Kill it with fire!\n\nIntroduction\n\nIn NextRoll\u2019s RTB Team, we have two passions while we maintain our codebases: killing dead code and automating things! That\u2019s why we thought that a tool for automating this process would be handy for the community and us. So, we started thinking more seriously about Hank , and we decided to spend our Winter HackWeek time to make it possible!\n\nNobody wants to maintain dead code. In fact, most of us are huge fans of negative PRs. Hank can help you with that by traversing your project, analyzing every .erl and .hrl file in it (optionally skipping some folders/files if you want to), applying the rules, and producing a list of all the code that you can effectively delete and/or refactor.\n\nThe best thing is that you can be sure that the dead code is, in fact, dead since Hank is built with Dialyzer levels of certainty\u2122\ufe0f.\n\nYou might be thinking: Why Hank? That\u2019s a job for my linter!\n\nThe answer is: No.\n\nWe use Elvis for code linting; it reviews our Erlang code style like function naming, nesting level, line length, variable naming convention, etc.\n\nHank doesn\u2019t do that.\n\nXref is a cross-reference tool that can be used for finding dependencies between functions, modules, applications, and releases. It does so by analyzing the defined functions and function calls. So it will warn us about a defined function that is never used around our source code.\n\nHank doesn\u2019t do that either.\n\nAnd Dialyzer? Dialyzer is a static analysis tool that identifies software discrepancies, such as success type errors, code that has become dead or unreachable because of a programming error, and unnecessary tests, among other things. It bases its analysis on the concept of success typings.\n\nHank does not rely on specs nor evaluates the \u201csemantics\u201d in functions params/returns.\n\nSo what exactly does Hank do?\n\nHank will detect and warn you about valid parts of your code that could potentially be deleted or at least refactored based on rules.\n\nIt works on entire projects (as opposed to Elvis, which works on individual files), on source code (as opposed to Xref, which works on compiled code), and on individual projects (as opposed to Dialyzer, which analyzes entire systems - including OTP and your dependencies).\n\nThe current version while writing this post is 0.2.1 . It\u2019s a minor version, but we\u2019re already using it in our systems, and it\u2019s practically ready for production usage.\n\nHow to use rebar3_hank\n\nJust add this to your rebar.config (either in your project or globally in ~/.config/rebar3/rebar.config ):\n\n{ plugins , [ rebar3_hank ]}.\n\nThen run\u2026\n\nrebar3 hank\n\n\u2026and kill it with fire!\n\nIgnoring rules\n\nThere are cases where you need to ignore some rules, like when developing libraries, where you can define hrls or modules which will be consumed by other projects. In those cases, you\u2019ll possibly need to ignore some rules (like single_use_hrl_attributes ). Something similar happens when using Xref.\n\nFor this purpose, you can ignore hank at the module level:\n\n% ignoring all the rules for this module - hank ignore % or ignoring specific rules - hank [ single_use_hrl_attributes ]\n\nOr add this configuration in your rebar.config :\n\n{ hank , [{ ignore , [ { \"test/*.erl\" , unused_ignored_function_params } ]}]}.\n\nThe Rules\n\nHere you can see the rules we\u2019ve already created, and you can use them with Hank directly.\n\nunused_ignored_function_params\n\nFunctions evolve, and some parameters that were used before may no longer be needed. A typical easy solution could be just ignoring them and forgetting about the issue.\n\nHank detects ignored parameters in the same position for all function clauses and lets you know that you can delete those parameters and refactor the places where the function is invoked, thus making your code cleaner. \ud83d\ude09\n\nFor instance, when analyzing this module\u2026\n\n- module ( my_module ). - export ([ external_fun / 1 ]). external_fun ( X ) -> multi_fun ( X , rand : uniform (), undefined ). %% A multi-clause function with unused 3rd param multi_fun ( undefined , _, _) -> ok ; multi_fun ( Arg1 , Arg2 , _ Arg3 ) when is_binary ( Arg1 ) -> Arg2 ; multi_fun ( Arg1 , _, _) -> Arg1 .\n\nHank will output\u2026\n\n$ rebar3 hank ===> Looking for code to kill with fire... ===> The following pieces of code are dead and should be removed: src/my_module.erl:9: Param #3 is not used at 'multi_fun/3'\n\nTo avoid this warning, remove the unused parameter(s).\n\nsingle_use_hrls\n\nSometimes you put some code in a header file that\u2019s supposed to be shared among multiple modules, but you end up writing just one module that uses it. In this case, it would be better to directly put the header file\u2019s contents in the module itself. And Hank has a rule for that!\n\nAssuming header.hrl:\n\n- define ( APP_HEADER , \"this is a header from an app that will be used in just one module\" ). - define ( SOME_MACRO ( A ), A ).\n\n- module ( app_include_lib ). - include ( \"header.hrl\" ). - export ([ my_function / 0 ]). my_function () -> % those are only used here! ? SOME_MACRO ( ? APP_HEADER ).\n\nIt will output:\n\n$ rebar3 hank ===> Looking for code to kill with fire... ===> The following pieces of code are dead and should be removed: header.hrl:0: This header file is only included at: src/app_include_lib.erl\n\nMove the hrl file\u2019s contents directly to the module that uses them, and you\u2019ll not see this warning again.\n\nSee a complete example here.\n\nsingle_use_hrl_attrs\n\nSometimes it\u2019s more subtle, tho. It\u2019s not that the whole file is used in just one module; it is shared among many modules. But some attributes (like macros or records) are not. They are defined in the header file but only used in a single module. Hank has a rule that will suggest you to place those attributes inside the module to limit the amount of stuff that\u2019s shared unnecessarily.\n\nGiven the previous files and including the hrl in another file:\n\n- module ( app_include_lib_2 ). - include ( \"header.hrl\" ).\n\nIt will output:\n\n$ rebar3 hank ===> Looking for code to kill with fire... ===> The following pieces of code are dead and should be removed: include/header.hrl:2: ?SOME_MACRO/1 is used only at src/app_include_lib.erl\n\nSee a complete example here\n\nunused_hrls\n\nSometimes the situation is even worse: You might have hrl files that are not included in any module. Hank will detect those and let you know that you can remove them entirely since they\u2019re virtually useless.\n\nAdding a header_2.hrl file which is not included, the output will be:\n\n$ rebar3 hank ===> Looking for code to kill with fire... ===> The following pieces of code are dead and should be removed: include/header_2.hrl:0: This file is unused\n\nSee an example here\n\nIt\u2019s worth mentioning that erlang-ls already provides a similar functionality.\n\nunused_macros\n\nHank also has a rule that will detect unused macros around the project. Those macros could be defined in any file within the source code but used in none of them. Therefore, they are effectively unnecessary and can be deleted.\n\nSee an example here\n\nunused_record_fields\n\nA fascinating one! With this rule, Hank will spot record declarations with fields that are defined (even giving them default values) but never used. Hank considers that you are using a record field when it is accessed or written.\n\nYou can use this warning to reduce your records\u2019 size by removing the unused fields from them.\n\nSee an example here\n\nExtensibility\n\nFollowing the lead of Elvis and rebar3_format, we built this project with extensibility in mind. Anybody can write their own rules for their projects by just implementing the hank_rule behavior.\n\nBut if you feel like sharing your new rules with the world, we are eager to get community contributions in the rebar3_hank GitHub! Check out the open issues, and feel free to open new ones! You can also use the discussions page to get in touch with us.\n\nTesting Hank\u2019s Power\n\nTo see how powerful Hank was, we decided to test it in a very large codebase.\n\nWe decided to try with Erlang/OTP itself. Since it\u2019s mainly composed by libraries, we had to limit the rules to apply to avoid some bogus results. We used this configuration:\n\n{ hank , [ { ignore , [ \"**/test/**\" ]}, %% Just \"production\" code, no tests { rules , [ unused_ignored_function_params , unused_hrls , unused_macros , unused_record_fields ]} ]}.\n\nWe hoped to find a large number of warnings, but never as large as what we found. Hank found more than 4000 pieces of dead code in OTP\u2019s production code (i.e., we didn\u2019t check the tests).\n\nIndeed, not all of them are supposed to be removed, but to give you a taste of the stuff that Hank found, check out the following warnings\u2026\n\nUnused Fields in Records\n\nHank found 130 unused fields in records, like this one in erl_tidy or remote_logger here.\n\nUnused Macros\n\nHank found more than 1000 unused macros in OTP, most of them in large modules of the megaco application and others like this one in xmerl_uri .\n\nUnused Parameters\n\nHank also found more than 2000 functions with unused params. Some of them are not actually errors, like this one that\u2019s masking a NIF function (Which will be fixed soon). But others are worth checking, like this non-exported function that never uses its first argument.",
            "published_at": "2021-01-06T00:00:00"
        },
        {
            "authors": [],
            "title": "Why isn't differential dataflow more popular?",
            "contents": "Differential dataflow is a library that lets you write simple dataflow programs and a) then runs them in parallel and b) efficiently updates the outputs when new inputs arrive. Compared to competition like spark and kafka streams, it can handle more complex computations and provides dramatically better throughput and latency while using much less memory.\n\nBut I'm only aware of a few companies that use it in production, even though it's been around for 5 years.\n\nPossible explanations:\n\nIt's missing some important feature, like persistence?\n\nIt's had very little advertising?\n\nThe api is too hard to use?\n\nThe docs / tutorials are not good enough?\n\nRust is intimidating?\n\nNo company to provide paid support?\n\nThese all seem plausible, but it's not clear which are most important.\n\nEven more surprising is that noone has copied the ideas into some enterprise friendly java monstrosity - despite the fact that differential dataflow is open source and is explained in depth in many papers and blog posts.\n\nI'm interested because materialize is expending a huge amount of effort adding a SQL layer on top of differential dataflow. That's all very well for people who like SQL, but I'm curious whether there are also potential users who would have been perfectly happy with javascript/python/R bindings and a good tutorial?\n\nIf you considered using differential dataflow and decided against, please let me know why."
        },
        {
            "authors": [
                "Dr. Ian Cutress"
            ],
            "title": "New Intel CEO Making Waves: Rehiring Retired CPU Architects",
            "contents": "We\u2019re following the state of play with Intel\u2019s new CEO, Pat Gelsinger, very closely. Even as an Intel employee for 30 years, rising to the rank of CTO, then taking 12 years away from the company, his arrival has been met with praise across the spectrum given his background and previous successes. He isn\u2019t even set to take his new role until February 15th, however his return is already causing a stir with Intel\u2019s current R&D teams.\n\nNews in the last 24 hours, based on public statements, states that former Intel Senior Fellow Glenn Hinton, who lists being the lead architect of Intel\u2019s Nehalem CPU core in his list of achievements, is coming out of retirement to re-join the company. (The other lead architect of Nehalem are Ronak Singhal and Per Hammerlund - Ronak is still at Intel, working on next-gen processors, while Per has been at Apple for five years.)\n\nHinton is an old Intel hand, with 35 years of experience, leading microarchitecture development of Pentium 4, one of three senior architects of Intel\u2019s P6 processor design (which led to Pentium Pro, P2, P3), and ultimately one of the drivers to Intel\u2019s Core architecture which is still at the forefront of Intel\u2019s portfolio today. He also a lead microarchitect for Intel\u2019s i960 CA, the world\u2019s first super-scalar microprocessor. Hinton holds more than 90+ patents from 8 CPU designs from his endeavors. Hinton spent another 10+ years at Intel after Nehalem, but Nehalem is listed in many places as his primary public achievement at Intel.\n\nOn his social media posts, Hinton states that he will be working on \u2018an exciting high performance CPU project\u2019. In the associated comments also states that \u2018if it wasn\u2019t a fun project I wouldn\u2019t have come back \u2013 as you know, retirement is pretty darn nice\u2019. Glenn also discloses that he has been pondering the move since November, and Gelsinger\u2019s re-hiring helped finalize that decision. His peers also opine that Glenn is probably not the only ex-Intel architect that might be heading back to the company. We know a few architects and specialists that have left Intel in recent years to join Intel's competitors, such as AMD and Apple.\n\nThere are a few key things to note here worth considering.\n\nFirst is that coming out of retirement for a big CPU project isn\u2019t a trivial thing, especially for an Intel Senior Fellow. Given Intel\u2019s successes, one would assume that the financial situation is not the main driver here, but the opportunity to work on something new and exciting. Plus, these sorts of projects take years of development, at least three, and thus Glenn is signing on for a long term despite already having left to retire.\n\nSecond point is reiterating that last line \u2013 whatever project Glenn is working on, it will be a long term project. Assuming that Glenn is talking about a fresh project within Intel\u2019s R&D ecosystem, it will be 3-5 years before we see the fruits of the labor, which also means creating a design aimed at what could be a variety of process node technologies. Glenn\u2019s expertise as lead architect is quite likely applicable for any stage of an Intel R&D design window, but is perhaps best served from the initial stages. The way Glenn seems to put it, this might be a black-ops style design. It also doesn't specify if this is x86, leaving that door open to speculation.\n\nThird here is to recognize that Intel has a number of processor design teams in-house and despite the manufacturing process delays, they haven\u2019t been idle. We\u2019ve been seeing refresh after refresh of Skylake lead Intel's portfolio, and while the first iterations of the 10nm Cove cores come to market, Intel\u2019s internal design teams would have been working on the next generation, and the next generation after that \u2013 the only barrier to deployment would have been manufacturing. I recall a discussion with Intel\u2019s engineers around Kaby Lake time, when I asked about Intel\u2019s progress on IPC \u2013 I requested a +10% gen-on-gen increase over the next two years at the time, and I was told that those designs were done and baked \u2013 they were already working on the ones beyond that. Those designs were likely Ice/Tiger Lake, and so Intel\u2019s core design teams have been surging ahead despite manufacturing issues, and I wonder if there\u2019s now a 3-4 year (or more) delay on some of these designs. If Glenn is hinting at a project beyond that, then we could be waiting even longer.\n\nFourth and finally, one of the critical elements listed by a number of analysts on the announcement of Gelsinger\u2019s arrival was that he wouldn\u2019t have much of an effect until 3+ years down the line, because of how product cycles work. I rejected that premise outright, stating that Pat can come in and change elements of Intel\u2019s culture immediately, and could sit in the room with the relevant engineers and discuss product design on a level that Bob Swan cannot. Pat has the opportunity to arrange the leadership structure and instill new confidence in those structures, some of which may have caused key architects in the past to retire, instead of build on exciting projects.\n\nAs we can see, Pat is already having an effect before his name is even on the door at HQ.\n\nToday is also Intel\u2019s end-of-year financial disclosure, at 5pm ET. We are expecting Intel\u2019s current CEO, Bob Swan, to talk through what looks to be another record breaking year of revenue, and likely the state of play for Intel's own 7nm process node technologies. That last point is somewhat thrown into doubt given the new CEO announcement and if Gelsinger is on the call. It is unknown if Gelsinger will participate.\n\nRelated Reading"
        },
        {
            "authors": [],
            "title": "Judge Refuses To Reinstate Parler After Amazon Shut It Down",
            "contents": "Judge Refuses To Reinstate Parler After Amazon Shut It Down\n\nEnlarge this image toggle caption Jaap Arriens/NurPhoto via Getty Images Jaap Arriens/NurPhoto via Getty Images\n\nUpdated at 5:50 p.m. ET\n\nA federal judge has refused to restore the social media site Parler after Amazon kicked the company off of its Web-hosting services over content seen as inciting violence.\n\nThe decision is a blow to Parler, an upstart that has won over Trump loyalists for its relatively hands-off approach to moderating content. The company sued Amazon over its ban, demanding reinstatement.\n\nU.S. District Judge Barbara Rothstein sided with Amazon, which argued that Parler would not take down posts threatening public safety even in the wake of the attack on the U.S. Capitol and that it is within Amazon's rights to punish the company over its refusal.\n\n\"The Court rejects any suggestion that the public interest favors requiring [Amazon Web Services] to host the incendiary speech that the record shows some of Parler's users have engaged in,\" Rothstein wrote on Thursday. \"At this stage, on the showing made thus far, neither the public interest nor the balance of equities favors granting an injunction in this case.\"\n\nParler's looser rules of engagement also attracted far-right activists among the some 15 million users who, the company says, posted messages before Amazon pulled the plug.\n\nThat anything-goes philosophy ran headlong into demands that social media platforms be held accountable for allowing rioters to discuss plans to storm the Capitol on the day Congress was certifying President Biden's election.\n\nShortly after the Jan. 6 attack, Parler began to feel the squeeze. First, Google and Apple banned it from their app stores, which made it nearly impossible to download the app. Then Amazon's Web-hosting services, Amazon Web Services, terminated Parler's account.\n\nParler filed a lawsuit, arguing that Amazon's crackdown was driven by \"political animus.\" Parler contended that the tech giant was abusing its power and attempting to kneecap a competitor.\n\nIn submissions to the court, Parler said Amazon's severing ties threatened Parler with \"extinction.\"\n\nAn attorney for Parler wrote that the last six Web hosts the company has approached have refused to work with the site.\n\nYet the website recently flicked back on as essentially no more than a welcome page. It promised to return soon with the message: \"We will not let civil discourse perish!\"\n\nJudge: Amazon doesn't have to host \"abusive, violent content\"\n\nIn defending against the suit, Amazon considered the matter a simple case of breach of contract. The company flagged dozens of posts advocating violence, which is against its policies, and Parler failed to remove the posts, according to Amazon's attorneys. The posts cited by Amazon include violent threats directed at Twitter's Jack Dorsey, Facebook's Mark Zuckerberg and leaders in the Democratic Party.\n\nIn defending its decision to boot Parler off its Web services, Amazon pointed to Section 230 of the Communications Decency Act, the much-debated 1996 federal law that prevents people from suing Internet companies over what users post.\n\nThe law also lets tech companies create and enforce rules over what is allowed and not allowed on their sites.\n\n\"That is precisely what AWS did here: removed access to content it considered 'excessively violent' and 'harassing,' \" attorneys for Amazon wrote in a submission to the court.\n\nIn her opinion, Rothstein agreed with Amazon, ruling that Parler's antitrust claim is \"dwindlingly slight\" and that the breach of contract argument \"failed.\" She wrote that it was Parler, not Amazon, that violated the terms of the contract.\n\nShe pointed to the rioters who stormed the Capitol and documented their violent acts on Parler.\n\n\"The Court explicitly rejects any suggestion that the balance of equities or the public interest favors obligating AWS to host the kind of abusive, violent content at issue in this case, particularly in light of the recent riots at the U.S. Capitol,\" Rothstein wrote. \"That event was a tragic reminder that inflammatory rhetoric can \u2014 more swiftly and easily than many of us would have hoped \u2014 turn a lawful protest into a violent insurrection.\"\n\nRothstein did not dismiss the lawsuit outright but rather rejected Parler's request for a preliminary injunction. That said, the decision does not bode well for the future of Parler's legal fight.\n\nParler is expected to appeal.\n\nIn a statement, Jeffrey Wernick, Parler's chief operating officer, said Rothstein not dismissing the case outright was notable. \"We remain confident that we will ultimately prevail in the main case,\" he said.\n\nMeanwhile, Parler is struggling to resuscitate its social network.\n\nDavid Groesbeck, a lawyer representing Parler, told the court that the company's hope that it could quickly find a new Web-hosting service has not come to fruition, creating a dire situation that Parler's CEO has said could spell the death of the site.\n\n\"The notoriety and fallout from the break-up have driven away current and potential business partners, utterly frustrating Parler's pre-termination plans to quickly replace and recover from AWS,\" Groesbeck wrote in a recent filing.\n\nParler, which is funded in part by Rebekah Mercer, a major donor of former President Donald Trump, has discussed housing its own servers and supporting its own Web hosting. Trump, too, floated the idea of launching his own social media service after Twitter permanently suspended him.\n\nDisinformation researchers said Amazon's shutdown of Parler eliminated a key gathering place for the sharing and discussion of the election-related conspiracies that Trump has often fanned.\n\n\"The reason why we're experiencing this corporate denial of service is because there are really no other levers possible to stop this group of people from reassembling and either trying this again or trying something else that's just as dangerous,\" said Joan Donovan, an expert on online extremism at Harvard. \"It's going to be really important that when they make these decisions, they stick and that they don't walk them back once the heat is off.\"\n\nA new focus on who controls \"the guts of the Web\"\n\nTo experts who study online speech and infrastructure, the predicament Parler finds itself in reveals just how much control over the Internet is vested in Web hosts, an out-of-sight part of the Web that has the power to decide which sites live or die.\n\n\"The guts of the Web that no one ever wants to see, or deal with, or think about\" is how Greg Falco, a cyber-risk management researcher at Stanford University, describes these service providers. \"It is critical infrastructure for our society, but it's been pushed behind a curtain.\"\n\nIn recent months, the biggest social media companies have drawn brighter lines around the limits of online free speech. And in the wake of the attack on the Capitol, they've taken uncharacteristically aggressive actions against groups and accounts that glorified the violence.\n\nBut, as the case of Parler shows, the pressure on social media companies to police the speech on their platforms is shared by Web-hosting companies.\n\n\"The question becomes tricky: When do you actually take someone down? It's a really gray territory,\" Falco said. \"The reality is, it comes down to understanding when it reaches some public attention, when there's actually some physical implications.\"\n\nIt is hard to find an example more stark than the insurrection on the Capitol, when droves of rioters turned to Parler and other alternative sites to post videos of vandalism, property damage and other violence, as ProPublica recently documented at length.\n\n\"When you have something that's outwardly violent or causes some other crisis or tragedy in the world, that's when Web infrastructure tends to come out of the shadows,\" said Dave Temkin, a former Netflix executive who oversaw the management of the company's servers.\n\nWeb-hosting companies, like social media platforms, address content in their terms of service. Violators can be punished.\n\nBack in 2018, GoDaddy, a major player in site-hosting, kicked Gab offline after it was revealed that the man accused of killing 11 people at a Pittsburgh synagogue had posted anti-Semitic messages to the site. Gab, which removed the suspect's account, came back online with the help of Epik, a company with links to the neo-Nazi website the Daily Stormer and theDonald.win, a far-right discussion board created after Reddit banned a forum popular with Trump's most ardent fans. The site recently rebranded as Patriot.win, and Epik supports its domain, the company confirmed.\n\nEvelyn Douek, a lecturer at Harvard Law School, predicts more battles over online speech will erupt between sites that choose a hands-off approach and Web hosts that demand a more aggressive stance. And that troubles her.\n\n\"Is that the right place for content moderation to be occurring?\" Douek asked. \"It's harder to bring accountability to those choices when we don't even know who's making them or why they're being made.\"\n\nIn other words, when a Web host has a problem with content on a client's site, usually these discussions are hashed out between the two parties, far from the public light. And Web hosts, unlike social media platforms, are not used to explaining these decisions publicly.\n\nAnother issue, Douek said, is the lack of oversight of Web hosts. She pointed to the 98 pieces of objectionable content Amazon cited in court papers about Parler.\n\n\"It sort of made me laugh a little bit,\" she said. \"Has Amazon read the rest of the Internet? Ninety-eight pieces of content or whatever is not that many. I mean, has Amazon read Amazon?\"\n\nEditor's note: Amazon is among NPR's recent financial supporters.",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [],
            "title": "pytoolz/toolz: A functional standard library for Python.",
            "contents": "Toolz\n\nA set of utility functions for iterators, functions, and dictionaries.\n\nSee the PyToolz documentation at https://toolz.readthedocs.io\n\nLICENSE\n\nNew BSD. See License File.\n\nInstall\n\ntoolz is on the Python Package Index (PyPI):\n\npip install toolz\n\nStructure and Heritage\n\ntoolz is implemented in three parts:\n\nitertoolz , for operations on iterables. Examples: groupby , unique , interpose ,\n\nfunctoolz , for higher-order functions. Examples: memoize , curry , compose ,\n\ndicttoolz , for operations on dictionaries. Examples: assoc , update-in , merge .\n\nThese functions come from the legacy of functional languages for list processing. They interoperate well to accomplish common complex tasks.\n\nRead our API Documentation for more details.\n\nExample\n\nThis builds a standard wordcount function from pieces within toolz :\n\n>> > def stem ( word ): ... \"\"\" Stem word to primitive form \"\"\" ... return word . lower (). rstrip ( \",.!:;'- \\\" \" ). lstrip ( \"' \\\" \" ) >> > from toolz import compose , frequencies , partial >> > from toolz . curried import map >> > wordcount = compose ( frequencies , map ( stem ), str . split ) >> > sentence = \"This cat jumped over this other cat!\" >> > wordcount ( sentence ) { 'this' : 2 , 'cat' : 2 , 'jumped' : 1 , 'over' : 1 , 'other' : 1 }\n\nDependencies\n\ntoolz supports Python 3.5+ with a common codebase. It is pure Python and requires no dependencies beyond the standard library.\n\nIt is, in short, a lightweight dependency.\n\nCyToolz\n\nThe toolz project has been reimplemented in Cython. The cytoolz project is a drop-in replacement for the Pure Python implementation. See CyToolz GitHub Page for more details.\n\nSee Also\n\nUnderscore.js: A similar library for JavaScript\n\nEnumerable: A similar library for Ruby\n\nClojure: A functional language whose standard library has several counterparts in toolz\n\nitertools: The Python standard library for iterator tools\n\nfunctools: The Python standard library for function tools\n\nContributions Welcome\n\ntoolz aims to be a repository for utility functions, particularly those that come from the functional programming and list processing traditions. We welcome contributions that fall within this scope.\n\nWe also try to keep the API small to keep toolz manageable. The ideal contribution is significantly different from existing functions and has precedent in a few other functional systems.\n\nPlease take a look at our issue page for contribution ideas.\n\nCommunity\n\nSee our mailing list. We're friendly."
        },
        {
            "authors": [],
            "title": "Share and accept documents securely",
            "contents": "SecureDrop is an open source whistleblower submission system that media organizations and NGOs can install to securely accept documents from anonymous sources. It was originally created by the late Aaron Swartz and is now managed by Freedom of the Press Foundation. SecureDrop is available in 20 languages.\n\n"
        },
        {
            "authors": [],
            "title": "Bertrand Might: Life, legacy and next steps",
            "contents": "Bertrand Might: Life, legacy & next steps\n\nDecember 9th 2007 - October 23rd 2020\n\nEstablished initially as a memorial after Bertrand \u201cBuddy\u201d Might\u2019s sudden passing on October 23rd 2020 at the age of 12, this page now serves to remember him as a person and also to coordinate and catalyze activities that honor his legacy, such as the newly formed Bertrand Might Endowment for Hope.\n\nIf you never knew Bertrand, there is a brief recap of his remarkable legislative, digital, communal, medical and scientific legacy and coverage of his life and final months by Casey Ross at STAT News.\n\nAs a step toward building his active legacy, as his father and in the spirit of the mission he gave me in life, I\u2019ve begun publishing articles on this site related to precision medicine.\n\nThe very first is The Algorithm for Precision Medicine. It is an overview of the process and the science that Bertrand taught me in his brief life on the road to diagnosis and to treatments. It is my hope that it will now benefit others.\n\nThis page includes:\n\nCauses in Bertrand\u2019s honor\n\nMany have asked about donating to a cause in Bertrand\u2019s memory.\n\nWe felt it best to offer several options, so that any of Bertrand\u2019s friends who wish to contribute to his legacy could find a facet of Bertrand\u2019s life that holds meaning for them.\n\nThe three main causes that we feel best embody Bertrand\u2019s spirit and character are:\n\nThere are many additional causes for which Bertrand actively advocated or from which he benefited in his life, and we have listed them as well.\n\nBertrand Might Endowment for Hope at UAB [Give here]\n\nWe are pleased to announce the creation of a permanent endowment at University of Alabama at Birmingham (UAB) \u2013 the Bertrand Might Endowment for Hope.\n\nWith Bertrand as our guide, our family experienced the power of hope through science.\n\nThe permanent nature of the endowment ensures that Bertrand\u2019s mission of helping individual patients will never come to an end.\n\nWe feel that giving at least one patient and family each year access to this kind of hope would be a meaningful way to honor this aspect of his legacy.\n\nThe purpose of these funds is to cover costs of individual patients in need of:\n\nadvanced diagnostics;\n\nresearch to identify novel therapeutic options where none exist; or\n\nan \u201cn = 1\u201d clinical trial for a single patient.\n\nInterest on the endowment will be reserved for science in the service of individual patients.\n\nOur goal is to raise $500,000 over the next year, and then as much as possible every year thereafter, so that more and more patients may benefit every year.\n\nThere are many patients for whom the next scientific step may only be $5,000, $10,000 or $25,000 \u2013 and yet this is often an insurmountable financial barrier for the family. Costs at this level could be covered by the endowment in perpetuity.\n\nThis fund creates a renewable source of hope for families like ours and patients like Bertrand, and it creates an active legacy in which his younger siblings will be able participate as it unfolds over their lifetimes.\n\nTo support the Bertrand Might Endowment for Hope, please give here.\n\nChristopher Kids: For spaces that serve children [Give here]\n\nBertrand's room, October 24th 2020\n\nBertrand was fortunate that he always had accessible, inclusive spaces.\n\nBertrand spent his final years in a house and bedroom that were designed for his needs.\n\nHis room was meant to accommodate his needs and the needs of his caregivers, and also to become the hub of activity for his siblings.\n\nIt worked: not only did it allow us to provide exceptional medical and daily care for him, but it also meant that Bertrand was constantly surrounded by family and friends engaged in activities in which he could share.\n\nIt was rare to walk past his room and not find his sister Victoria, his brother Winston or both sitting in his bed with him and watching TV or playing video games.\n\nRecognizing that not all families with children of special needs can be as fortunate, we strongly believe in the mission of Christopher for Kids, which seeks to create such spaces for families without the resources to do so.\n\nIt\u2019s a foundation run by the same architecture and design team that created Bertrand\u2019s room, bringing accessibility and inclusion to the homes of other families in need.\n\nFoundations for NGLY1 and Disorders of Glycosylation\n\nBertrand\u2019s genetic disorder, NGLY1 deficiency, was part of the broader space of glycosylation disorders.\n\nBertrand helped co-found The NGLY1 Foundation, which in the last months of his life was absorbed into the larger foundation covering all disorders of glycosylation \u2013 CDG Care.\n\nCDG Care will continue the patient support and scientific missions of the NGLY1 Foundation, including its active drug development project with NCATS at NIH and Retrophin.\n\nThe Grace Science Foundation [give here] is specifically dedicated to research for a cure to NGLY1 deficiency, and it is run by another pair of dedicated NGLY1 parents \u2013 Matt and Kristen Wilsey.\n\nThough we are saddened that Bertrand did not live to see the day that a cure reached him, we find it poetic, beautiful and inspiring that the day before Bertrand passed, Matt Wilsey (unaware at the time of Bertrand\u2019s condition) sent an email to the NGLY1 patient mailing list with the following update:\n\nI have great news to share after today's pre-IND meeting with the FDA. The discussion went exceptionally well. They were highly collaborative and informative. We received clear guidance on what we need to do over the coming months. Everything is doable. A significant gene therapy trial is on the horizon.\n\nBertrand\u2019s father tearfully yet joyfully read this email to him at his hospital bed, letting Bertrand know that his hope of a cure was at last within reach.\n\nIt is incredibly meaningful to us that before Bertrand passed, he knew that a cure for his fellow NGLY1 brothers and sisters was on the way.\n\nAdditional causes\n\nBertrand benefited from and advocated for many causes in his life, including:\n\nYou are welcome to place a comment on Bertrand\u2019s online obituary.\n\nYou can also leave a comment on this twitter thread:\n\nI'm struggling to find words: @bertrandmight passed last night. We are grateful for his 12 years with us -- 10 more than predicted long ago. His courage, his strength, his kindness, his cheer and his pure heart will always make him our inspiration. Our hero. pic.twitter.com/Gr18B15QCy \u2014 Matt Might (@mattmight) October 24, 2020\n\nArchived Celebration of Life\n\nBertrand\u2019s Celebration of Life service was on Sunday, November 1st 2020 at 3 PM Central at St. Luke\u2019s Episcopal Church in Mountain Brook, Alabama.\n\nFor the in-person service, due to social distancing constraints from the COVID19 pandemic, entry was limited, but it was livestreamed so that all could share in the celebration of Bertrand\u2019s life.\n\nHis memorial service is archived, with links to different parts here:\n\nAt the end, his siblings launched balloons in his memory:\n\nBertrand\u2019s life and legacy\n\nNote: We will be adding photos from across Bertrand\u2019s life here later. Please check back.\n\nBertrand was a beautiful, kind, loving, happy and gentle soul to all whom knew him.\n\nNot long after birth, Bertrand\u2019s struggles and diagnostic odyssey began, as he faced a constellation of symptoms: seizures, movement disorder, developmental delay and an inability to make tears.\n\nAt eight months old, Bertrand\u2019s developmental pediatrician concluded that something was wrong. It was the beginning of years of intensive medical evaluations. His medical team predicted that he would survive until age two or three at best.\n\nIn between frequent hospitalizations, Bertrand applied himself in his school and his therapy, always determined to exceed whatever limits his body had tried to impose on him: Bertrand danced; Bertrand climbed mountains; and Bertrand swam with dolphins. We\u2019re grateful for many happy memories with him.\n\nHis body may have suffered throughout his life from NGLY1 deficiency, but neither his heart nor his spirit ever diminished or lacked for love.\n\nThough with us for just less than thirteen years, Bertrand leaves behind a rich legacy:\n\n2007 \u2013 2011\n\nAt age one, Bertrand made his first pilgrimage for clinical evaluation at the NIH, visiting Dr. Constantine Stratakis for an evaluation of possible rare endocrine disorders.\n\nAt age two, Bertrand participated in a clinical trial for the use of autologous cord blood transfusion in the treatment of neurodegenerative disease at Duke University under Dr. Joanne Kurtzberg.\n\nBertrand traveled the country on a diagnostic odyssey seeking medical specialists that might have had any insight into what was causing his mysterious condition \u2013 or might have provided him any relief.\n\nFinding no answers, Bertrand enrolled in a second clinical trial at Duke University under Dr. Vandana Shashi and Dr. David Goldstein \u2013 a pilot study on the use of then-novel exome sequencing to solve difficult diagnostic odysseys.\n\n2012\n\nAt age four, the results of the exome sequencing trial were released \u2013 and Bertrand was diagnosed. The success of the trial in diagnosing patients whose diagnoses had been otherwise intractable helped pave the way for the now widespread use of clinical genomic sequencing.\n\nIn that same effort, Bertrand also became the first patient ever diagnosed with the ultra-rare genetic disorder NGLY1 deficiency, effectively discovering a new disease in the process of receiving his diagnosis.\n\nA month later, Bertrand became the first patient to use social media in the discovery of a patient community for a novel disorder, as Bertrand\u2019s life story went viral.\n\n2013\n\nAt age five, by having a confirmed genetic diagnosis, Bertrand gave the gift of both health and life to his brother-yet-to-be Winston by ensuring that he would never suffer from NGLY1 deficiency.\n\n2014\n\nAt age six, Bertrand participated in the first patient-clinician-scientist summit for NGLY1 deficiency at Dr. Hudson Freeze\u2019s lab at what was then Sanford-Burnham.\n\nLater that summer, Bertrand became the first NGLY1 patient to enroll in the NIH Natural History Study for Disorders of Glycosylation under Dr. Bill Gahl, Lynne Wolfe, Dr. Christina Lam and Dr. Carlos Ferreira.\n\nShortly thereafter, Seth Mnookin\u2019s article in The New Yorker chronicled Bertrand\u2019s journey from undiagnosed to an \u201cn of 1\u201d to founder of an entire patient community.\n\nLater that fall, based on evolving understanding of the disease, Bertrand began an experimental treatment of N-acetylglucosamine. It allowed him to begin producing small amounts of tears and eye moisture, sufficient to avoid a proposed surgery to sow his eyes closed and preserve his vision.\n\nBertrand\u2019s N-acetylglucosamine trial was the first of several \u201cn of 1\u201d trials in which Bertrand participated as science continued to gain insight into his disorder. Bertrand\u2019s key scientific collaborators in these lifelong efforts to find a treatment include Dr. Hudson Freeze, Dr. Tadashi Suzuki, Dr. Clement Chow, Dr. Kuby Balagurunathan, Dr. Ethan Perlstein, Dr. Hariprasad Vankayalapati, Dr. Yiling Bi, Dr. Eva Morava, Dr. Steve Rodems, Dr. Wei Zheng, Dr. Atena Farkhondeh, Dr. Will Byrd, the entire NCATS Translator Consortium and Beth Aselage.\n\nAfter advocating for legalizing medical CBD (a marijuana extract) for the treatment of children like him, Bertrand received the first medical hemp extract license ever issued in the State of Utah.\n\nBertrand was then subsequently given the Purple Star Award by the Epilepsy Association of Utah for his advocacy.\n\n2015\n\nIn January 2015, prior to the launch of President Obama\u2019s Precision Medicine Initiative, the President personally conveyed to Bertrand\u2019s father that Bertrand\u2019s story had been an inspiration for the initiative.\n\nLater that year, at the age of eight, Bertrand co-founded The NGLY1 Foundation, a patient support and research non-profit dedicated to NGLY1 deficiency.\n\nBertrand then advocated for the successful passage of the Pilot for Medically Complex Children\u2019s Waiver Act in 2015 in Utah (which was then approved in full in 2018).\n\nBertrand also then successfully advocated for the passage of the Right to Try Law in the State of Utah. The law gave patients like Bertrand the \u201cright to try\u201d otherwise inaccessible experimental therapies. Has was personally invited by the governor to attend the signing. (In 2018, Right to Try became a federal law spanning the entire country.)\n\nBertrand then moved to federal advocacy: Bertrand met with Senator Orrin Hatch, asking him to be the founding Republican Senate co-chair of the bipartisan Rare Disease Congressional Caucus. Senator Hatch agreed, expanding the forum for voices of rare disease advocates to both chambers of Capitol Hill.\n\n2016\n\nIn 2016, Bertand began attending events at The White House, starting with the Easter Egg Roll and culminating in a Scientific Meeting for NGLY1 Deficiency followed by a party at The White House bowling alley for his 9th birthday.\n\nAt age nine, Bertrand\u2019s foundation entered into a first-of-its-kind three-way agreement with Retrophin and NCATS at the NIH to begin drug development for NGLY1 deficiency, an effort that continues moving swiftly toward treatments today.\n\n2017\n\nAt age ten, Bertrand finally qualified to use an eye-gaze assistive communication device, and in his final years, we began to experience a new depth to Bertrand. His vocabulary and expressiveness rose swiftly. He continued improving in his usage of his communication device through the end of his life.\n\n2018\n\nBertrand appeared in The New York Times in an article on the emergence of a strategy for fighting rare diseases inspired by his life. That strategy has been actively embodied in the Hugh Kaul Precision Medicine Institute at UAB \u2013 and it is the strategy the Bertrand Might Endowment for Hope supports for patients. Bertrand attended several patient case review sessions at the Institute, where he could see others being helped first hand.\n\n2019\n\nAt age eleven, his first bout with septic shock landed Bertrand in the ICU for six weeks. After nearing death several times, Dr. Shawn Levy at HudsonAlpha conducted custom metagenomic sequencing to look for the possible pathogens driving the illness. Extending an artificial intelligence tool on the fly to analyze the data, Bertrand\u2019s father found an unusual strain of pseudomonas as the likely culprit in Bertrand\u2019s body. Changing treatments to match resolved the infection almost immediately, Bertrand recovered rapidly, and he swam with dolphins to celebrate just one week later. Casey Ross and Hyacinth Empinado chronicled this fight and much of his life to date in STAT [pdf available].\n\n2020\n\nIn February 2020, at age twelve, even with his body weakening from years of ICU stays, he enrolled in an \u201cn of 1\u201d treatment trial at the Mayo Clinic, under Dr. Eva Morava, to begin a proposed treatment discovered by Dr. Ethan Perlstein and his team at biotech company Perlara. He passed his initial health screens to qualify, but hospitalizations immediately following his visit at Mayo coupled with the delays from the COVID-19 pandemic left him unable to start the treatment prior to his passing.\n\nIn March 2020, the same strategy and technology originally developed to find potential therapeutics for Bertrand and patients like him was applied to COVID-19, resulting in the immediate launch of a clinical trial of anti-androgen therapy for patients with COVID-19 at the VA in May 2020. (Trial results pending.)\n\nOn the day Bertrand passed, he became the first patient to donate his remains to the NIH Natural History Study for NGLY1 deficiency. Even in death, he hasn\u2019t stopped furthering the science of his disorder.\n\nAfter Bertrand\u2019s passing, President Obama personally reached out to convey his sympathies:\n\nArchived announcement\n\nThe original announcement at the time of Bertrand\u2019s memorial read:\n\nIt is with broken hearts that we announce the passing of Bertrand \u201cBuddy\u201d Might.\n\nHappy and smiling only hours before, Bertrand progressed rapidly into septic shock on the night of Wednesday, October 21st.\n\nThough hospitalized immediately, Bertrand declined rapidly and passed away Friday, October 23rd with the superhuman grace and dignity that had come to define his life.\n\nWhile nothing eases our pain at his sudden and unexpected absence, we are grateful that he was not in pain and that he was surrounded by his loving parents all the way to the end.\n\nWe miss his courageous heart, his uncommon kindness, his quiet strength, his joyous love. We miss his infectious smile, his warm laughter, his soulful eyes.\n\nWe are proud of Bertrand\u2019s legislative, digital, communal, medical and scientific achievements in his nearly thirteen years.\n\nWe ask that in lieu of flowers, friends of Bertrand please consider a donation to one of the causes below, which carries his impact forward.\n\nQuestions\n\nIn response to questions we\u2019ve received:"
        },
        {
            "authors": [],
            "title": "How do you tag a jellyfish?",
            "contents": "They\u2019re so soft\u2014so squishy! Where to put a tag\u2014and why bother? Questions like these moved scientists from the Monterey Bay Aquarium, the Monterey Bay Aquarium Research Institute (MBARI), Hopkins Marine Station and other institutions around the world to publish the first comprehensive how-to tagging paper for jellyfish researchers everywhere. This missing manual was long in the making\n\nTommy Knowles, a senior aquarist at Monterey Bay Aquarium, explains why. Historically, ocean researchers demonized jellies as \u201cblobs of goo that hurt you,\u201d and that interfered with scientific gear. That changed in the latter part of the 20th century as scientists grew keen to understand entire ecosystems, not just individual plants and animals. Knowing who eats what, how, where and when, they learned, is critical for conservation.\n\nJellyfish, however, remained a very under-appreciated member of the ecosystem for years, largely because so little was known about them.\n\n\u201cPeople didn\u2019t know how to keep them alive in the lab or even on the boat,\u201d says Knowles. Today, the field is coming into its own at a time when climate change has added urgency to the need to understand ecosystems in order to preserve ocean health.\n\nA growing subject of interest\n\nUnderstanding jellies is a concern for fisheries managers, too, since some jellyfish species prey upon the young and compete for food with the adults of commercially important fish. Other jellies impact tourism when blooms of stinging species foul beaches.\n\nIt\u2019s not all negatives. We know that jellyfish play important roles in healthy marine ecosystems, by sheltering juvenile fish and crabs under their swimming bells, and nourishing hundreds of ocean predators. Jellies are a significant food source for ocean sunfish (the largest bony fish on the planet) and the endangered Pacific leatherback sea turtle, California\u2019s state marine reptile.\n\nAs with other marine species that live and travel underwater\u2014out of sight of human researchers\u2014electronic data tags are useful tools for tracking jellies\u2019 movements. Which gets back to the question: Just how do you tag a jellyfish?\n\nIn the 1990s, an Australian researcher wondered if box jellyfish sleep at night. He used early model radio tags developed for other species to discover that box jellies swam to the seafloor and rested there at night.\n\nRapid advances in tagging technology\n\nSince then, tag technology has advanced rapidly, fostered by efforts like the ambitious research project, Global Tagging of Pelagic Predators, (GTOPP). While GTOPP primarily focused on birds, fishes and marine mammals, researchers did tag one invertebrate species, the Humboldt squid, which presented similar challenges to jellyfish.\n\nA small but growing number of scientists continue to explore new and better ways to tag soft and squishy ocean drifters.\n\n\u201cWhat surprised me most was that you could do it!\u201d says MBARI scientist Kakani Katija. She learned there\u2019s more to the bell than meets the eye. Contrary to her expectation that it would be \u201call jelly and mucus, it actually has some heft and structure,\u201d and could support a tag, she notes.\n\nJellyfish tags are similar to fitness trackers and dive computers, recording water temperature, depth, light and motion. Attachment methods include glue, suction cups and cable ties. The first two methods work well for the relatively small jellies tagged in Monterey Bay.\n\nIn other locations, researchers have been able to tag larger species by wrapping cable ties around the manubrium, where the swimming bell connects to the oral arms.\n\nField research is challenging\n\nThe hardest part of tagging jellyfish, says Senior Aquarist Wyatt Patry, can be finding them in the field.\n\nThe technique for success? According to Wyatt, \u201cWe read the ocean\u201d scanning the bay for \u201cslicks\u201d\u2014surface patches where food collects, concentrated by currents and wind.\n\nFinding jellies is only the first hurdle.\n\n\u201cSometimes the jellyfish are a little out of reach,\u201d Tommy explains. \u201cWe can see them but they\u2019re 20 feet down.\u201d That\u2019s when it\u2019s time for a \u201cjelly donut,\u201d he says. Positioning themselves on top of the jelly, the team drives their boat in circles. \u201cDoing donuts\u201d creates an upwelling current, popping a smack (group of jellies) to the surface, Tommy says.\n\nSometimes, even the best laid plans can go awry. In the lab, the team practiced tagging Pacific sea nettles, a species commonly found in the Monterey Bay. But during an El Ni\u00f1o year, nettles were nowhere to be found. For three days in the field, the team tagged the next best thing\u2014egg yolk jellies, Tommy says. (Though they were, he notes, much slimier to deal with than sea nettles.)\n\nFrom their surface boat, the team followed radio signals transmitted by the jellyfishs\u2019 tags. After two to six hours, the tags came loose and surfaced for retrieval. In the lab, researchers then downloaded the data.\n\nBuilding the big picture\n\nThese and similar efforts are adding details to a larger research picture that has yet to fully emerge. Now that they\u2019ve published a comprehensive guide for tagging still more species of jellyfish, and laying a foundation for improving this field of inquiry, a world of exploration awaits. And it\u2019s a large field: So far, only nine of the 200-plus known jellyfish species have been tagged.\n\nWorking with captive jellyfish in the lab, Wyatt and Tommy are helping fellow researchers learn about the larval life stages of comb jellies, and about deep sea species as the Aquarium pioneers new methods of breeding and raising these delicately beautiful animals for exhibit.\n\nFor Tommy, it\u2019s a challenge and a delight because, \u201cJellies are just so awesome!\u201d\n\nHow to tag a jellyfish? A methodological review and guidelines to successful jellyfish tagging. Sabrina Fossette, Kakani Katija, Jeremy A. Goldbogen, Steven Bograd, Wyatt Patry, Michael J. Howard, Thomas Knowles, Steven H.D. Haddock, Loryn Bedell, Elliott L. Hazen, Bruce H. Robison, T. Aran Mooney, K. Alex Shorter, Thomas Bastian and Adrian C. Gleiss. J Plankton Res (2016) 38 (6): 1347-1363.\n\n\u2014Diane Richards\n\nGet a closer look at sea nettles on the Aquarium\u2019s streaming \u201cJelly Cam\u201d.",
            "published_at": "2017-08-01T00:00:00"
        },
        {
            "authors": [
                "Niall Cooling",
                "View All Posts Niall Cooling",
                "Posted On",
                "Root",
                "--M-A-Box-Bp",
                "--M-A-Box-Bp-L"
            ],
            "title": "VSCode, Dev Containers and Docker: moving software development forward - Sticky Bits - Powered by Feabhas",
            "contents": "Long term readers of this blog will know our devotion to using container-based technology, especially Docker, to significantly improve software quality through repeatable builds.\n\nIn the Autumn/fall of 2020, Microsoft introduced a Visual Studio Code (VSCode) extension Remote \u2013 Containers. With one quick stroke, this extension allows you to open a VSCode project within a Docker container.\n\nGetting started with Dev Containers and Docker\n\nThere are several different approaches to using Dev Containers. In this post, we shall cover three options:\n\nUsing an existing Docker image from Docker Hub Using a pre-build Microsoft container setup Using a custom Docker image based on a project specific Dockerfile\n\nThere are a couple of prerequisites:\n\nDocker is installed \u2013 Install Docker Engine\n\nThe VSCode extension Remote \u2013 Containers is installed\n\nUsing an existing Docker image \u2013 TDD in C with Ceedling\n\nAnyone using or experimenting with Test-Driven-Development in C will probably be aware of Ceedling, unity and CMock.\n\nWhether or not you have Ceedling, or any dependents, such as Ruby, installed we can begin using Dev Container with an existing Dockerhub container image. Containerisation ensures we can quickly get up and running with Ceedling in a known environment. In true \u2018Blue Peter\u2018 style, we happen to have a pre-built Ceedling based Docker image on Docker Hub.\n\nCreate an empty folder, e.g.\n\n$ mkdir ceedling_test\n\nIn the new folder, create another folder called .devcontainer (note the preceding . )\n\n$ cd ceedling_test $ mkdir .devcontainer\n\nIn that new folder, add a file called devcontainer.json with the contents\n\n{ \"image\": \"feabhas/ceedling\" }\n\nYour project structure should now be:\n\n.devcontainer \u2514\u2500\u2500 devcontainer.json\n\nNow open VSCode in the working directory\n\n$ code .\n\nVScode will detect the Dev Container configuration file and ask if you want to reopen the folder in a container. Click Reopen in Container . Open a terminal window within VSCode, and you will be presented with a shell prompt # . We are now running within a Docker container based on the image feabhas/ceedling .\n\nTest the container, e.g.\n\n# ceedling new test_project Welcome to Ceedling! create test_project/project.yml Project 'test_project' created! - Execute 'ceedling help' from test_project to view available test & build tasks # cd test_project # ceedling module:create[widget] File src/widget.c created File src/widget.h created File test/test_widget.c created Generate Complete # ceedling test Test 'test_widget.c' -------------------- Generating runner for test_widget.c... Compiling test_widget_runner.c... Compiling test_widget.c... Compiling unity.c... Compiling widget.c... Compiling cmock.c... Linking test_widget.out... Running test_widget.out... -------------------- IGNORED TEST SUMMARY -------------------- [test_widget.c] Test: test_widget_NeedToImplement At line (15): \"Need to Implement widget\" -------------------- OVERALL TEST SUMMARY -------------------- TESTED: 1 PASSED: 0 FAILED: 0 IGNORED: 1\n\nceedling-test\n\nAfter exiting VSCode, all files created will exist in your local file system. Reopening VSCode, you will once again be prompted to reopen in the container.\n\nUsing a pre-build Microsoft container environment \u2013 C++ and CMake\n\nStarting with a classic \u201cHello World\u201d project:\n\nCreate an empty working directory In that directory, create a simple main.cpp , e.g.\n\n#include <iostream> int main() { std::cout << \"Hello from Dev Containers\n\n\"; }\n\nOpen VSCode:\n\n$ code .\n\nOpen the VSCode command pallet ( F1 on all platforms) and select\n\nRemote-Containers: Reopen In Container\n\nVSCode will present several pre-defined development container alternatives.\n\nSelect C++ ; this will now reopen the current VSCode project in a container. Next, you can select your preferred base Linux image \u2013 I have used the ubuntu-20.04 base (it doesn\u2019t matter for this example).\n\nThe default Microsoft C++ container image has many additional packages suitable for hosted C/C++ development already installed in the container, e.g. (at the time of writing) GCC version 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04)\n\nmake version 3.16.3.\n\ngit version 2.25.1\n\netc.\n\nNext, add a simple CMakeLists.txt file\n\ncmake_minimum_required(VERSION 3.16) # set the project name project(Test) # add the executable add_executable(App main.cpp)\n\nFinally, we can build our \u2018application\u2019. In a VSCode terminal window, build the CMake project, e.g.:\n\nvscode \u279c /workspaces/ms-cpp $ mkdir build && cd build mkdir: created directory 'build' /workspaces/ms-cpp/build vscode \u279c /workspaces/ms-cpp/build $ cmake .. -- The C compiler identification is GNU 9.3.0 -- The CXX compiler identification is GNU 9.3.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Configuring done -- Generating done -- Build files have been written to: /workspaces/ms-cpp/build vscode \u279c /workspaces/ms-cpp/build $ make Scanning dependencies of target App [ 50%] Building CXX object CMakeFiles/App.dir/main.cpp.o [100%] Linking CXX executable App [100%] Built target App vscode \u279c /workspaces/ms-cpp/build $ ./App Hello from Dev Containers\n\nSo how does this all work? If you examine the project, you will see that a .devcontainer folder is created along with two files:\n\ndevcontainer.json\n\nDockerfile\n\nThe devcontainer.json file references the Dockerfile defining the core Microsoft C++ Docker image. If you know Docker, then the files are pretty intuitive. We shall build on these in the next example.\n\nUsing a custom Docker image based on a local Dockerfile \u2013 GoogleTest, GoogleMock\n\nThe default Microsoft image does not include the capabilities for using Googletest, GoogleMock or an alternative build system, such as the Meson Build system.\n\nWe could builder our own Docker image and store this on Dockerhub, as shown previously. However, depending on your container requirements\u2019 complexity, it can be easier to build on the base Microsoft Dockerfile and add the required packages.\n\nAdding GoogleTest and GoogleMock\n\nUsing the previous \u201chello world\u201d project, we want to write a simple test using GoogleTest.\n\nThe default Dockerfile has the following lines commented out:\n\n# [Optional] Uncomment this section to install additional packages. # RUN apt-get update && export DEBIAN_FRONTEND=noninteractive \\ # && apt-get -y install --no-install-recommends <your-package-list-here>\n\nUsefully both GoogleTest and GoogleMock are standard ubuntu packages:\n\nAnd can be installed using the apt package manager. As GoogleMock is dependent on GoogleTest, we only need to specify the package libgmock-dev and apt will also install libgtest-dev . Uncomment and modify the lines in the Dockerfile to read:\n\nRUN apt-get update && export DEBIAN_FRONTEND=noninteractive \\ && apt-get -y install --no-install-recommends libgmock-dev\n\nReopen the workspace in a container (you may be prompted to rebuild the container \u2013 if so, then select the rebuild option). We can quickly test if GoogleTest is installed by creating a simple test file test.cpp :\n\n#include \"gtest/gtest.h\" TEST(setup_test_case, testWillFail) { ASSERT_EQ(42, 0); }\n\nand modifying our CMakeList.txt to\n\ncmake_minimum_required(VERSION 3.16) # set the project name project(Test) enable_testing() find_package(GTest REQUIRED) # add the executable add_executable(gtest_test test.cpp) target_link_libraries(gtest_test GTest::GTest GTest::Main) add_test(FailingTest gtest_test)\n\nBuild and test with CMake\n\nFirst, create a build directory and generate the Makefile :\n\nvscode \u279c /workspaces/ms-cpp $ mkdir build && cd build mkdir: created directory 'build' /workspaces/ms-cpp/build vscode \u279c /workspaces/ms-cpp/build $ cmake .. ... -- Configuring done -- Generating done -- Build files have been written to: /workspaces/ms-CPP/build\n\nBuild the executable:\n\nvscode \u279c /workspaces/ms-cpp/build $ make [ 50%] Building CXX object CMakeFiles/gtest_test.dir/test.cpp.o [100%] Linking CXX executable gtest_test [100%] Built target gtest_test\n\nRun the test code:\n\nvscode \u279c /workspaces/ms-cpp/build $ ctest -V UpdateCTestConfiguration from :/workspaces/ms-cpp/build/DartConfiguration.tcl UpdateCTestConfiguration from :/workspaces/ms-cpp/build/DartConfiguration.tcl Test project /workspaces/ms-cpp/build Constructing a list of tests Done constructing a list of tests Updating test list for fixtures Added 0 tests to meet fixture requirements Checking test dependency graph... Checking test dependency graph end test 1 Start 1: FailingTest 1: Test command: /workspaces/ms-cpp/build/gtest_test 1: Test timeout computed to be: 10000000 1: Running main() from /build/googletest-j5yxiC/googletest-1.10.0/googletest/src/gtest_main.cc 1: [==========] Running 1 test from 1 test suite. 1: [----------] Global test environment set-up. 1: [----------] 1 test from setup_test_case 1: [ RUN ] setup_test_case.testWillFail 1: /workspaces/ms-cpp/test.cpp:4: Failure 1: Expected equality of these values: 1: 42 1: 0 1: [ FAILED ] setup_test_case.testWillFail (0 ms) 1: [----------] 1 test from setup_test_case (0 ms total) 1: 1: [----------] Global test environment tear-down 1: [==========] 1 test from 1 test suite ran. (0 ms total) 1: [ PASSED ] 0 tests. 1: [ FAILED ] 1 test, listed below: 1: [ FAILED ] setup_test_case.testWillFail 1: 1: 1 FAILED TEST 1/1 Test #1: FailingTest ......................***Failed 0.01 sec 0% tests passed, 1 tests failed out of 1 Total Test time (real) = 0.04 sec The following tests FAILED: 1 - FailingTest (Failed) Errors while running CTest\n\nAdding Meson and Ninja\n\nFinally, rather than using CMake/make for our build we prefer to use Meson and Ninja for projects. Meson is a Python-based build system, which I find far more intuitive than CMake (Note: CMake can also produce Ninja build files instead of Makefiles).\n\nMeson is also a standard Ubuntu package and has dependents:\n\nAdd meson to the apt package list. It is also worth adding pkg-config as well as meson uses this when looking for package dependencies (it\u2019s not essential but keeps things cleaner).\n\nRUN apt-get update && export DEBIAN_FRONTEND=noninteractive \\ && apt-get -y install --no-install-recommends libgmock-dev meson pkg-config\n\nRebuild the container and confirm meson is installed:\n\nvscode \u279c /workspaces/ms-cpp $ meson -h\n\nSetting up meson\n\nCreate a simple meson.build file, e.g.\n\nproject('tdd-cpp', 'cpp', default_options: ['cpp_std=c++17']) gtest_dep = dependency('gtest', main : true, required : true) gmock_dep = dependency('gmock', main : true, required : true) gtest_test = executable( 'gtest_test', sources : [ 'test.cpp', ], dependencies : [ gtest_dep, ] ) test('failing_test', gtest_test)\n\nAs with CMake :\n\nCreate a build directory :\n\nvscode \u279c /workspaces/ms-cpp $ meson builddir && cd builddir The Meson build system Version: 0.53.2 Source dir: /workspaces/ms-cpp Build dir: /workspaces/ms-cpp/builddir Build type: native build Project name: tdd-cpp Project version: undefined C++ compiler for the host machine: c++ (gcc 9.3.0 \"c++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\") C++ linker for the host machine: c++ ld.bfd 2.34 Host machine cpu family: x86_64 Host machine cpu: x86_64 Found pkg-config: /usr/bin/pkg-config (0.29.1) Run-time dependency GTest found: YES 1.10.0 Run-time dependency GMock found: YES 1.10.0 Build targets in project: 1 Found ninja-1.10.0 at /usr/bin/ninja /workspaces/ms-cpp/builddir\n\nBuild and run the test:\n\nvscode \u279c /workspaces/ms-cpp/builddir $ meson test -v ninja: Entering directory `/workspaces/ms-cpp/builddir' [2/2] Linking target gtest_test. Running main() from /build/googletest-j5yxiC/googletest-1.10.0/googletest/src/gtest_main.cc [==========] Running 1 test from 1 test suite. [----------] Global test environment set-up. [----------] 1 test from setup_test_case [ RUN ] setup_test_case.testWillFail ../test.cpp:4: Failure Expected equality of these values: 42 0 [ FAILED ] setup_test_case.testWillFail (0 ms) [----------] 1 test from setup_test_case (0 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test suite ran. (1 ms total) [ PASSED ] 0 tests. [ FAILED ] 1 test, listed below: [ FAILED ] setup_test_case.testWillFail 1 FAILED TEST 1/1 failing_test FAIL 0.01 s (exit status 1) Ok: 0 Expected Fail: 0\n\nExtending the Dev Container\n\nThe devcontainer.json file enables the created container to be configured and extended (devcontainer.json reference), including VSCode extensions (Visual Studio Marketplace).\n\nFor example, VSCode extensions are available for both meson and GoogleTest . There are a number of specific extensions, here we shall use asabil.meson and C++ TestMate. These can be added to the container configuration using the devcontainer.json file, e.g.\n\n// Add the IDs of extensions you want installed when the container is created. \"extensions\": [ \"ms-vscode.cpptools\", \"asabil.meson\", \"matepek.vscode-catch2-test-adapter\" ],\n\nNow when we open the container, the extensions are present and allow us to manage the project.\n\nNote, you\u2019ll need to modify the file .vscode/settings.json to include\n\n{ \"testMate.cpp.test.advancedExecutables\": [ \"builddir/gtest_test\", ] }\n\nto get C++ TestMate to pick up the test executable.\n\nAreas of note\u2026\n\nBefore you rush off turning all your projects into VSCode Dev Containers, I\u2019ve come across a couple of issues.\n\nFirst, some of our existing containers, especially those optimised for small size (notably multi-build, alpine-based images) fail to run using Dev Containers. I\u2019ve yet to get to the bottom of these issues (yet another backlog item).\n\nAlso, I have had problems where existing Dockerfiles make significant use of bash scripts for internal configuration. Often these scripts are built around specific path configurations. When using Dev Containers the workspace is mounted under the internal path /workspaces/<git repo name> and can cause scripts to fail. It appears this can be managed using advanced container configuration, but it\u2019s yet another thing I\u2019ve yet to experiment with.\n\nFinishing off\n\nThe integration of container technology, specifically Docker, to VSCode creates an exciting development environment. It cements the relationship between local TDD development and CI pipeline builds by ensuring common build and test environments using Docker.\n\nThe example CMake and meson projects are elementary and not intended to represent real-world CMake or meson projects, but hopefully are enough to get going with. A slightly better, but by still no means complete, Meson/gtest/gmock example project can be found at here.\n\nGitHub Codespaces\n\nIf Dev Containers pique your interest, then you love Codespaces \u00b7 GitHub. GitHub Codepsaces are the topic of the next post.",
            "published_at": "2021-01-21T14:00:31+00:00"
        },
        {
            "authors": [],
            "title": "Create your virtual world in one click.",
            "contents": "Create your virtual world in one click.\n\nExplore on your own, or bring your friends, family & co-workers to hang out."
        },
        {
            "authors": [],
            "title": "Your free and decentralized audio platform",
            "contents": "Listen to your music, everywhere\n\nUpload your personal library to your pod, share it with friends and family, and discover talented creators.\n\nFunkwhale gives you access to your playlists, favorite tracks, and artists everywhere, from our web interface or the wide range of compatible apps for every platform.\n\nGet started"
        },
        {
            "authors": [
                "Ben De Haas"
            ],
            "title": "What my retraction taught me",
            "contents": "In the middle of the pandemic, I got an e-mail asking whether I had access to data from the experiments behind a paper I\u2019d published in 2014. Three months later, I requested that the paper be retracted. The experience has not left me bitter: if anything, it brought me back to my original motivation for doing research.\n\nThe query was about work I was proud of. My colleagues and I had asked dozens of participants in a brain-imaging experiment to solve a visual task, which was either hard or easy. We wanted to know how distraction affects the processing of irrelevant stimuli. Our results suggested that distraction blurs the representation of images in the brain\u2019s visual cortex, inducing a sort of neural tunnel vision. Exciting stuff, we thought at the time. It turns out that it might have been a statistical artefact.\n\nIt helped that I knew the researcher who had raised the alarm \u2014 Susanne Stoll, a PhD student with neuroscientist Sam Schwarzkopf at University College London (UCL). At a 2019 conference in Brussels, we had discussed her perplexing results in a project that built on the work I\u2019d done during my PhD, in collaboration with Sam and others.\n\nSusanne and her co-workers never treated me as a suspect, but as a colleague in the same boat. We all wanted to know what on earth was going on with her unexpected results. They told me how a problem with the analysis might have affected my study (and possibly many others). It involved regression towards the mean \u2014 when noisy data are measured repeatedly, values that at first look extreme become less so. I was sceptical. After all, the effects in my paper were the opposite: parameter values moved away from the mean.\n\nHow scientists fool themselves and how they can stop\n\nWe set up a video meeting, and decided that Susanne would go through simulations, and I would go through my old data, if I could dig them up. That was a challenge. Only months before, my current university had suffered a cyberattack, and access to my back-up drive was prohibited at first. It would have been easy to tell the others that the data were gone (as happens all too frequently).\n\nBut Susanne and Sam wanted to crack the mystery \u2014 and that curiosity was contagious. I spent a week piecing together the necessary files and coding a pipeline to reproduce the original findings. To my horror, I also reproduced the problem that Susanne had found. The main issue was that I had used the same data for selection and comparison, a circularity that crops up again and again. That this could be a problem in our particular context didn\u2019t dawn on me and my colleagues \u2014 nor on anyone else in the field \u2014 before Susanne\u2019s discovery. The resulting biases were very different from the textbook example, and became apparent only through simulations and stress tests of the data.\n\nSuddenly, everything felt much more serious. I immediately drafted a summary of my findings and sent them to my original co-authors, complete with a first draft of a retraction note. I will never forget the reply from my PhD adviser, Geraint Rees at UCL. His e-mail began: \u201cGreat that we\u2019ve persisted in attempting to understand our methodology and findings!\u201d He encouraged me to dig deeper and run an unbiased analysis. This showed trends in the direction of our original findings, but these were much less robust than we had thought.\n\nSo, we decided to retract. Our retraction notice explains what happened and points to a technical paper led by Susanne, so that others can learn from our mistakes (S. Stoll et al. Preprint at bioRxiv https://doi.org/fqs8; 2020). Over the years, the field of neuroimaging has discovered a number of possible pitfalls, and has changed its practices accordingly. My hope is that we can contribute to this evolution and foster improvements such as sanity checks with simulated data.\n\nRein in the four horsemen of irreproducibility\n\nBut the lessons here go beyond the technical.\n\nI think that most scientists would like to be more critical of their data and conclusions, because they are driven by the simple desire to learn. However, we all face career incentives that punish flagging up mistakes and negative results. So far, my co-authors and I have not experienced repercussions from our retraction, but we were willing to face the risks. As a junior principal investigator without tenure, juggling pandemic home-schooling and remote working, I\u2019m acutely aware of how costly a reanalysis and retraction is in terms of time and CV points. As a student, I was even told never to attempt to replicate before I publish. That is not a career I would want \u2014 luckily, my PhD adviser taught me the opposite.\n\nWhat we need are incentives that foster the openness and curiosity that motivated us to become researchers in the first place. Painting each other as villains, trying to oversell or hide data or embarking on a witch-hunt will only achieve the opposite.\n\nSeeing each other as peers with the common goal of understanding the world is win\u2013win. When I started publishing my data and code in 2017, it was because I knew how much my own research could benefit from others doing the same. That desire to know is what kept Susanne exploring puzzling results, what led me to re-analyse my data and what encouraged our colleagues to support us along the way.\n\nScientific progress will always involve the detection and correction of errors. Some tenure committees and grant agencies have started asking candidates whether they practise open science. I suggest they add: \u2018What have you learnt from your mistakes?\u2019"
        },
        {
            "authors": [],
            "title": "Basic Scripting With Awk And Gnuplot",
            "contents": "We're going to need variables, like an index to a for loop, to print the 'trial number'. No problem, we can initialize one with the -v option like so awk -v j=1 [script] (set j to 1).\n\nWe can also ask awk to give us the nth word in the current line with $n , where n is some integer.\n\nAnd that's basically all we need! Here is the script:\n\n# \"Time\" will appear before \"java\" in my output (e.g. \"Time ...\n\njava ...\n\nTime ...\n\n...\"). # Also, substr used to remove the 's' character from time's output awk -v j = 1 '/Time/{printf(\"%d %s \", j++, $3)} /java/{printf(\"%s\n\n\", substr($5, 1, length($5)-1))}'\n\nI shall elaborate a little more. My program could be run like time ./java arg1 arg2 2>&1 which would produce 2 lines of output, the first containing \"Time = [integer]\", the second containing the output of time . So my awk script will either see \"Time\", where it will print the index and integer, or it will print the real time after seeing \"java\". Notice that I don't print a newline with the \"Time\" action (which will come first in my output). So I can build a row from data that spans multiple lines (2 lines in this case.) Also, my output doesn't have any lines that contain both \"Time\" and \"java\".\n\nWhen my program is run five times and the output is piped to that awk command, it produces data like this:\n\n1 12.11 2.547 2 11.294 3.07 3 14.375 3.102 4 12.407 3.208 5 10.147 3.212\n\nWhich is what we want for gnuplot.",
            "published_at": "2021-01-17T00:00:00+00:00"
        },
        {
            "authors": [],
            "title": "Learn and Master Vim faster with interactive exercises",
            "contents": ""
        },
        {
            "authors": [
                "Ja Patel",
                "Juraj M.",
                "Tom Kwong",
                "K. R.",
                "Gian-Carlo Pascutto",
                "Andrew Morrow"
            ],
            "title": "Porting Firefox to Apple Silicon",
            "contents": "The release of Apple Silicon-based Macs at the end of last year generated a flurry of news coverage and some surprises at the machine\u2019s performance. This post details some background information on the experience of porting Firefox to run natively on these CPUs.\n\nWe\u2019ll start with some background on the Mac transition and give an overview of Firefox internals that needed to know about the new architecture, before moving on to the concept of Universal Binaries.\n\nWe\u2019ll then explain how DRM/EME works on the new platform, talk about our experience with macOS Big Sur, and discuss various updater problems we had to deal with. We\u2019ll conclude with the release and an overview of various other improvements that are in the pipeline.\n\nApple Silicon Approaching\n\nSpeculation that Apple would switch its Mac lineup to use ARM CPUs had been ongoing in the industry for several years. As early as 2013, Apple had referred to the custom ARM chips they were putting in the iPhone as \u201cdesktop-class\u201d designs.\n\nWhile the claim initially met some scepticism, near the end of 2018 computer hardware magazine AnandTech published the results of running the industry-standard SPEC benchmark on the iPhone XS, showing that even workloads that reflect real-world desktop use cases reached desktop chip performance, and were doing so at significantly better power efficiency. This provided us with some warning that Apple might be ready to start the transition to the ARM architecture in the near future.\n\nFrom the perspective of Mozillla\u2019s platform team, an area of particular interest for such an architecture change on macOS is Firefox\u2019s use of macOS APIs. Firefox and Gecko\u2019s roots go back to the Netscape codebase, which already supported the Mac as it was in 1994.\n\nAlthough continuously updated, Firefox still uses a wide range of macOS APIs that followed the Mac\u2019s evolution over the years (Carbon, Cocoa, HITheme, Quartz, \u2026).\n\nApple has generally kept them \u2014 the code is there and working, after all \u2014 and has even added compatibility shims in some places where behavior has changed. But they\u2019re not willing to keep compatibility forever, and in fact had removed 32-bit support in the previous macOS Catalina which had an impact on applications that were relying on this, among them many games.\n\nAs such, we were concerned that not all APIs would still be supported on the new architecture and we\u2019d have to go in and rewrite some amount of widget, toolkit or theming code in short order.\n\nBased on the performance from the aforementioned benchmarks and Apple\u2019s historical release schedule, the platform team estimated in March that \u201cmacOS 10.16\u201d was likely to appear around September or October 2020 and that there was a significant risk it could involve API changes in order to add ARM support, which we took into account in our planning.\n\nThe Announcement\n\nOn the 22nd of June 2020, Apple confirmed it would begin moving its Mac hardware to their own ARM chips \u2013 referred to as Apple Silicon. They also confirmed that the machines would ship with an Intel x64 emulator (Rosetta 2) and would support iOS apps.\n\nThe latter led to some guessing within Mozilla\u2019s platform team as to whether the new Macs would have a touchscreen. While we were \u2014 and still are \u2014 quite ready to support it, at least the eventual first Apple Silicon-based Macs didn\u2019t end up having one.\n\nTogether with the announcement of the transition, Apple also announced the availability of Developer Transition Kits (DTK), essentially containing the iPad Pro\u2019s chip in a Mac Mini housing. What Apple didn\u2019t share was when exactly the final machines were coming to market.\n\nBased on the timing of the DTK availability and Apple\u2019s hint that it would be \u201cby the end of the year\u201d, we guessed this was likely to be somewhat before the Christmas holidays.\n\nLooking back, we noticed that Apple has very consistently been able to make hardware available near immediately after announcing it, so we figured that any next planned announcement should be taken as a release date.\n\nWhen another announcement was planned for November 10th \u2013 about a month before our original estimate \u2013 we took it as the shipping date. And indeed, Apple did end up shipping the first hardware one week later on November 17th.\n\nFirst Steps\n\nOf all the work needed to support the new hardware, porting Firefox to the 64-bit ARM architecture was not actually something we needed to do: we\u2019ve supported 64-bit ARM on Android and Linux for years.\n\nWe refrained from publishing the 64-bit Android builds until late 2019 because before that point our JavaScript JIT was not fully optimized for 64-bit ARM, and the resulting 64-bit version would have been slower than the 32-bit one. There wasn\u2019t much demand for the browser to be able to use over 4GB of memory on phones either! In 2019, we released the first Firefox version for Windows on 64 bit ARM which gave us some additional experience in exactly the kind of effort we were facing now.\n\nWhile Windows on ARM hardware has failed to catch on with our users so far, expectations were for the Apple transition to be very different. Not only was there a good reason to expect hardware performance to be groundbreaking as explained in the first section. Apple made it clear they were switching their entire lineup and not releasing a single device as a \u201cfeeler\u201d. To top it off, they had a proven track record of successful architecture transitions on the Mac.\n\nSo with 64-bit ARM support already in the codebase, the first pass of work was to go through all the Firefox code, dependencies, and various third-party build systems to see if they correctly dealt with the novel idea that a Mac could have an ARM chip inside.\n\nSecondly, we needed to adapt and fix the various parts of the Firefox codebase that deal with low-level calling conventions and particularly the interfaces between the JavaScript and C++ (and nowadays Rust) parts of the code.\n\nRust in particular was a concern. Firefox depends on Rust code, and we require a working Rust compiler to build the browser. Although Apple Silicon support for Rust was underway, it took until mid-August for there to be functional compiler builds, which limited the amount of progress possible for Firefox.\n\nOnce the compiler was working, a similar exercise needed to be done with all the Rust crates we depend on. The need to update the compiler and the reliance of some crates on the exact compiler version, especially parts dealing with SIMD support, would end up biting us later on as it made it hard to push Apple Silicon support forward to an earlier release of Firefox without potentially affecting other platforms.\n\nUniversal Binaries\n\nAn important decision to be made was whether to produce separate builds for Intel- and ARM-based Macs, or to generate Universal Binaries which bundle both builds and select the correct version at runtime. Producing Universal Binaries is a bit more complicated, but we had existing tooling in place dating back to the time when Apple supported both 32-bit and 64-bit binaries, which could be adapted.\n\nIt greatly simplifies things for the user \u2014 there is no risk of downloading the wrong version \u2014 and also meant that our download pages and some infrastructure like localization could remain unchanged.\n\nThe main downside is the installer significantly increasing in size, not just for ARM users but also for Intel ones. As this only affects the initial install, and users typically receive new versions through updates that are much smaller, we felt that this was an acceptable downside and proceeded along this route.\n\nNetflix and DRM\n\nWhile we can port the open-source parts of Firefox to 64-bit ARM ourselves, Netflix and some other video streaming services such as Hulu, Disney+, or Amazon Prime require their video to be decoded with closed source, proprietary DRM software.\n\nIf the user visits such a site, Firefox will automatically download and install such a proprietary EME/CDM module. This presented a problem to us as we would be dependent on those third-party vendors to publish ARM64 versions of those decoders.\n\nWe did not manage to get a commitment to a release date for such updates, and even if we did, there was no guarantee that they would be before the unknown release date of the Apple Silicon hardware. As a significant number of our users use the browser to watch video online, this presented a potential showstopper for a native Apple Silicon release.\n\nWe ended up leveraging a technique that we are also using for the Windows on ARM version of Firefox. The DRM video decoder already executes in a separate process so we can sandbox the proprietary code from the user\u2019s system.\n\nIf we force this decoding process to run under emulation, we would be able to use the existing Intel x64 decoder modules and have them communicate with the main browser that was running natively.\n\nThere were a few catches to getting this to work: because the process that loads the Google Widevine DRM module itself depends on some runtime libraries, we needed Intel x64 copies of those as well.\n\nLuckily, due to the Universal Binary containing both versions of Firefox, we were able to pick them up directly from the Application Bundle.\n\nSecondly, Apple did not actually ship their Rosetta 2 emulator preinstalled on the Apple Silicon machines but its installation is triggered when the user tries to run an Intel application.\n\nSo while it is very likely in practice that Rosetta is installed on the user\u2019s system, we could not rely on this always being the case. Triggering the installation of Rosetta programmatically works, but some of our colleagues found out the hard way it is not very reliable, so we backed off on doing this in our first release and fell back to referring people that hit the relevant error to a support article.\n\nmacOS Big Sur\n\nThe macOS Big Sur betas arrived independently of the Apple Silicon hardware and allowed us to get an early look at the compatibility story. To our relief, no APIs we depended on were deprecated and any backwards compatibility problems or missing shims were limited to small cosmetic issues, which we typically managed to fix quickly. Other open-source projects with a similarly old codebase were not as lucky.\n\nBumping the version numbers from 10.x to 11.0 \u2013 somewhat predictably \u2013 produced errors both in our code and in external websites relying on UA sniffing, despite Apple\u2019s attempts to mitigate the problem by returning the old version number in apps built with older SDKs.\n\nUpdate Woes\n\nPushing the updated Firefox application bundle to users \u2013 which was now a Universal Binary supporting both types of Apple hardware, instead of only Intel x64 as before \u2013 revealed some further complications.\n\nDuring an update, after updating the files on disk, Firefox will relaunch the updated version of itself. Any application on Apple Silicon that is running under Intel x64 emulation and launches another process will also cause that process to be launched under emulation.\n\nSo when the old Firefox 83 \u2013 running under emulation \u2013 launches the new Firefox 84 with native support, it would not launch the new native binary but end up forcing it to be run under emulation as well, at least until the application was fully restarted.\n\nWhile we developed a workaround for this, we didn\u2019t feel it was sufficiently tested by the release date for the marginal benefit it gave and ended up simply adding a release note to cover this case.\n\nMore of a concern was user reports that some antivirus software was flagging all our Universal Binaries as malware, and corrupting the Firefox installation the moment the update arrived.\n\nThe software was using machine learning techniques and presumably observed that our combined Universal Binaries didn\u2019t quite look like any other legitimate software it had ever seen before.\n\nAttempts to contact the vendor through regular support channels were unsuccessful so we ended up searching LinkedIn and managed to find an engineer working on the core antivirus detection.\n\nThey immediately understood the seriousness of the problem and took prompt action to get a fix shipped, thus preventing quite the disaster for the users of this product. It\u2019s notable that without this last-ditch effort we would have been effectively blocked from releasing a native Apple Silicon version for an indefinite period.\n\nThis wouldn\u2019t be the first time that browser makers are exasperated at the misaligned incentives for anti-virus vendors when anti-virus software and browsers don\u2019t get along.\n\nRelease Calls\n\nComparing the Firefox release schedule with the predicted release date from Apple meant that our Firefox 83 \u2013 scheduled for November 17th \u2013 aligned with the availability of release hardware.\n\nWhile it would have been nice to announce native support in the stable version as soon as the first production machines arrived to customers, this would also have meant that it would be completely untested on the real hardware.\n\nWe decided to be conservative here and keep our initial support to the Firefox 84 beta, which was released the same day as Firefox 83, giving both us and our users a window of time to evaluate the stability of the product on the actual Apple Silicon hardware.\n\nThough somewhat disappointed that after being one of the first to announce native support in Nightly we ended up delaying a stable release slightly, the difficulties experienced by other browser vendors with shipping a working version supported our decision. Firefox with native Apple Silicon support went into the wider world when 84 beta rolled into the Firefox 84 release, on December 15th, 2020.\n\nWhile most benchmarking of Apple Silicon indicated that the performance impact of Rosetta emulation was typically low and that applications could be expected to run at about 70-80% of native performance, we saw much larger gains when testing the native Firefox build, including doubled performance on some key benchmarks and a spectacular 2.5 times faster startup.\n\nOne reasonable explanation for this faster startup could be that many parts of Firefox itself are written in the web\u2019s own languages \u2013 JavaScript, CSS, and HTML \u2013 and thus use a JavaScript JIT for much of its own functionality.\n\nOn startup, the JIT has to translate the JavaScript to machine code and while this is typically a very fast operation when running under emulation Rosetta has to then translate this JIT-generated machine code to machine code for another architecture.\n\nApple introduced a translation cache that likely removes this overhead completely for most applications but it does not work for code that is output by a JIT. With the native build, this second translation is avoided completely and we\u2019re back to having a snappy browser.\n\nFuture Support\n\nWith the initial release out, there are a number of further improvements that we can and will make to the Apple Silicon version, some of which will already be available in Firefox 85.\n\nFirst of all, we had to disable WebRender in the initial version because it triggered graphics driver bugs in the first Big Sur releases for Apple Silicon. Now that these have been addressed and we\u2019ve validated WebRender on the final hardware, we have re-enabled it and it will ship in 85.\n\nSecondly, Firefox currently uses the baseline compiler for WebAssembly on 64-bit ARM. There is a faster optimizing compiler called Cranelift available for testing on Firefox Nightly, and in a few weeks, we expect to finish the 64-bit ARM port of our own optimizing compiler, Ion, which is likely to become the new default.\n\nThe Apple Silicon chips are one of the first desktop chips that are a heterogeneous design with distinct performance and efficiency cores. We\u2019re revising much of our core threading and thread pooling architecture to handle the distinction better, improve efficiency, and eventually be able to schedule less performance-critical tasks on the efficiency cores.\n\nFinally, we\u2019re cleaning up and modernizing our usage of legacy macOS drawing APIs, and in some cases, removing our custom drawing code entirely. This is expected to help with some of the outstanding glitches in dark mode support, as the legacy macOS APIs simply don\u2019t support it and the color values must be obtained via newer APIs. It will also remove some of our deprecation worries!\n\nWe hope you enjoyed this inside view of how the Firefox team experienced the Apple Silicon transition and we\u2019re looking forward to pushing the Firefox experience on macOS to even higher levels in the year to come.\n\nThanks to Mike Hommey and Haik Aftandilian for their substantial input to this post. They also did a lot of the engineering work described here. Further editorial suggestions were provided by Andrew Overholt, Sylvestre Ledru, and Selena Deckelmann."
        },
        {
            "authors": [],
            "title": "Philips lighting",
            "contents": "Bulb 2 Watt\n\n\n\nWith a light output of 400 lumen, this lamp can replace a 40W incandescent bulb, for example in decorative fixtures or areas where not much light is needed. Its filament LED technology gives the same decorative impression as the original incandescent lamps. The lamp is available in warm white and cool daylight. The lifetime is 25,000 hours. The lamp has an E27 base and is not dimmable. This product contains no mercury."
        },
        {
            "authors": [
                "Lisa Visentin"
            ],
            "title": "Google threatens to disable search in Australia if media code becomes law",
            "contents": "Google says it will stop making its search function available in Australia if Parliament passes the Morrison government's proposed laws to force it and Facebook to pay news businesses for their journalism.\n\nGoogle Australia managing director Mel Silva told a Senate hearing on Friday the proposed news media bargaining code remained \"unworkable\", and the company was prepared to exit the Australian market.\n\nGoogle Australia managing director Mel Silva told a Senate hearing the company would disable Google Search in Australia if the parliament proceeded to pass the Morrison government's proposed media bargaining code. Credit:Louie Douvis\n\n\"If this version of the code were to become law, it would give us no real choice but to stop making Google Search available in Australia,\" Ms Silva told the inquiry.\n\nIt is the first time the digital giant has made the threat to disable its primary search function to all Australians in its response to the proposed laws.",
            "published_at": "2021-01-22T00:00:00"
        },
        {
            "authors": [
                "Shannon Casey"
            ],
            "title": "Meet Virginia Apgar, the unlikely anesthesiologist who saved newborn babies",
            "contents": "The newborn\u2019s skin was blue and he wasn\u2019t breathing. A few years earlier, the doctors would have documented the baby as stillborn, not believing there was anything they could do to help. If this were the mid-1950s though, a recent development in the field of obstetrics would have given them hope \u2013 the Apgar score. The newborn\u2019s 1-minute Apgar score indicated that the newborn was in poor condition, but they treated him with oxygen. Sure enough, his 5-minute Apgar score showed improvement. Maybe he had a chance after all.\n\nVirginia Apgar's invention helps saves newborns.\n\nVirginia Apgar was born on June 7, 1909 in Westfield, New Jersey. Although her father\u2019s day job was that of an insurance executive, on the side he was an amateur inventor and astronomer. As a child, Virginia Apgar learned to play the violin and later joined her high school orchestra. Around this time, she set her sights on medicine. At Mount Holyoke College, she pursued a degree in zoology and was lauded as an exceptional student.\n\nAfter graduating in 1929, Apgar began her medical training at Columbia University's College of Physicians and Surgeons. Women were drastically underrepresented in the field at the time; there were only eight other women in her class of 90 people. After completing her MD in 1933, she became one of the first female surgical residents at Columbia University College of Physicians and Surgeons. However, her mentor, surgeon Allen Whipple, was concerned that since she was a woman, she would have a hard time attracting patients. As a result of this feedback, Apgar pivoted to anesthesiology, a relatively new specialty established in the mid-1940s. This was a much less prestigious career path, but since anesthesia was mostly handled by nurses at the time, gender discrimination wouldn't present as much of a hurdle.\n\nVia Library of Congress\n\nAs an anesthesiologist, Apgar had never even delivered a baby and was therefore an unlikely candidate for revolutionizing the field of obstetrics. However, part of her job entailed providing anesthesia for deliveries, so she had plenty of exposure to newborns. In the 1950s, reducing infant mortality (the death of an infant before their first birthday) was a daunting problem: an astonishing one in 30 newborns died at birth.\n\nAt that time, if a newborn didn\u2019t seem to be thriving \u2013 if, for example, the baby\u2019s skin was too blue or they were deemed too small \u2013 the baby would be left to die and documented as a stillborn. Of course, it wasn\u2019t that anyone wanted these babies to die, but doctors simply believed that they were too sick to survive. Apgar believed that if such babies received better care, many of them would live.\n\nSince Apgar was a \u201clowly\u201d anesthesiologist, she wasn\u2019t in a position to directly challenge how obstetricians did their work. Moreover, she was a woman in a man\u2019s world. Nevertheless, she devised the \"Apgar score\" \u2013 a way to observe and document the condition of every newborn. It was a simple, indirect approach that had powerful results in terms of helping to dramatically decrease the infant mortality rate.\n\nVia Wikimedia\n\nThe Apgar score is an acronym with each letter standing for a component of the score: Activity, Pulse, Grimace, Appearance, and Respiration. In other words, the score evaluates a newborn\u2019s movement, heart rate, irritability, color, and breathing. In each category, a newborn receives 0, 1, or 2 points. In the \u201cPulse\u201d category, for example, the newborn\u2019s pulse may either be absent (0 points), below 100 beats per minutes (1 point), or over 100 beats per minute (2 points). The total score ranges from 0-10, with a low score indicating a newborn in poor condition, and a high score indicating a newborn in excellent condition.\n\nThe Apgar score enables healthcare providers to systematically observe and document the condition of every newborn \u2013 starting one minute after the baby is born and again five minutes after birth. Implementing the Apgar score introduced a spirit of competition because the doctors inherently wanted the newborns they delivered to have better scores. It became readily apparent that a baby with a low score initially could improve remarkably and have an excellent score five minutes after birth.\n\nMoreover, Apgar worked with colleagues such as pediatrician L. Stanley James and anesthesiologist Duncan Holaday to establish the physiological basis for the Apgar score\u2019s profound effect on reducing infant mortality. By analyzing neonatal blood chemistry, such as the oxygen and carbon dioxide levels in the newborn's blood, Apgar and others were better able to correlate a newborn\u2019s Apgar scores to the effects of labor, delivery, and maternal anesthesia practices. Today in the US, thanks in large part to Apgar\u2019s contribution to the field of obstetrics, only about three out of five hundred newborns don\u2019t live to see their first birthday.\n\nToday the Apgar score remains the standard of care, although some have wondered whether or not it is still pertinent in the context of modern medicine. However, despite suggestions that the Apgar score may be antiquated, this study from 2013 concluded that the Apgar score \u201chas continuing value for predicting neonatal and post-neonatal adverse outcomes.\u201d\n\nVia Library of Congress\n\nIn the process of attending over 17,000 births, Apgar had seen many newborns with birth defects, and she went on to become a leader in the emerging field of teratology \u2013 the study of birth defects. In 1958, she took a sabbatical from clinical practice and pursued a master's degree in public health from Johns Hopkins University. Subsequently, she became the director of a division at what is now the March of Dimes. In this role, Apgar worked to increase research in the field of teratology in order to help prevent and ameliorate birth defects to the greatest extent possible. Her medical training coupled with incredible determination and perseverance enabled her to leave an inspiring legacy in the field of medicine.\n\nApgar never retired, but she developed progressive liver disease and died on August 7, 1974. In addition to receiving numerous awards during her lifetime, her contributions have also been recognized far and wide posthumously. In 1994, her portrait was included in the commemorative U.S. postage stamp series of \u201cGreat Americans.\u201d The following year, Apgar was inducted into the National Women's Hall of Fame.",
            "published_at": "2021-01-19T04:36:56.756000+00:00"
        },
        {
            "authors": [],
            "title": "Alphabet shuts down Loon internet balloon company \u2013 TechCrunch",
            "contents": "Google\u2019s parent firm, Alphabet, is done exploring the idea of using a fleet of balloons to beam high-speed internet in remote parts of the world.\n\nThe firm said on Thursday evening that it was winding down Loon, a nine-year-old project and a two-and-a-half-year-old spin off firm, after failing to find a sustainable business model and partners for one of its most prominent moonshot projects.\n\nThe demise of Loon, which assumed spotlight after the project helped restore cell services knocked out by a hurricane in Puerto Rico, comes a year after the Android-maker ended Google Station, its other major connectivity effort to bring internet to the next billion users.\n\nThrough Station, Google provided internet connectivity at over 400 railway stations in India and sought to replicate the model in other public places in more nations.\n\nThat said, Alphabet\u2019s move today is still surprising. Just last year, Loon had secured approval from the government of Kenya to launch first balloons to provide commercial connectivity services \u2014 something it did successfully achieve months later, giving an impression that things were moving in the right direction.\n\nOn its website, Loon has long stated its mission as: \u201cLoon is focused on bringing connectivity to unserved and underserved communities around the world. We are in discussions with telecommunications companies and governments worldwide to provide a solution to help extend internet connectivity to these underserved areas.\u201d\n\nPerhaps the growing interest of SpaceX and Amazon in this space influenced Alphabet\u2019s decision \u2014 if not, the two firms are also going to have to confront some difficult feasibility questions in the future.\n\n\u201cWe talk a lot about connecting the next billion users, but the reality is Loon has been chasing the hardest problem of all in connectivity \u2014 the last billion users,\u201d wrote Alastair Westgarth, chief executive of Loon, in a blog post.\n\n\u201cThe communities in areas too difficult or remote to reach, or the areas where delivering service with existing technologies is just too expensive for everyday people. While we\u2019ve found a number of willing partners along the way, we haven\u2019t found a way to get the costs low enough to build a long-term, sustainable business. Developing radical new technology is inherently risky, but that doesn\u2019t make breaking this news any easier.\u201d\n\nThe blog post characterised Loon\u2019s connectivity effort as success.\n\n\u201cThe Loon team is proud to have catalyzed an ecosystem of organizations working on providing connectivity from the stratosphere. The world needs a layered approach to connectivity \u2014 terrestrial, stratospheric, and space-based \u2014 because each layer is suited to different parts of the problem. In this area, Loon has made a number of important technical contributions,\u201d wrote Westgarth.\n\nWhat happens next\n\nIn a separate blog post, the firm said it had pledged a fund of $10 million to support nonprofits and businesses focussed on connectivity, internet, entrepreneurship and education in Kenya.\n\nAlphabet also plans to take some of Loon\u2019s technology forward and share what it learned from this moonshot idea with others.\n\nAdditionally, \u201csome of Loon\u2019s technology \u2014 like the high bandwidth (20Gbps+) optical communication links that were first used to beam a connection between balloons bopping in the stratosphere \u2014 already lives on in Project Taara. This team is currently working with partners in Sub-Saharan Africa to bring affordable, high-speed internet to unconnected and under-connected communities starting in Kenya,\u201d the firm said.\n\nScores of firms including Google and Facebook have visibly scaled down several of their connectivity efforts in recent years after many developing nations such as India that they targeted solved their internet problems on their own.\n\nIt has also become clear that subsidizing internet access to hundreds of millions of potential users is perhaps not the most sustainable way to acquire customers.",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [
                "Avi Loeb",
                "About The Author S",
                "Avi Loeb Is Former Chair",
                "Of The Astronomy Department At Harvard University",
                "Founding Director Of Harvard'S Black Hole Initiative",
                "Director Of The Institute For Theory",
                "Computation At The Harvard-Smithsonian Center For Astrophysics. He Also Chairs The Board On Physics",
                "Astronomy Of The National Academies",
                "The Advisory Board For The Breakthrough Starshot Project",
                "Is A Member Of"
            ],
            "title": "Why Do We Assume Extraterrestrials Might Want to Visit Us?",
            "contents": "It is presumptuous to assume that we are worthy of special attention from advanced species in the Milky Way. We may be a phenomenon as uninteresting to them as ants are to us; after all, when we\u2019re walking down the sidewalk we rarely if ever examine every ant along our path.\n\nOur sun formed at the tail end of the star formation history of the universe. Most stars are billions of years older than ours. So much older, in fact that many sunlike stars have already consumed their nuclear fuel and cooled off to a compact Earth-size remnant known as a white dwarf. We also learned recently that of order half of all sunlike stars host an Earth-size planet in their habitable zone, allowing for liquid water and for the chemistry of life.\n\nSince the dice of life were rolled in billions of other locations within the Milky Way under similar conditions to those on Earth, life as we know it is likely common. If that is indeed the case, some intelligent species may well be billions of years ahead of us in their technological development. When weighing the risks involved in interactions with less-developed cultures such as ours, these advanced civilizations may choose to refrain from contact. The silence implied by Fermi's paradox (\u201cWhere is everybody?\u201d) may mean that we are not the most attention-worthy cookies in the jar.\n\nAs a first approximation to what humans look like, it is reasonable to glance at the mirror. This approach relies on the unremarkable assumption that each of us shares a common genetic ancestry with all people. But this might not be the case for life that developed independently on other planets. For example, animals and vegetation on the nearest habitable exoplanet, Proxima Centauri b, could be shockingly different from those on Earth. In particular, the animals might possess strange-looking eyes, optimized to detect the infrared radiation emitted by Proxima Centauri, a dwarf star with half the surface temperature of the sun.\n\nSince Proxima b is 20 times closer to its star than the Earth is to the sun, we expect it to be tidally locked, showing the same face to its star at all times, as the moon always shows the same face to us. The species residing on its permanent dayside may be completely different from those on its colder nightside, exhibiting distinct patterns of enforced sleep. Any vegetation on the planet\u2019s surface would adapt to harvesting infrared light, showing a \u201cred edge\u201d at a longer wavelength than plants on Earth. As a result, the grass in our neighbor\u2019s yard may be dark red and not green like ours.\n\nIt is even more difficult to forecast what technologies that are billions of years old would look like. In searching for them, we must flag anomalies seen through our telescopes and not sweep unexpected signals under the rug of conservatism. If our instruments are not sensitive enough or our search techniques are inadequate, we will not discover technosignatures. Processing data without suitable machine-learning algorithms may resemble casting an ineffective fishing net that never catches fish because its holes are too big.\n\nWe design our searches based on what we see in the mirror. After radio communication and lasers were invented, we started searching for radio and laser signals from outer space; search considerations progressed similarly with the technology of light sails. As we imagine new technologies, we may ultimately find the one that would allow us to detect many other species who use it.\n\nHowever, we should take caution with anecdotal observations that are not up to the standards of quantitative scientific evidence. This includes conspiracy theories without supporting evidence, which appear with some regularity, or reports about unidentified flying objects (UFOs), which do not stand up to the scrutiny of reproducibility\u2014the prerequisite for being counted as credible scientific data. UFO reports provide clues that are always on the borderline of detectability. Since our recording devices have improved considerably over time, one would expect that a fuzzy photo taken by an old camera from 50 years ago would turn into a crisp image in today\u2019s advanced cameras, thus providing conclusive evidence beyond any reasonable doubt.\n\nBut the clues are always marginal, implying that UFOs are most likely artifacts in our instruments or natural phenomena. To achieve scientific credibility, any finding of an unusual object must be followed by studying it or other objects of its type quantitatively through well-documented scientific procedures. Scientific evidence restricts our imagination and brings salvation from far-fetched ideas.\n\nFermi\u2019s paradox is pretentious in that it assumes we humans have some sort of cosmic significance. The reality may be that we are ordinary and doomed to perish, just like the dinosaurs, consequent to some catastrophe. Why would our galactic neighbors care about how green our grass is? Given that dwarf stars like Proxima Centauri are much more abundant than the sun, most habitable planets might be covered with dark red grass, which would be as soothing to the infrared eyes of most exo-vacationers as green grass is to us. As a result, interstellar tourist agencies may find Proxima b to be a more attractive destination than Earth. We could wonder, as Enrico Fermi did, why no exo-tourists have shown up to admire us. But better yet, we could get in touch with Proxima b and entice the locals to visit and share a water-based drink with us."
        },
        {
            "authors": [
                "Peter Corless",
                "Ivan Prisyazhynyy",
                "About Ivan Prisyazhynyy",
                "Ivan Is A Software Engineer Interested In Distributed Systems",
                "Open-Source",
                "Databases. He Came The Background Of Web Projects",
                "Infrastructure Systems To Help Scylla Make Its Way To The Cloud."
            ],
            "title": "CockroachDB vs. Scylla Benchmark",
            "contents": "The database space has always been a complicated field that introduced many options and tradeoffs. Two particular classes of databases, NoSQL and NewSQL, have been most recently pitted against each other in the industry. However, both terms were coined long ago. These two families of databases offer expanded capabilities beyond traditional databases. They also often overlap so that today the boundaries are even more blurred as more new database implementations are born.\n\nIn this article, we compare what we consider the best of breed in NoSQL versus the best in class in NewSQL. Admittedly biased, we selected ourselves for NoSQL. For NewSQL, we chose CockroachDB. The latter represents distributed SQL, the segment which represents not only the SQL API but also the distributed relational database model.\n\nObviously, the comparison is of the apples and oranges type. We expect Scylla will be faster \u2014 providing lower latencies and greater throughputs \u2014 while CockroachDB should have stronger consistency and be friendlier to use with its SQL interface. Our goal was to put forward a rational analysis and put a price tag on the differences in workloads that could be addressed by the two databases. So we won\u2019t cover SQL JOINs which are supported by CockroachDB alone and we won\u2019t cover timeseries workloads where Scylla has a clear design advantage.\n\nTAKE ME STRAIGHT TO THE BENCHMARK RESULTS\n\nBackground\n\nCockroachDB (CRDB) was built from the ground up to support global OLTP workloads while maintaining high availability and strong consistency. It is based on ideas from Google Spanner [whitepapers] [2012-GDD] [2017-TT] [2017-SQL] (and F1). Cockroach Labs, the company that supports it, was founded in 2014.\n\nScylla has likewise been under development since 2014. It was modeled after Apache Cassandra and Amazon DynamoDB, supporting API compatibility for both.\n\nScylla\n\nScylla is a wide-column distributed NoSQL database that uses an eventual consistency model to provide fast and efficient reads and writes along with multi-datacenter high availability.\n\nIn Scylla all nodes are equal: there are no leaders, followers, or replica sets. And there is no single point of failure; client applications can interact with any node in the cluster. This allows Scylla to scale linearly to many nodes without performance degradation and overhead costs.\n\nPerformance\n\nScylla is a Cassandra rewrite in C++, however, the performance gains arrive from its radical, asynchronous, shard-per-core design and its desire to achieve full control of the entire resource allocation and execution.\n\nThe computing model is completely async with futures and promises and has its own task switching mechanism, developed in order to run a million continuations (lambda functions) per second per core. In a high performance database, control is even more important than efficiency. Scylla makes every computation and IO operation belong to a priority class. Its CPU and IO schedulers control the execution of all continuations. Latency sensitive classes such as read and write operations are given a higher, dynamic priority over background tasks such as compaction, repair, streaming, etc.\n\nScylla makes sure the OS does not play its traditional role by pinning threads to cores, memory to shards nor overload the filesystem/disks with IO beyond their capacity. Compaction controllers measure the compaction debt and dynamically set the compaction priority. Thus compaction control, which makes Log Structured Merge (LSM) trees complicated, is a solved problem with Scylla. More on the thread-per-core approach can be found here, and on seastar.io, our application framework. These capabilities to precisely control resource utilization are behind another unique feature of Scylla: workload prioritization, where different workloads can have different priorities, permitting OLTP and OLAP (driven by Presto/Spark) workloads to co-exist running simultaneously in the same cluster on the same hardware.\n\nAvailability\n\nScylla uses an eventual consistency model. Many workloads do not require strong consistency guarantees while they do require availability. For example, during partitioning, an isolated datacenter should continue to accept reads and writes. Scylla has an API for stronger consistency using lightweight transactions (LWT).\n\nScylla can even be tuned per transaction, so that you can state the desired level of consistency. Do you just want the transaction to succeed if even one node acknowledges a read or write (a consistency level of 1, or CL=1)? If so, the rest of the nodes will be caught up in due time. Or do you want a majority of replica nodes to acknowledge a read or write (CL=QUORUM, LOCAL_QUORUM, ..)? Or do you want every replica node to acknowledge that the transaction successfully completed (CL=ALL)?\n\nData Distribution\n\nIn Scylla data is spread as uniformly as possible across all nodes by the means of the hash function (partitioner) so that the load can be evenly distributed and efficiently processed by all cores in the cluster. Token ring and cluster topology configuration is shared with the clients. This makes clients aware of the nodes where data actually resides. They can efficiently choose the closest nodes that own the data and even reach the specific cpu core that handles partition within the node, minimizing extra hops and maximizing load balancing.\n\nRead and Write Path\n\nClients choose the operations coordinator either randomly or as close to the target replicas as possible and send their requests. Multiple policies exist to select the coordinator with different load balancing options. The coordinator asynchronously replicates writes to the number of replicas. A user can pick the number of replicas data shall be read from. That makes request processing very predictable in terms of implied operations, IO and round trips.\n\nScylla uses a commitlog and in parallel memtable, SSTables, etc. You can read more about Scylla\u2019s read path in this article about our row-level cache, and more on the write path in our documentation.\n\nData Model\n\nScylla offers a natural RDBMS-like model when data is organized in tables that consist of rows and columns on top of the wide-column storage. A row key consists of a partition key and optionally clustering key. A clustering key defines rows ordering inside of the partition. A partition key determines partition placement.\n\nUsers define tables schemas, insert data into rows, and then read it. There are usual concepts such as Secondary Indexes, Materialized Views, Complex Data Types, Lightweight Transactions, and other features built on top.\n\nThe wide-column data model differs from the classical RDBMS-style model in that rows are not first-class citizens but the cells are. Rows consist of cells.\n\nCQL and ACID\n\nThough the Scylla CQL user language can be deceptively very similar to what most of us are used to with SQL:\n\nSELECT * FROM Table; UPDATE Table (a, b, c) VALUES (1, 2, 3) WHERE Id = 0;\n\nHowever, Scylla does not provide full ACID semantics for its operations. Usually, ACID is applied to transactions but let\u2019s take a look at what is provided for a single operation:\n\natomicity is provided with a commitlog\n\nconsistency is provided in an ACID sense that it preserves application invariants if the operations preserve them\n\ndurability is provided with a commit log and replication.\n\nWhat Scylla does not provide from an ACID perspective is isolation. But why would you need isolation if you don\u2019t have multi-statement cross-partition transactions? If you want to know a detailed analysis to this question, you can read the Jepsen analysis of Scylla here \u2014 section 3.4 Normal Writes Are Not Isolated, and our accompanying post here.\n\nLimitations\n\nWith such great flexibility and freedom there comes a great price.\n\nData ordering prohibits efficient sequential primary key scanning because every partition has random placement. This limits an opportunity to make range-based JOIN operations. Even though it allows sequential local scan, it is a tradeoff that Scylla makes in order of delivering maximum performance possible out of available hardware.\n\nCell-based data organization along with the timestamp non-monotonicity prevents an efficient usage of levels in LSM storage and opens an opportunity for torn row writes.\n\nWhile all of the nodes in Scylla are homogenous and cluster resizing is easy, the data is sharded through key range allocations to nodes. Sharding is not transactional thus only done in a serial way and there is no software enforcement. In addition, these ranges are static once the topology is set and can result in hot shards. Our recently-announced Project Circe is about to address these limitations.\n\nLack of multi-partition transactions prevents users from developing applications that require higher consistency guarantees.\n\nCockroachDB\n\nCockroachDB is a distributed NewSQL database built from the ground up to support global OLTP workloads while maintaining high availability and strong consistency.\n\nIt is built on the foundation of ideas that stand behind Google\u2019s Spanner database. CockroachDB focuses on providing fully serializable ACID transactions on top of its strongly consistent highly-available distributed KV store. Its transactions are serializable and reads are linearizable from the beginning by default.\n\nIt has a completely different design than Scylla and exploits that to serve SQL requests efficiently.\n\nPerformance\n\nCockroachDB\u2019s primary focus is consistency, topology flexibility and SQL compatibility. It provides high availability the same way Scylla does \u2014 with redundancy, but keeps replicas consistent throughout operations. Maintaining consistency all the time implies additional overhead. However, CockroachDB uses different innovative approaches to provide high performance quality of service such as Raft with Leases, cost-free linearization, parallel two-phase commit, an innovative hybrid transactions scheduler, vectorization, in-memory data layout optimization and more. CockroachDB is written in Go and susceptible to garbage collection spikes.\n\nAvailability\n\nCockroachDB favors consistency in place of availability. It provides different options to maintain high availability and build hybrid cluster topologies. Availability is based on redundancy. Because of Raft to be available every ReplicaSet requires a quorum of its nodes to be alive. This limits availability.\n\nData Distribution\n\nIn an architectural sense, it is very similar to Google Spanner: ordered data keyspace is split into Data Ranges or Tablets. Data range replicas are grouped into ReplicaSets. Each ReplicaSet has a dedicated leader that is determined by the consensus process. For consensus, CockroachDB uses Raft with different optimizations.\n\nRead and Write Path\n\nIn every ReplicaSet, there is only one dedicated node that serves reads and writes. It is called a leaseholder. Only the leaseholder can offer a write to the ReplicaSet leader. Because there always exists only one leaseholder in a group, reads that served from it are linearizable.\n\nNow it must be clear that reads are cheap and linearizable. The writes in its turn are synchronous \u2014 the transaction coordinator always waits until all writes would be replicated before committing the transaction. Transaction commit is asynchronous though.\n\nAll reads and writes in CockroachDB execute in the context of transactions. A transaction is an interactive session with respect to the ACID properties in which a client sends requests and then finalizes them with a commit or abort keyword.\n\nData Model\n\nCockroachDB offers a classical relational data model with tables and rows built on top of LSM-based key-value storage. CockroachDB is wire compatible with PostgreSQL.\n\nConsistency and Isolation\n\nCockroachDB currently supports only one transaction execution mode: SERIALIZABLE isolation. This is good when a user needs strong isolation guarantees free of anomalies. CockroachDB does not offer picking a weaker isolation model for higher performance.\n\nTo serialize transactions CockroachDB offers a parallel two-phase commit variant and a novel hybrid serialization scheduler. In the best simple case, CockroachDB is capable of committing a transaction in 1 Round Trip Time (RTT). In general, though, it requires a serialization check on every read and write and waiting that all writes were replicated. To atomically switch data visibility CockroachDB uses indirection in the read path.\n\nOverall, CockroachDB provides near strong-1SR consistency.\n\nTablets\n\nCockroachDB offers maximum flexibility on data placement policy. Users have control over how data is organized on disk (families) and how it is split and distributed across the world. The model supports independent data rebalance (instead of a ring rebalance), an independent isolated cluster topology change, and even data access control.\n\nThis design approach allows CockroachDB to flexibly and seamlessly control cluster membership. Nodes can be added or removed instantly and in parallel into the cluster and they can start serving data as soon as they have the first data range replicated. Data split and rebalance occurs automatically in the background based on the load distribution and resources utilization.\n\nIn data modeling, a user has great flexibility in keeping all data in an ordered fashion or randomly distributing it in the keyspace depending on his load patterns.\n\nData replicas are always kept in a consistent state. That gives an opportunity to compact delete tombstones in the LSM trees right away and simplifies many operations.\n\nLimitations\n\nWrites inside a ReplicaSet require consensus coordination and are limited in throughput in that sense. To mitigate this CockroachDB implies dynamic data ranges splitting and rebalancing.\n\nClocks instability affects replicas performance.\n\nTransactions require serialization that is not cost-free and contended keys transactions do not scale.\n\nReads preliminary are served only from the leaseholder nodes which means that you don\u2019t utilize the rest of the replicas.\n\nIt\u2019s easier to miss load distribution and get hot data ranges. If load distribution is uneven hot data ranges will affect performance.\n\nAvailability guarantees are weaker. There is an overhead implied by the language with automatic memory management.\n\nBenchmarking\n\nOverall both databases are focused on different things and use different approaches to serve reads and write. However, let\u2019s take a look at their performance.\n\nIn order to measure performance differences, we ran YCSB workloads on AWS for Scylla and CockroachDB with two datasets, with sizes of 1B and 100M keys. All clusters consisted of 3 \u00d7 i3.4xlarge AWS EC2 nodes (each with 16 vCPU, 122 GiB RAM, 10GiB network, and 2 \u00d7 1.9TiB NVMe SSDs) in a single region (eu-north-1) spread across 3 availability zones (abc) with the standard replication factor (RF=3) and almost default configuration.\n\nTo measure CockroachDB performance, we used the brianfrankcooper/YCSB 0.17.0 benchmark with PostgreNoSQL binding and CockroachDB v20.1.6 YCSB port to the Go programming language that offers better support for this database. For Scylla 4.2.0, we used the brianfrankcooper/YCSB 0.18.0 with a Scylla-native binding and a Token Aware balancing policy.\n\nData loading\n\nWhile the official CockroachDB documentation states that the storage capacity limit is 150GiB per vCPU and up to 2.5 TB per node total, we did not manage [#56362 with 20.1.6] [#38778 with 20.2.0] to successfully load 1B keys into the CockroachDB cluster \u2014 for most of the trials it went unresponsive after about 3-5 hours of loading with some critical errors in the logs. A few times we observed a similar behavior during a 30 minute long sustained workload.\n\nAnother problem was load throughput degradation from 12K TPS down to 2.5K TPS in 3-5 hours. Loading 1B keys at a rate of 2.5K keys inserted per second could take about 111 hours or 4.5 days. We decided not to wait for it. Similar issues were observed by YugaByte: [1B trial], [slowdown] and [results].\n\nWe reduced the dataset size for CockroachDB to 100M. Loading took 7 hours and resulted in 1.1TB of data overall that later was compacted to 450GiB. The latency graph over this 7 hours period can be seen below.\n\nIn its turn, Scylla did well with 1B keys \u2014 Scylla loaded 4.8TB of data (before major compaction, 3.9 TB after) in about 3 hours and showed the same performance characteristics as with the smaller dataset.\n\nKey Observation: Loading 10 times the data into Scylla took less than half the time it took for CockroachDB. Scylla was over 20x more efficient in initial data loading.\n\nYCSB Workload A Results\n\nFrom YCSB\u2019s github: \u201cThis workload has a mix of 50/50 reads and writes. An application example is a session store recording recent actions.\u201d\n\nScylla showed the capability to produce 120K TPS under 60% CPU utilization with P99 latency <4.6ms, and 150K TPS with P99 <12ms for the 1B dataset size.\n\nIn this Grafana diagram, taken from the Scylla Monitoring Stack dashboard, you can see that 2 clients have 600 \u00b5sec P99 latency for writes, <4ms P99 latency for reads while serving 60k ops for reads and 60k ops for writes; 120K TPS in total.\n\nCockroachDB in this example of workload A produced at most 16K TPS with P99 52ms and intermediate spikes that reach 200ms at utilization varying from 50% \u2013 75%:\n\nKey Observation: Scylla handled 10x the amount of data, while providing 9.3x the throughput at 1/4th the latency.\n\nResults for Workloads A through F\n\nScylla\n\nFor the large, 1B key dataset, Scylla successfully managed to serve 150K-200K TPS on most of the workloads at 75-80% utilization with decent latency. One of the best Scylla results was 180K TPS with p99 latencies <5.5ms at average load 75% on workload D with 1B keys. 200K TPS resulted in small overload and gave latencies around 20-60ms.\n\nThese two tables present the throughput and latency achieved per workload.\n\nAs shown in this Grafana chart, it\u2019s not only the performance numbers that are better but also show how Scylla achieves almost maximum system utilization.\n\nFor example for workload D with 1B keys dataset, Scylla demonstrated 180K TPS with p99 latency of <5.5ms and CPU utilization only at 75%. This level of performance scalability is rare for most of the OSS distributed database systems.\n\nWorkload E produced the worst performance, only 10k ops. This was expected as the operations are short range scans instead of individual records. From YCSB: \u201cApplication example: threaded conversations, where each scan is for the posts in a given thread assumed to be clustered by thread id\u201d.\n\nWorkload E is an antipattern for Scylla unless it is modeled as a clustering key. Scylla\u2019s range partition scans are token-based which are randomly placed in the cluster, thus many random reads across multiple nodes are required to satisfy a single scan request.\n\nEven with that said, for the range scan use-case (E) Scylla outperformed CRDB by 5x.\n\nCockroachDB\n\nCockroachDB in its turn demonstrated performance scalability limits much earlier even with the 100M keys dataset: for workload A it showed 16K TPS with p99 of <52ms. It\u2019s best result was achieved with workload D that showed 40K TPS with p99 <176ms at 80% utilization. Further increasing of the load did not lead to any throughput growth, but only to the growth of latency and its variance.\n\nBelow are the workload E results with 2k TPS and 537ms P99 latency.\n\nConclusion\n\nNoSQL and NewSQL models are moving towards each other, each providing more functionality and better performance and availability than traditional database offerings. It is not a surprise that a NoSQL database such as Scylla outperforms a distributed SQL database as CockroachDB by a large margin. The results do not mean that one should select Scylla/NoSQL for every workload.\n\nWith 1B keys dataset, Scylla showed 5x to 10x better throughput and stable low latencies while handling 10x amount of data. CockroachDB demonstrated throughput degradation while data loading, and during the YCSB workloads we measured throughput that closely matched the CRDB whitepaper, yet with larger and greater varying latencies.\n\nMany modern workloads do not require strong consistency and can\u2019t be straightjacketed by the imposition of operational limits to enormous scale requirements. These workloads are ideal for Scylla. Other workloads, where strong consistency guarantees and transactions are required or the flexibility of a relational database model, with JOINs and sorted keys, and moderate amounts of data should consider a database such as CockroachDB.\n\nRecently at the Scylla summit we announced Project Circe, a 12 month roadmap plan that among other things adds the Raft consensus protocol and allows big improvements for strongly consistent workloads. CockroachDB on their end closed a huge round of funding and improved their LSM performance. Both databases are embraced by their respective communities for the capabilities they provide, and we hope our analysis helps you understand the differences between these systems. Stay tuned for more breakthroughs in this rapidly-evolving, fifty year old domain of distributed databases.\n\nAppendix A\n\nWorkload class CRDB Whitepaper* [TPS] CRDB Trial Throughput** [TPS] Scylla Trial Throughput [TPS] CRDB Whitepaper* Latency p99 [ms] CRDB Trial Latency p99** [ms] Scylla Trial Latency p99 [ms] YCSB A 20,000 16,000 150,000 <3 ms 52.4 ms 12 ms YCSB B 50,000 35,000 150,000 <3 ms 125.8 ms 7 ms YCSB C 62,000 38,000 180,000 <3 ms 56.6 ms 12 ms YCSB D 53,000 40,000 180,000 <3 ms 176.2 ms 5.5 ms YCSB E 17,000 2,000 10,000 <3 ms 536.9 ms 21 ms YCSB F 18,000 6,000 100,000 <3 ms 385.9 ms 26 ms\n\n* results observed by the CockroachDB authors published in section \u201c6.1 Scalability of CockroachDB\u201d and Figure 7: Throughput of CRDB and Spanner on YCSB A-F, Latency of CRDB and Spanner under a light load.\n\n** YugaByte results match with ours: [throughput], [latency], [1B trial], [slowdown].\n\nAppendix B\n\nAppendix C\n\nIt is possible to compare basic requests (SELECTs and INSERTs) of those 2 databases because their read and write paths in the best cases are similar in terms of implied round trips (RTTs). Specifically Scylla performs a Replication factor (RF) number of writes per insert waiting only for the Consistency level number of responses, and CL number of reads per select, while the CockroachDB can commit transaction in the best case in 1RTT replicating writes (RF) in parallel and serves reads directly from the lease holders (eq CL=1).\n\nThe best benchmark tool that can emulate different mixes of basic reads and writes operations is YCSB.",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [
                "Alastair Westgarth"
            ],
            "title": "Saying goodbye to Loon",
            "contents": "Saying goodbye to Loon\n\nLoon\u2019s journey is coming to an end. Thank you to everyone who believed in us and our mission of connecting people everywhere. Alastair Westgarth Follow Jan 22 \u00b7 4 min read\n\nI first discovered Loon through a photographer I follow who posted a snap of a balloon floating through the New Zealand skies. The image inspired a sense of wonder, awe \u2014 and possibility. I\u2019d never seen anything like that in my 30+ years working in the telecom industry. The more I learned, the more I wanted to be part of the mission. This sense of excitement and possibility has stayed with me over the last four years as I had the privilege to lead Loon in our mission to expand connectivity around the world.\n\nWe talk a lot about connecting the next billion users, but the reality is Loon has been chasing the hardest problem of all in connectivity \u2014 the last billion users: The communities in areas too difficult or remote to reach, or the areas where delivering service with existing technologies is just too expensive for everyday people. While we\u2019ve found a number of willing partners along the way, we haven\u2019t found a way to get the costs low enough to build a long-term, sustainable business. Developing radical new technology is inherently risky, but that doesn\u2019t make breaking this news any easier. Today, I\u2019m sad to share that Loon will be winding down.\n\nLoon\u2019s legacy\n\nI couldn\u2019t be prouder of the Loon team and their achievements. Loon\u2019s journey has been a series of \u201cfirsts\u201d as we\u2019ve tackled and solved a multitude of problems we couldn\u2019t have even imagined we\u2019d face when Loon started. Working side-by-side with governments and global aviation and communications regulators to showcase and enable these new technologies, we found ways to safely fly a lighter-than-air vehicle for hundreds of days in the stratosphere to anywhere in the world. We built a system for quickly and reliably launching a vehicle size of a tennis court, and we built a global supply chain for an entirely new technology and business. We also scaled up our communications equipment from technology that could have been made in a college dorm room (literally: WiFi routers inside styrofoam beer coolers), to a communications system capable of delivering mobile internet coverage over an 11,000 square kilometer area \u2014 200x that of an average cell tower.\n\nThe Loon team is proud to have catalyzed an ecosystem of organizations working on providing connectivity from the stratosphere. The world needs a layered approach to connectivity \u2014 terrestrial, stratospheric, and space-based \u2014 because each layer is suited to different parts of the problem. In this area, Loon has made a number of important technical contributions. This includes creating communications payloads that can connect from the stratosphere to many types of devices on the ground \u2014 from mobile phones to sailing ships to smart sprinklers \u2014 possibly paving the way for more options in unlocking the full potential of the Internet of Things. Loon also pioneered software that manages constellations of connectivity vehicles, ensuring they can provide the right service to the right area at the right time.",
            "published_at": "2021-01-22T00:20:02.548000+00:00"
        },
        {
            "authors": [],
            "title": "Ask HN: Anyone know any funny programming jokes?",
            "contents": "Can be super esoteric or super generalized, I love it when I get them, or when I just learn something new."
        },
        {
            "authors": [
                "Kyle Benzle"
            ],
            "title": "With Covid-19 Rates Rising Adding an Air Purifier To Your Home is the Single Most Important Thing You Can Do to Improve Your Health",
            "contents": "Particles with a diameter greater than 10 \u03bcm have a relatively small suspension half-life and are largely filtered out by the nose and upper airway.\n\nThose with a diameter between 2.5 and 10 \u03bcm (PM2.5\u201310) are classified as \u201ccoarse,\u201d less than 2.5 \u03bcm as \u201cfine,\u201d and less than 0.1 \u03bcm as \u201cultrafine\u201d particles.\n\nParticles <10 \u03bcm in diameter are capable of entering the respiratory system, and particles <2.5 \u03bcm are capable of reaching the alveoli and ultrafine particles systemically affect the blood and organs such as the heart and even the brain.\n\n4. FILTRATION OF INDOOR AIR\n\nSeveral measures are recommended to reduce exposure to contaminants of biological origin (dust mites, household pets, mold and mice) and non-biological origin (tobacco smoke, wood smoke, volatile organic compounds). With a better understanding of indoor pollutants, new and effective measures have evolved, including the development of indoor air filters.\n\nAir filtration is frequently recommended as a component of environmental control measures. Indoor air filtration can be provided by whole house filtration via the home\u2019s heating, ventilation, or air conditioning system, by portable room air cleaners, or a combination of the two.\n\nThe key attribute of any air filter, is a balance of the following:\n\nAir flow to assure adequate ventilation.\n\nEfficiency to filter out a range of small particle sizes.\n\nCapacity to allow for reasonable cost-effective maintenance schedules without adversely affecting airflow and efficiency.\n\nCurrently available air purifiers usually use a multilayer filter system composed, often of a prefilter, a carbon filter, an antibacterial filter, and a HEPA filter.\n\nThe use of HEPA filters traditionally used in hospitals, has indeed been a significant inclusion to home air purifiers. A HEPA filter uses mechanical filtration to remove airborne particles. A HEPA filter is standardized at a minimum 99.97% efficiency rating for removing particles greater than or equal to 0.3\u03bcm in diameter.\n\n5. AIR FILTER EFFICACY\n\nA study by van der Heide et al., assessed the efficacy of air-cleaners with respect to their capacity to capture airborne allergen particles. Over a 6-month period, the efficacy of air filters to capture particulate matter and allergens was measured. The study included three interventions -application of active air-cleaners in living-rooms and bedrooms, placebo air-cleaners used in combination with allergen-impermeable mattress covers or active air-cleaners used in combination with allergen-impermeable mattress covers.\n\nThe last filter consisted of a high efficiency particulate air (HEPA)-type filter, filtering 70% of 0.3-\u03bcm particles and 95% of 1.0-\u03bcm particles. The air cleaners in this study clearly showed the capacity to capture substantial amounts of airborne dust particles and airborne allergens.\n\nAnother study, a randomized controlled trial, evaluated the effectiveness of free-standing air filters and window air conditioners in 126 low-income households of children with asthma. It was found that a reduction in PM, by an average of 69 to 80% suggested that while PM levels in homes with asthmatic children can be high, levels can be dramatically reduced using filters.\n\n6. AIR FILTERS IMPROVE OVERALL HEALTH\n\nIn a year-long, randomized, parallel-group study, Francis et al., measured the clinical outcomes for the use of indoor HEPA air cleaners of 30 adult asthmatics who were sensitized to, yet lived with an indoor cat or dog. Outcomes were statistically improved in the treatment group over the controls.\n\nAnother study by Sulser et al., compared sham versus HEPA portable room air cleaners in asthmatic children sensitized to cat or dog. A significant reduction in nocturnal symptoms including stuffy nose was observed in the HEPA filter group.\n\nExposure to particulate matter is associated with risk of cardiovascular events, as a consequence of oxidative stress and inflammation.\n\nThe effects of controlled exposure to indoor air particles were studied in a healthy elderly population. The study suggested that a reduction of particle exposure by filtration of recirculated indoor air for only 48 hours improved lung health elderly citizens and suggested that this may be a feasible way of reducing the risk of cardiovascular disease\n\nIn one study by Weichenthal et al. the benefits of an electrostatic air filter was assessed in residents from 20 homes. The indoor PM2.5 decreased substantially during the period when air filter was used relative to placebo and on average, air filter use was associated with a decrease in systolic blood pressure.\n\nSUMMARY\n\nDespite the rapid rise in environmental pollutants, the causal pathways leading to adverse health effects is often complex and poorly understood.\n\nChildren, the elderly, and women are most vulnerable to potential indoor air pollution health effects because they spend more time in the home environment.\n\nThere are many sources of indoor air pollution. Air pollution inside homes consists of a complex mixture of agents penetrating from ambient (outdoor) air and agents generated by indoor sources. Indoor pollutants can vary in their potential health effects and intensity, as well as in their distribution across geographic areas, cultural backgrounds, and socioeconomic status. Exposure to indoor air pollutants can cause health effects ranging from sneezing and coughing to exacerbation of chronic respiratory disorders such as asthma and outcomes such as cardiovascular disease and even cancer.\n\nStudies appear to suggest, that reduction in particulate matter and allergens results in reducing symptoms and in certain cases, preventing disease progression across all age groups, including the elderly and children. The evidence is apparent, in chronic respiratory diseases, such as asthma and in cardiovascular health.\n\nTechnologically advanced air filter systems are now available which efficiently remove particulate matter, resulting in significant health benefits to patients of asthma and cardiovascular disease.\n\nKEY WORDS: Air filters, air pollution, cardiorespiratory health, enhancing indoor air quality, HEPA\n\nREFERENCES\n\n1. Ambient (outdoor) Air Quality and Health, Fact Sheet \u2116313. World Health Organisation. [Last accessed on 2015 Aug 20]. Available from: http://www.who.int/mediacentre/factsheets/fs313/en/\n\n2. Household Air Polluton and Health, Fact Sheet \u2116292. World Health Organisation. [Last accessed on 2015 Aug 20]. Available from: http://www.who.int/mediacentre/factsheets/fs292/en/\n\n3. Suades-Gonz\u00e1lez E, Gascon M, Guxens M, Sunyer J. Air Pollution and Neuropsychological Development: A Review of the Latest Evidence. Endocrinology. 2015 en20151403 [Epub ahead of print] [PMC free article] [PubMed] [Google Scholar]\n\n4. Stafoggia M, Cesaroni G, Peters A, Andersen ZJ, Badaloni C, Beelen R, et al. Long-term exposure to ambient air pollution and incidence of cerebrovascular events: Results from 11 european cohorts within the escape project. Environ Health Perspect. 2014;122:919\u201325. [PMC free article] [PubMed] [Google Scholar]\n\n5. Cesaroni G, Forastiere F, Stafoggia M, Andersen ZJ, Badaloni C, Beelen R, et al. Long term exposure to ambient air pollution and incidence of acute coronary events: Prospective cohort study and meta-analysis in 11 European cohorts from the ESCAPE project. BMJ. 2014;348:f7412. [PMC free article] [PubMed] [Google Scholar]\n\n6. MacIntyre EA, Gehring U, M\u00f6lter A, Fuertes E, Kl\u00fcmper C, Kr\u00e4mer U, et al. Air pollution and respiratory infections during early childhood: An analysis of 10 European birth cohorts within the ESCAPE Project. Environ Health Perspect. 2014;122:107\u201313. [PMC free article] [PubMed] [Google Scholar]\n\n7. Gehring U, Gruzieva O, Agius RM, Beelen R, Custovic A, Cyrys J, et al. Air pollution exposure and lung function in children: The ESCAPE project. Environ Health Perspect. 2013;121:1357\u201364. [PMC free article] [PubMed] [Google Scholar]\n\n8. Lim SS, Vos T, Flaxman AD, Danaei G, Shibuya K, Adair-Rohani H, et al. A comparative risk assessment of burden of disease and injury attributable to 67 risk factors and risk factor clusters in 21 regions, 1990\u20132010: A systematic analysis for the Global Burden of Disease Study 2010. Lancet. 2012;380:2224\u201360. [PMC free article] [PubMed] [Google Scholar]\n\n9. WHO Guidelines for indoor air quality: Selected pollutants. [Last accessed on 2014 Sep 12]. Available from: http://www.euro.who.int/__data/assets/pdf_file/0009/128169/e94535.pdf .\n\n10. Gupta D, Agarwal R, Aggarwal AN, Maturu VN, Dhooria S, Prasad KT, et al. S.K. Jindal for the COPD Guidelines Working Group. Guidelines for diagnosis and management of chronic obstructive pulmonary disease: Joint ICS/NCCP (I) recommendations. Lung India. 2013;30:228\u201367. [PMC free article] [PubMed] [Google Scholar]\n\n11. Salvi S, Agrawal A. India needs a national COPD prevention and control programme. J Assoc Physicians India. 2012;60(Suppl):5\u20137. [PubMed] [Google Scholar]\n\n12. Gaude GS, Hattiholi J, Chaudhury A. Role of health education and self-action plan in improving the drug compliance in bronchial asthma. J Family Med Prim Care. 2014;3:33\u20138. [PMC free article] [PubMed] [Google Scholar]\n\n13. Jindal SK, Aggarwal AN, Gupta D, Agarwal R, Kumar R, Kaur T, et al. Indian study on epidemiology of asthma, respiratory symptoms and chronic bronchitis in adults (INSEARCH) Int J Tuberc Lung Dis. 2012;16:1270\u20137. [PubMed] [Google Scholar]\n\n14. Paramesh H. Epidemiology of asthma in India. Indian J Pediatr. 2002;69:309\u201312. [PMC free article] [PubMed] [Google Scholar]\n\n15. Upadhyay RP. An overview of the burden of non-communicable diseases in India. Iran J Public Health. 2012;41:1\u20138. [PMC free article] [PubMed] [Google Scholar]\n\n16. [Last accessed on 2015 Mar 6]. Available from: http://www.education.nationalgeographic.com/education/encyclopedia/air-pollution/?ar_a=1 .\n\n17. Guarnieri M, Balmes JR. Outdoor air pollution and asthma. Lancet. 2014;383:1581\u201392. [PMC free article] [PubMed] [Google Scholar]\n\n18. Anderson JO, Thundiyil JG, Stolbach A. Clearing the air: A review of the effects of particulate matter air pollution on human health. J Med Toxicol. 2012;8:166\u201375. [PMC free article] [PubMed] [Google Scholar]\n\n19. Diette GB, McCormack MC, Hansel NN, Breysse PN, Matsui EC. Environmental issues in managing asthma. Respir Care. 2008;53:602\u201317. [PMC free article] [PubMed] [Google Scholar]\n\n20. Rumana HS, Sharma RC, Beniwal V, Sharma AK. A retrospective approach to assess human health risks associated with growing air pollution in urbanized area of Thar Desert, western Rajasthan. J Environ Health Sci Eng. 2014;12:23. [PMC free article] [PubMed] [Google Scholar]\n\n21. Patra S, Sharma S, Behera D. Passive smoking, indoor air pollution and childhood tuberculosis: A case control study. Indian J Tuberc. 2012;59:151\u20135. [PubMed] [Google Scholar]\n\n22. King BA, Mirza SA, Babb SD. GATS Collaborating Group. A cross-country comparison of secondhand smoke exposure among adults: Findings from the Global Adult Tobacco Survey (GATS) Tob Control. 2013;22:e5. [PMC free article] [PubMed] [Google Scholar]\n\n23. Singh P, Kaur M, John S. Assessment of human health effects associated with exposure to indoor air pollution. Int J Appl Engineer Res. 2012;7:1\u20135. [Google Scholar]\n\n24. Yerramsetti VS, Sharma AR, Gauravarapu Navlur N, Rapolu V, Dhulipala NS, Sinha PR. The impact assessment of Diwali fireworks emissions on the air quality of a tropical urban site, Hyderabad, India, during three consecutive years. Environ Monit Assess. 2013;185:7309\u201325. [PubMed] [Google Scholar]\n\n25. Lawrence A, Fatima N. Urban air pollution and its assessment in Lucknow City \u2014 the second largest city of North India. Sci Total Environ. 2014:488\u2013489. 447\u201355. [PubMed] [Google Scholar]\n\n26. Goyal R, Khare M. Indo air quality modelling for PM 10, PM 2.5, PM 2.5, and PM 1.0 in naturally ventilated classrooms of an urban Indian school building. Environ Monit Assess. 2011;176:501\u201316. [PubMed] [Google Scholar]\n\n27. Firdaus G, Ahmad A. Indoor air pollution and self-reported diseases \u2014 A case study of NCT of Delhi. Indoor Air. 2011;21:410\u20136. [PubMed] [Google Scholar]\n\n28. Kumar R, Nagar JK, Kumar H, Kushwah AS, Meena M, Kumar P, et al. Indoor air pollution and respiratory function of children in Ashok Vihar, Delhi: An exposure-response study. Asia Pac J Public Health. 2008;20:36\u201348. [PubMed] [Google Scholar]\n\n29. Kumar A, Scott Clark C. Lead loadings in household dust in Delhi, India. Indoor Air. 2009;19:414\u201320. [PubMed] [Google Scholar]\n\n30. Kulshreshtha P, Khare M, Seetharaman P. Indoor air quality assessment in and around urban slums of Delhi city, India. Indoor Air. 2008;18:488\u201398. [PubMed] [Google Scholar]\n\n31. Sharman JE, Cockcroft JR, Coombes JS. Cardiovascular implications of exposure to traffic air pollution during exercise. QJM. 2004;97:637\u201343. [PubMed] [Google Scholar]\n\n32. Bonner JC. Nanoparticles as a potential cause of pleural and interstitial lung disease. Proc Am Thorac Soc. 2010;7:138\u201341. [PMC free article] [PubMed] [Google Scholar]\n\n33. Health Quality Ontario. Air cleaning technologies: An evidence-based analysis. Ont Health Technol Assess Ser. 2005;5:1\u201352. [PMC free article] [PubMed] [Google Scholar]\n\n34. Sublett JL. Effectiveness of air filters and air cleaners in allergic respiratory diseases: A review of the recent literature. Curr Allergy Asthma Rep. 2011;11:395\u2013402. [PMC free article] [PubMed] [Google Scholar]\n\n35. van der Heide S, Kauffman HF, Dubois AE, de Monchy JG. Allergen reduction measures in houses of allergic asthmatic patients: Effects of air-cleaners and allergen-impermeable mattress covers. Eur Respir J. 1997;10:1217\u201323. [PubMed] [Google Scholar]\n\n36. Batterman S, Du L, Mentz G, Mukherjee B, Parker E, Godwin C, et al. Particulate matter concentrations in residences: An intervention study evaluating stand-alone filters and air conditioners. Indoor Air. 2012;22:235\u201352. [PMC free article] [PubMed] [Google Scholar]\n\n37. Du L, Batterman S, Parker E, Godwin C, Chin JY, O\u2019Toole A, et al. Particle concentrations and effectiveness of free-standing air filters in bedrooms of children with asthma in Detroit, Michigan. Build Environ. 2011;46:2303\u201313. [PMC free article] [PubMed] [Google Scholar]\n\n38. Francis H, Fletcher G, Anthony C, Pickering C, Oldham L, Hadley E, et al. Clinical effects of air filters in homes of asthmatic adults sensitized and exposed to pet allergens. Clin Exp Allergy. 2003;33:101\u20135. [PubMed] [Google Scholar]\n\n39. Sulser C, Schulz G, Wagner P, Sommerfeld C, Keil T, Reich A, et al. Can the use of HEPA cleaners in homes of asthmatic children and adolescents sensitized to cat and dog allergens decrease bronchial hyper responsiveness and allergen contents in solid dust? Int Arch Allergy Immunol. 2009;148:23\u201330. [PubMed] [Google Scholar]\n\n40. van der Heide S, van Aalderen WM, Kauffman HF, Dubois AE, de Monchy JG. Clinical effects of air cleaners in homes of asthmatic children sensitized to pet allergens. J Allergy Clin Immunol. 1999;104:447\u201351. [PubMed] [Google Scholar]\n\n41. Pedroletti C, Millinger E, Dahl\u00e9n B, S\u00f6derman P, Zetterstr\u00f6m O. Clinical effects of purified air administered to the breathing zone in allergic asthma: A double-blind randomized cross-over trial. Respir Med. 2009;103:1313\u20139. [PubMed] [Google Scholar]\n\n42. Br\u00e4uner EV, Forchhammer L, M\u00f8ller P, Barregard L, Gunnarsen L, Afshari A, et al. Indoor particles affect vascular function in the aged: An air filtration-based intervention study. Am J Respir Crit Care Med. 2008;177:419\u201325. [PubMed] [Google Scholar]\n\n43. Allen RW, Carlsten C, Karlen B, Leckie S, van Eeden S, Vedal S, et al. An air filter intervention study of endothelial function among healthy adults in a woodsmoke-impacted community. Am J Respir Crit Care Med. 2011;183:1222\u201330. [PubMed] [Google Scholar]\n\n44. Weichenthal S, Mallach G, Kulka R, Black A, Wheeler A, You H, et al. A randomized double-blind crossover study of indoor air filtration and acute changes in cardiorespiratory health in a First Nations community. Indoor Air. 2013;23:175\u201384. [PubMed] [Google Scholar]\n\n45. British Guideline on the Management of Asthma, British Thoracic Society May 2008, revised May 2011. [Last accessed on 2014 Jul 21]. Available from: https://www.brit-thoracic.org.uk/document-library/clinical-information/asthma/btssign-asthma-guideline-2011 .",
            "published_at": "2020-12-03T16:11:54.836000+00:00"
        },
        {
            "authors": [],
            "title": "As Adobe Flash stops running, so do some railroads in China \uff5c Apple Daily",
            "contents": "",
            "published_at": "2021-01-17T00:00:00"
        },
        {
            "authors": [
                "Jon Porter",
                "Jan"
            ],
            "title": "Gaze in wonder at this astonishingly high-res scan of an iconic painting",
            "contents": "For the past hour, I\u2019ve been avoiding work by browsing this amazingly high-resolution scan of Johannes Vermeer\u2019s iconic Girl with a Pearl Earring. With a total resolution of 93,205 x 108,565, PetaPixel notes the scan is believed to be the first 10 billion pixel (10-gigapixel) panorama ever created, allowing you to zoom in close enough to turn the tiniest flecks of paint into puddles and minuscule cracks into crevasses. The scan appears to have been posted online early last year.\n\nThe Mauritshuis art gallery, which normally houses the work, recently had to temporarily close due to COVID restrictions. But for the time being, this scan is a nice replacement. In some ways, it\u2019s actually better, letting you press your virtual nose up against the painting in a way that\u2019d get you thrown out of most art galleries or scolded by an irate assistant at the very least.\n\nThe scan is the work of Hirox\u2019s Emilien Leonhardt and Vincent Sabatier, who photographed the painting using a high-resolution microscope back in March 2018. Scanning the painting involved taking around 9,100 photographs of it using a high-resolution microscope before stitching them together. The resulting scan allowed the team to assess its condition, learn more about Vermeer\u2019s painting technique, and understand past restorations of the work. You can learn more about the scanning process in the short video Hirox published below:\n\nThe 2D image is one thing, but where things get especially interesting is with the 3D scans, which cover 10 specific areas of the painting like the subject\u2019s eyes and iconic earring. These scans let you peer at sections of the painting from any angle and see that its apparent flat surface is anything but, thanks to the layers of dried paint that make up the painting. There\u2019s even a small virtual light you can drag around inside the program to see how these surface imperfections cast a shadow over their surrounding area.\n\nTo learn more about what these scans and other technical research show about this painting, check out this blog post from the Mauritshuis last year. Meanwhile, the rest of the museum has also been digitized, with 36 masterpieces available to view in detail.",
            "published_at": "2021-01-21T12:32:55-05:00"
        },
        {
            "authors": [],
            "title": "Screensaver lock by-pass via the virtual keyboard \u00b7 Issue #354 \u00b7 linuxmint/cinnamon-screensaver",
            "contents": "Have a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.\n\nPick a username Email Address Password Sign up for GitHub\n\nBy clicking \u201cSign up for GitHub\u201d, you agree to our terms of service and privacy statement. We\u2019ll occasionally send you account related emails.\n\nAlready on GitHub? Sign in to your account"
        },
        {
            "authors": [
                "Sofia Moutinho"
            ],
            "title": "Why cats are crazy for catnip",
            "contents": "Why cats are crazy for catnip\n\nCat owners flood the internet with videos of their kitties euphorically rolling and flipping out over catnip-filled bags and toys. But exactly how catnip\u2014and a substitute, known as silver vine\u2014produces this feline high has long been a mystery. Now, a study suggests the key intoxicating chemicals in the plants activate cats\u2019 opioid systems much like heroin and morphine do in people. Moreover, the study concludes that rubbing the plants protects the felines against mosquito bites.\n\n\u201cThis study essentially has revealed a new potential mosquito repellent\u201d by examining the \u201cpharmaceutical knowledge\u201d of cats, says Emory University biologist Jacobus de Roode, who did not participate in the study.\n\nCatnip (Nepeta cataria) and silver vine (Actinidia polygama) both contain chemical compounds called iridoids that protect the plants against aphids and are known to be the key to the euphoria produced in cats. To determine the physiological effect of these compounds, Iwate University biologist Masao Miyazaki spent 5 years running different experiments using the plants and their chemicals.\n\nFirst, his team extracted chemicals present in both catnip and silver vine leaves and identified the most potent component that produces the feline high: a minty silver vine chemical called nepetalactol that had not been shown to affect cats until this study. (The substance is similar to nepetalactone, the key iridoid in catnip.) Then, they put 10 leaves\u2019 worth of nepetalactol into paper pouches and presented them, together with pouches containing only a saline substance, to 25 domestic cats to gauge their response. Most of the animals only showed interest in the pouches with nepetalactol.\n\nTo make sure this was the object of the felines\u2019 attraction, they repeated the experiment with 30 feral cats\u2014and one leopard, two lynxes, and two jaguars living in Japan\u2019s Tennoji and Oji zoos. Big or small, the felines surrendered to the substance, rubbing their heads and bodies in the patches for an average of 10 minutes (see video, above). In contrast, dogs and mice that were tested showed no interest in the compound.\n\nNext, the researchers measured beta-endorphins\u2014one of the hormones that naturally relieves pain and induces pleasure by activating the body\u2019s opioid system\u2014in the bloodstreams of five cats 5 minutes before and after exposure. The researchers found that levels of this \u201chappiness hormone\u201d became significantly elevated after exposure to nepetalactol compared with controls. Five cats that had their opioid systems blocked did not rub on the nepetalactol-infused pouches.\n\nBut the researchers wanted to know whether there was a reason for the cats to go wild, beyond pure pleasure. That is when one of the scientists heard about the insect-repelling properties of nepetalactone, which about 2 decades ago was shown to be as good as the famed mosquito-stopper DEET. The researchers hypothesized that when felines in the wild rub on catnip or silver vine, they\u2019re essentially applying an insect repellant.\n\nThey first showed cats can transfer the chemical to their skin, and then conducted a live mosquito challenge\u2014similar to when people\u2019s arms are used to evaluate insect repellants. They put the nepetalactol-treated heads of sedated cats into chambers full of mosquitoes and counted how many landed on them\u2014it was about half the number that landed on feline heads treated with a neutral substance, they report today in Science Advances .\n\nMost scientists and pet owners assumed the only reason that cats roll around in catnip was for the euphoric experience, Miyazaki says. \u201cOur findings suggest instead that rolling is rather a functional behavior.\u201d\n\nThe researchers speculate that cat ancestors might have rubbed their bodies against the plants by chance, enjoyed the feeling, and kept doing it. It is not clear, though, whether it was the euphoric response\u2014or the insect-repelling properties of the plant\u2014that kept them rolling. \u201cAnyone who has ever sat in the field to observe animals ambushing prey knows just how difficult it is for them to keep still when there are many biting mosquitoes around,\u201d Miyazaki says. \u201cIt does not seem unreasonable, therefore, to argue that there is a strong selection pressure\u201d to keep away annoying bugs.\n\nThe team, which has already patented an insect repellent based on nepetalactol, plans next to identify the cat genes involved in the catnip response and examine the substance\u2019s action against other insect pests. De Roode, who is impressed by how thorough the experiments were, says the work provides a \u201creally interesting\u201d example of how insects can shape animal behavior. \u201cIt is amazing how much we can learn from animals.\u201d",
            "published_at": "2021-01-20T14:00:00-05:00"
        },
        {
            "authors": [
                "Josh Taylor"
            ],
            "title": "Google threatens to shut down search in Australia if digital news code goes ahead",
            "contents": "Google has threatened to remove its search engine from Australia and Facebook has threatened to remove news from its feed for all Australian users if a code forcing the companies to negotiate payments to news media companies goes ahead.\n\nThe move would mean the 19 million Australians who use Google every month would no longer be able to search, and 17 million Australians who log into Facebook every month would not be able to see or post any news articles.\n\nThe two companies are fighting against legislation currently before the parliament which would force the digital platforms to enter into negotiations with news media companies for payment for content, with an arbiter to ultimately decide the payment amount if no agreement can be reached.\n\nOn Friday, Google delivered an ultimatum to the government, saying it would not be viable to continue offering search in Australia if the code goes ahead.\n\nThe company\u2019s Australian managing director, Mel Silva, told a Senate committee the proposed news code was untenable and would set a \u201cdangerous precedent\u201d for paying for links.\n\n\u201cThe principle of unrestricted linking between websites is fundamental to search and coupled with the unmanageable financial and operational risk is this version of the code were to become law, it would give us no real choice but to stop making Google Search available in Australia,\u201d she said.\n\n\u201cWithdrawing our services from Australia is the last thing that Google want to have happen, especially when there is another way forward.\u201d\n\nSilva said the company wanted to make changes to the code to make it \u201cworkable\u201d, and the company was keen to enter into agreements with media companies to pay for content, pointing out around 450 deals have been made with media companies around the world.\n\nPrime minister Scott Morrison said at a press conference in Brisbane the government would not respond to threats.\n\n\u201cLet me be clear. Australia makes our rules for things you can do in Australia. That\u2019s done in our parliament. It\u2019s done by our government. And that\u2019s how things work here in Australia and people who want to work with that, in Australia, you\u2019re very welcome.\n\n\u201cBut we don\u2019t respond to threats.\u201d\n\nRepresentatives from Facebook repeated the company\u2019s previous threat to pull news content from user feeds. Josh Machin, Facebook\u2019s head of public policy in Australia, said if the code goes ahead, Facebook would potentially prevent not just news companies from posting links to news articles on Facebook, but all users based in Australia.\n\nMachin said news articles make up under 5% of what the average user sees in their feed, and Facebook did not get much commercial benefit from news articles posted on Facebook.\n\nWhen asked whether Facebook profited from fake news posted on the platform, Machin said no commercial benefit was gained by Facebook on its users posting fake news.\n\nGuardian Australia managing director Dan Stinton told the committee the claims were misleading because news content keeps users engaged on Facebook.\n\n\u201cThe way you advertise on Facebook is within the Facebook newsfeed and news content does provide at least some of the engagement that the Facebook newsfeed delivers,\u201d he said.\n\n\u201cSo I\u2019m surprised that they say, they see no value in journalism within the Facebook newsfeed.\u201d\n\nSenators repeatedly questioned Silva about what they said was a threat, asking whether it was simply about avoiding the precedent it would create worldwide for paying for news in search results.\n\nSilva denied it was a threat, just the \u201cworst case scenario\u201d if the code went ahead.\n\nIndependent senator Rex Patrick compared Google\u2019s threat with China threatening Australia\u2019s trade in response to the inquiry into Covid-19. He said Google\u2019s response was not about \u201cbreaking\u201d search, but Google protecting its revenue.\n\n\u201cIt\u2019s about breaking your bank account, that\u2019s what this is about,\u201d he said. \u201cIt does not touch the the internet and the way in which it works.\u201d he said.\n\nSilva told the hearing that last year Google paid $59m in tax on profits before tax of $134m, and $4.8bn in revenue in Australia.\n\nGoogle paying Twitter to advertise a link to an article of ours on someone against the code that would require them to pay for links to articles. pic.twitter.com/Q8ncqfPKjw \u2014 Josh Taylor (@joshgnosis) January 21, 2021\n\nFacebook has called the code unworkable in its current form, and has asked for digital platforms to be given six-months\u2019 grace to negotiate deals with news companies directly before being hit with the \u201cbig stick\u201d of the mandatory code.\n\nStinton told the committee that the code is a \u201cpragmatic way\u201d to facilitate negotiations between news media and the digital platforms, and argued the claim of opponents that the legislation would break an open internet ignored the internet had changed.\n\n\u201cOpponents of the code are defending an open internet that ceased to exist years ago, and instead has become dominated by a small number of very, very large US tech companies,\u201d he said.\n\n\u201cIn fact, Google and Facebook are the internet for most Australians, or at least the key gateway to it. Google has a monthly audience of 19 million, and Facebook of 17 million.\n\n\u201cWhere people go online is largely determined by these two companies\u2019 algorithms.\u201d\n\nNine has argued the digital platforms need to be regulated because they are monopolies within Australia, and should pay for news content because \u201caccess to news content drives credibility for and value of the platforms\u201d.\n\nNewswire service AAP has said it supports the code, but notes that as a wholesale news provider it would not directly benefit from the code so would need other government support.\n\nA poll released on Friday morning found three in five Australians believe social media companies should prioritise news websites in news feeds.\n\nDynata polled 1,003 people on 14 and 15 January on behalf of the Australia Institute and found 62% of people agreed social media companies should prioritise journalism in user feeds, and 84% agreed they should be transparent about how their algorithms influence user feeds, and should take steps to stop the spread of misinformation.\n\nThe poll found 75% believed anonymous accounts should be banned, but people aged over 60 were much more likely to call for a ban on anonymous accounts (88%) compared with those 18 to 29 (59%).",
            "published_at": "2021-01-22T00:00:00"
        },
        {
            "authors": [],
            "title": "francisrstokes/super-expressive: \ud83e\udd9c Super Expressive is a zero-dependency JavaScript library for building regular expressions in (almost) natural language",
            "contents": "Super Expressive\n\nSuper Expressive is a JavaScript library that allows you to build regular expressions in almost natural language - with no extra dependencies, and a lightweight code footprint (less than 4kb with minification + gzip!).\n\nRegex is a very powerful tool, but its terse and cryptic vocabulary can make constructing and communicating them with others a challenge. Even developers who understand them well can have trouble reading their own back just a few months later! In addition, they can't be easily created and manipulated in a programmatic way - closing off an entire avenue of dynamic text processing.\n\nThat's where Super Expressive comes in. It provides a programmatic and human readable way to create regular expressions. It's API uses the fluent builder pattern, and is completely immutable. It's built to be discoverable and predictable:\n\nproperties and methods describe what they do in plain English\n\norder matters! quantifiers are specified before the thing they change, just like in English (e.g. SuperExpressive().exactly(5).digit )\n\n) if you make a mistake, you'll know how to fix it. SuperExpressive will guide you towards a fix if your expression is invalid\n\nsubexpressions can be used to create meaningful, reusable components\n\nincludes an index.d.ts file for full TypeScript support\n\nSuperExpressive turns those complex and unwieldy regexes that appear in code reviews into something that can be read, understood, and properly reviewed by your peers - and maintained by anyone!\n\nInstallation and Usage\n\nnpm i super-expressive\n\nconst SuperExpressive = require ( 'super-expressive' ) ; // Or as an ES6 module import SuperExpressive from 'super-expressive' ;\n\nExample\n\nThe following example recognises and captures the value of a 16-bit hexadecimal number like 0xC0D3 .\n\nconst SuperExpressive = require ( 'super-expressive' ) ; const myRegex = SuperExpressive ( ) . startOfInput . optional . string ( '0x' ) . capture . exactly ( 4 ) . anyOf . range ( 'A' , 'F' ) . range ( 'a' , 'f' ) . range ( '0' , '9' ) . end ( ) . end ( ) . endOfInput . toRegex ( ) ; // Produces the following regular expression: / ^ (?: 0x ) ? ( [ A-Fa-f0-9 ] { 4 } ) $ /\n\nPlayground\n\nYou can experiment with SuperExpressive in the Super Expressive Playground by @nartc. This is a great way to build a regex description, and test it against various inputs.\n\nPorts\n\nSuper Expressive has been ported to the following langauges:\n\nPHP\n\nhttps://github.com/bassim/super-expressive-php by @bassim\n\nRuby\n\nhttps://github.com/hiy/super-expressive-ruby by @hiy\n\nAPI\n\nSuperExpressive()\n\nCreates an instance of SuperExpressive .\n\nUses the g flag on the regular expression, which indicates that it should match multiple values when run on a string.\n\nExample\n\nSuperExpressive ( ) . allowMultipleMatches . string ( 'hello' ) . toRegex ( ) ; // -> /hello/g\n\n.lineByLine\n\nUses the m flag on the regular expression, which indicates that it should treat the .startOfInput and .endOfInput markers as the start and end of lines.\n\nExample\n\nSuperExpressive ( ) . lineByLine . string ( '^hello$' ) . toRegex ( ) ; // -> / \\^ hello \\$ /m\n\nUses the i flag on the regular expression, which indicates that it should treat ignore the uppercase/lowercase distinction when matching.\n\nExample\n\nSuperExpressive ( ) . caseInsensitive . string ( 'HELLO' ) . toRegex ( ) ; // -> /HELLO/i\n\nUses the y flag on the regular expression, which indicates that it should create a stateful regular expression that can be resumed from the last match.\n\nExample\n\nSuperExpressive ( ) . sticky . string ( 'hello' ) . toRegex ( ) ; // -> /hello/y\n\nUses the u flag on the regular expression, which indicates that it should use full unicode matching.\n\nExample\n\nSuperExpressive ( ) . unicode . string ( 'h\u00e9llo' ) . toRegex ( ) ; // -> /h\u00e9llo/u\n\nUses the s flag on the regular expression, which indicates that the input should be treated as a single line, where the .startOfInput and .endOfInput markers explicitly mark the start and end of input, and .anyChar also matches newlines.\n\nExample\n\nSuperExpressive ( ) . singleLine . string ( 'hello' ) . anyChar . string ( 'world' ) . toRegex ( ) ; // -> /hello.world/s\n\nMatches any single character. When combined with .singleLine, it also matches newlines.\n\nExample\n\nSuperExpressive ( ) . anyChar . toRegex ( ) ; // -> /./\n\nMatches any whitespace character, including the special whitespace characters: \\r\n\n\\t\\f\\v .\n\nExample\n\nSuperExpressive ( ) . whitespaceChar . toRegex ( ) ; // -> / \\s /\n\nMatches any non-whitespace character, excluding also the special whitespace characters: \\r\n\n\\t\\f\\v .\n\nExample\n\nSuperExpressive ( ) . nonWhitespaceChar . toRegex ( ) ; // -> / \\S /\n\nMatches any digit from 0-9 .\n\nExample\n\nSuperExpressive ( ) . digit . toRegex ( ) ; // -> / \\d /\n\nMatches any non-digit.\n\nExample\n\nSuperExpressive ( ) . nonDigit . toRegex ( ) ; // -> / \\D /\n\nMatches any alpha-numeric ( a-z, A-Z, 0-9 ) characters, as well as _ .\n\nExample\n\nSuperExpressive ( ) . word . toRegex ( ) ; // -> / \\w /\n\nMatches any non alpha-numeric ( a-z, A-Z, 0-9 ) characters, excluding _ as well.\n\nExample\n\nSuperExpressive ( ) . nonWord . toRegex ( ) ; // -> / \\W /\n\nMatches (without consuming any characters) immediately between a character matched by .word and a character not matched by .word (in either order).\n\nExample\n\nSuperExpressive ( ) . digit . wordBoundary . toRegex ( ) ; // -> / \\d \\b /\n\nMatches (without consuming any characters) at the position between two characters matched by .word.\n\nExample\n\nSuperExpressive ( ) . digit . nonWordBoundary . toRegex ( ) ; // -> / \\d \\B /\n\nMatches a\n\ncharacter.\n\nExample\n\nSuperExpressive ( ) . newline . toRegex ( ) ; // -> /\n\n/\n\nMatches a \\r character.\n\nExample\n\nSuperExpressive ( ) . carriageReturn . toRegex ( ) ; // -> / \\r /\n\nMatches a \\t character.\n\nExample\n\nSuperExpressive ( ) . tab . toRegex ( ) ; // -> / \\t /\n\nMatches a \\u0000 character (ASCII 0 ).\n\nExample\n\nSuperExpressive ( ) . nullByte . toRegex ( ) ; // -> / \\0 /\n\nMatches a choice between specified elements. Needs to be finalised with .end() .\n\nExample\n\nSuperExpressive ( ) . anyOf . range ( 'a' , 'f' ) . range ( '0' , '9' ) . string ( 'XXX' ) . end ( ) . toRegex ( ) ; // -> / (?: XXX | [ a-f0-9 ] ) /\n\nCreates a capture group for the proceeding elements. Needs to be finalised with .end() . Can be later referenced with backreference(index).\n\nExample\n\nSuperExpressive ( ) . capture . range ( 'a' , 'f' ) . range ( '0' , '9' ) . string ( 'XXX' ) . end ( ) . toRegex ( ) ; // -> / ( [ a-f ] [ 0-9 ] XXX ) /\n\nCreates a named capture group for the proceeding elements. Needs to be finalised with .end() . Can be later referenced with namedBackreference(name) or backreference(index).\n\nExample\n\nSuperExpressive ( ) . namedCapture ( 'interestingStuff' ) . range ( 'a' , 'f' ) . range ( '0' , '9' ) . string ( 'XXX' ) . end ( ) . toRegex ( ) ; // -> / (?< interestingStuff > [ a-f ] [ 0-9 ] XXX ) /\n\nMatches exactly what was previously matched by a namedCapture.\n\nExample\n\nSuperExpressive ( ) . namedCapture ( 'interestingStuff' ) . range ( 'a' , 'f' ) . range ( '0' , '9' ) . string ( 'XXX' ) . end ( ) . string ( 'something else' ) . namedBackreference ( 'interestingStuff' ) . toRegex ( ) ; // -> / (?< interestingStuff > [ a-f ] [ 0-9 ] XXX ) something else\\k <interestingStuff>/\n\nMatches exactly what was previously matched by a capture or namedCapture using a positional index. Note regex indexes start at 1, so the first capture group has index 1.\n\nExample\n\nSuperExpressive ( ) . capture . range ( 'a' , 'f' ) . range ( '0' , '9' ) . string ( 'XXX' ) . end ( ) . string ( 'something else' ) . backreference ( 1 ) . toRegex ( ) ; // -> / ( [ a-f ] [ 0-9 ] XXX ) something else\\1/\n\nCreates a non-capturing group of the proceeding elements. Needs to be finalised with .end() .\n\nExample\n\nSuperExpressive ( ) . optional . group . range ( 'a' , 'f' ) . range ( '0' , '9' ) . string ( 'XXX' ) . end ( ) . toRegex ( ) ; // -> / (?: [ a-f ] [ 0-9 ] XXX ) ?/\n\nSignifies the end of a SuperExpressive grouping, such as .anyOf, .group, or .capture.\n\nExample\n\nSuperExpressive ( ) . capture . anyOf . range ( 'a' , 'f' ) . range ( '0' , '9' ) . string ( 'XXX' ) . end ( ) . end ( ) . toRegex ( ) ; // -> / ( (?: XXX | [ a-f0-9 ] ) ) /\n\nAssert that the proceeding elements are found without consuming them. Needs to be finalised with .end() .\n\nExample\n\nSuperExpressive ( ) . assertAhead . range ( 'a' , 'f' ) . end ( ) . range ( 'a' , 'z' ) . toRegex ( ) ; // -> / (? = [ a-f ] ) [ a-z ] /\n\nAssert that the proceeding elements are not found without consuming them. Needs to be finalised with .end() .\n\nExample\n\nSuperExpressive ( ) . assertNotAhead . range ( 'a' , 'f' ) . end ( ) . range ( 'g' , 'z' ) . toRegex ( ) ; // -> / (? ! [ a-f ] ) [ g-z ] /\n\nAssert that the elements contained within are found immediately before this point in the string. Needs to be finalised with .end() .\n\nExample\n\nSuperExpressive ( ) . assertBehind . string ( 'hello ' ) . end ( ) . string ( 'world' ) . toRegex ( ) ; // -> / (?< = hello ) world/\n\nAssert that the elements contained within are not found immediately before this point in the string. Needs to be finalised with .end() .\n\nExample\n\nSuperExpressive ( ) . assertNotBehind . string ( 'hello ' ) . end ( ) . string ( 'world' ) . toRegex ( ) ; // -> / (?< ! hello ) world/\n\nAssert that the proceeding element may or may not be matched.\n\nExample\n\nSuperExpressive ( ) . optional . digit . toRegex ( ) ; // -> / \\d ?/\n\nAssert that the proceeding element may not be matched, or may be matched multiple times.\n\nExample\n\nSuperExpressive ( ) . zeroOrMore . digit . toRegex ( ) ; // -> / \\d * /\n\nAssert that the proceeding element may not be matched, or may be matched multiple times, but as few times as possible.\n\nExample\n\nSuperExpressive ( ) . zeroOrMoreLazy . digit . toRegex ( ) ; // -> / \\d * ?/\n\nAssert that the proceeding element may be matched once, or may be matched multiple times.\n\nExample\n\nSuperExpressive ( ) . oneOrMore . digit . toRegex ( ) ; // -> / \\d + /\n\nAssert that the proceeding element may be matched once, or may be matched multiple times, but as few times as possible.\n\nExample\n\nSuperExpressive ( ) . oneOrMoreLazy . digit . toRegex ( ) ; // -> / \\d + ?/\n\nAssert that the proceeding element will be matched exactly n times.\n\nExample\n\nSuperExpressive ( ) . exactly ( 5 ) . digit . toRegex ( ) ; // -> / \\d { 5 } /\n\nAssert that the proceeding element will be matched at least n times.\n\nExample\n\nSuperExpressive ( ) . atLeast ( 5 ) . digit . toRegex ( ) ; // -> / \\d { 5, } /\n\nAssert that the proceeding element will be matched somewhere between x and y times.\n\nExample\n\nSuperExpressive ( ) . between ( 3 , 5 ) . digit . toRegex ( ) ; // -> / \\d { 3,5 } /\n\nAssert that the proceeding element will be matched somewhere between x and y times, but as few times as possible.\n\nExample\n\nSuperExpressive ( ) . betweenLazy ( 3 , 5 ) . digit . toRegex ( ) ; // -> / \\d { 3,5 } ?/\n\nAssert the start of input, or the start of a line when .lineByLine is used.\n\nExample\n\nSuperExpressive ( ) . startOfInput . string ( 'hello' ) . toRegex ( ) ; // -> / ^ hello/\n\nAssert the end of input, or the end of a line when .lineByLine is used.\n\nExample\n\nSuperExpressive ( ) . string ( 'hello' ) . endOfInput . toRegex ( ) ; // -> /hello $ /\n\nMatches any of the characters in the provided string chars .\n\nExample\n\nSuperExpressive ( ) . anyOfChars ( 'aeiou' ) . toRegex ( ) ; // -> / [ aeiou ] /\n\nMatches any character, except any of those in the provided string chars .\n\nExample\n\nSuperExpressive ( ) . anythingButChars ( 'aeiou' ) . toRegex ( ) ; // -> / [ ^aeiou ] /\n\nMatches any string the same length as str , except the characters sequentially defined in str .\n\nExample\n\nSuperExpressive ( ) . anythingButString ( 'aeiou' ) . toRegex ( ) ; // -> / (?: [ ^a ] [ ^e ] [ ^i ] [ ^o ] [ ^u ] ) /\n\nMatches any character, except those that would be captured by the .range specified by a and b .\n\nExample\n\nSuperExpressive ( ) . anythingButRange ( 0 , 9 ) . toRegex ( ) ; // -> / [ ^0-9 ] /\n\nMatches the exact string s .\n\nExample\n\nSuperExpressive ( ) . string ( 'hello' ) . toRegex ( ) ; // -> /hello/\n\nMatches the exact character c .\n\nExample\n\nSuperExpressive ( ) . char ( 'x' ) . toRegex ( ) ; // -> /x/\n\nMatches any character that falls between a and b . Ordering is defined by a characters ASCII or unicode value.\n\nExample\n\nSuperExpressive ( ) . range ( 'a' , 'z' ) . toRegex ( ) ; // -> / [ a-z ] /\n\nopts.namespace: A string namespace to use on all named capture groups in the subexpression, to avoid naming collisions with your own named groups (default = '' )\n\nnamespace to use on all named capture groups in the subexpression, to avoid naming collisions with your own named groups (default = ) opts.ignoreFlags: If set to true, any flags this subexpression specifies should be disregarded (default = true )\n\n) opts.ignoreStartAndEnd: If set to true, any startOfInput/endOfInput asserted in this subexpression specifies should be disregarded (default = true )\n\nMatches another SuperExpressive instance inline. Can be used to create libraries, or to modularise you code. By default, flags and start/end of input markers are ignored, but can be explcitly turned on in the options object.\n\nExample\n\n// A reusable SuperExpressive... const fiveDigits = SuperExpressive ( ) . exactly ( 5 ) . digit ; SuperExpressive ( ) . oneOrMore . range ( 'a' , 'z' ) . atLeast ( 3 ) . anyChar . subexpression ( fiveDigits ) . toRegex ( ) ; // -> / [ a-z ] + . { 3, } \\d { 5 } /\n\nOutputs a string representation of the regular expression that this SuperExpression models.\n\nExample\n\nSuperExpressive ( ) . allowMultipleMatches . lineByLine . startOfInput . optional . string ( '0x' ) . capture . exactly ( 4 ) . anyOf . range ( 'A' , 'F' ) . range ( 'a' , 'f' ) . range ( '0' , '9' ) . end ( ) . end ( ) . endOfInput . toRegexString ( ) ; // -> \"/^(?:0x)?([A-Fa-f0-9]{4})$/gm\"\n\nOutputs the regular expression that this SuperExpression models.\n\nExample"
        },
        {
            "authors": [],
            "title": "IPFS Support in Brave",
            "contents": "Over the past several months, the Brave team has been working with Protocol Labs on adding InterPlanetary File System (IPFS) support in Brave. This is the first deep integration of its kind and we\u2019re very proud to outline how it works in this post.\n\nIPFS is an exciting technology that can help content creators distribute content without high bandwidth costs, while taking advantage of data deduplication and data replication. There are performance advantages for loading content over IPFS by leveraging its geographically distributed swarm network. IPFS is important for blockchain and for self described data integrity. Previously viewed content can even be accessed offline with IPFS! The IPFS network gives access to content even if it has been censored by corporations and nation-states, such as for example, parts of Wikipedia.\n\nIPFS support allows Brave desktop users to download content by using a content hash, known as the Content identifier (CID). Unlike HTTP(S), there is no specified location for the content.\n\nEach node in the IPFS network is a potential host for the content being requested, and if a node doesn\u2019t have the content being requested, the node can retrieve the content from the swarm of peers. The retrieved content is verified locally, removing the need to trust a third party\u2019s integrity.\n\nHTTP(S) uses Uniform Resource Locators (URLs) to specify the location of content. This system can be easily censored since the content is hosted in specific locations on behalf of a single entity and it is susceptible to Denial of Service Attacks (DDoS). IPFS identifies its content by content paths and/or CIDs inside of Uniform Resource Identifier (URIs) but not URLs.",
            "published_at": "2021-01-19T14:52:43+00:00"
        },
        {
            "authors": [],
            "title": "Covert Shores",
            "contents": "US Navy Submarine Aircraft Carrier design\n\n\n\nOriginal Artwork, CLICK for HIGH RESOLUTION:\n\n\n\nIn the late 1950s the US Navy experimented with the concept of aircraft carrying submarines armed with jet fighters. Several submarines capable of carrying aircraft-sized cruise missiles had been built as a stop-gap and contingency to ballistic missile submarines: converted fleet boats USS Tunny and Barbero were fitted with external hangars on the aft deck for trials, each carrying two Regulus-I missiles. These were followed by two diesel-electric cruise missile submarines of the Grayback Class were put into service, each with four missiles, and one submarine of the Halibut Class with five missiles. If these boats could carry such large missiles, then the US Navy supposed that they could alternatively carry manned aircraft.\n\n\n\nRegulus-I missiles displayed aboard USS Halibut, and the larger Regulus-II tested aboard USS Grayback.\n\n\n\nInitially the idea was to built aircraft which could be launched from the existing boats, but soon the larger purpose-designed AN-1 concept was born. Based on the Halibut Class, the AN-1 would carry eight fighters. The aircraft would be a tail-sitting Vertical Take-Off and Landing (VTOL) design with the same overall size footprint as the Regulus-II missile. The aircraft would be carried in two internal hangers similar to those on USS Halibut, and rolled out of the hanger to vertical launch positions while the boat is surfaced. The launch boosters would be carried in separate smaller hangers and mated with the aircraft on the launch rail. Four aircraft could be launched in under 6 minutes, or all eight in under 8 minutes.\n\n\n\n\n\n\n\nGet The essential guide to World Submarines\n\nThis Covert Shores Recognition Guide Covers over 80 classes of submarines including all types currently in service with World Navies.\n\nCheck it out on Amazon\n\n\n\n\n\n\n\nAN-1 Specifications\n\nDisplacemet: 9,260 tons surfaced, 14,700 tons submerged\n\nLength: 498.5 ft\n\nBeam: 44.25 ft\n\nOperating Depth: tbc\n\nSpeed: 16 kt\n\nEndurance: unlimited\n\nPropulsion: 15,000 shp Pressurized Water Reactor\n\nCrew: 149 + 12 pilots and 2 fight crew\n\nAircraft: 8 x Boeing Mach-3 VTOL fighters\n\nArmament: 6 x bow 21\" (533mm) heavyweight torpedo tubes plus 8 reloads, and two stern short 21\" (533mm) torpedo tues with six reloads\n\n\n\nThe AN-1 was much larger than halibut, with a four decks enclosed in a fully double-hull. The forward hangar also incorporated the bow torpedo tubes.\n\n\n\nCLICK for HIGH RESOLUTION:\n\n\n\n\n\n\n\nEarlier aircraft carrying submarines\n\nThe concept of aircraft carrying submarines was not new although examples are rare. The first aircraft launched from a submarine was a Parnall Peto floatplane from the Royal Navy\u2019s HMS M-2 which was fitted with an experimental hangar in 1925. This was followed by the French Navy\u2019s Surcouf cruiser submarine in 1934 which had a hangar for a Besson MB.411 floatplane for reconnaissance. The Japanese also equipped some cruiser submarines with float planes and the most impressive design was the I-400 which carried three Aichi M6A1 Seiran floatplanes. These were for strike missions as well as reconnaissance.\n\n\n\nHMS M-2 launching a Parnall Peto floatplane, French submarine Surcouf with a Besson MB.411 floatplane on the aft casing (note also the large gun turret forward), and the Japanese I-400 Class floatplane carrying submarine.\n\n\n\n\n\n\n\nThe ultimate book of Special Forces subs Covert Shores 2nd Edition is the ONLY world history of naval Special Forces, their missions and their specialist vehicles. SEALs, SBS, COMSUBIN, Sh-13, Spetsnaz, Kampfschwimmers, Commando Hubert, 4RR and many more.\n\nCheck it out on Amazon\n\n\n\nFlying Carpet\n\nInitially relatively conventional fighter aircraft were envisioned, including modified Grumman F11F Tiger fighters and the unbuilt Douglas Model 640. But with the supersonic age rapidly changing the face of aviation, Boeing designed a new Mach-3 fighter for carriage by the AN-1. The fighters would be launched vertically on a stack of three 23,000 lb Wright SE-105 turbojets using regular aircraft-carrier grade JP-5 jet fuel. Two of the three engines would be attached as a \u2018flying carpet\u2019 detachable booster phase which would fall away once the aircraft was airborne and could be recovered for reuse.\n\n\n\nGrumman F-11F Tiger, F-11F with Flying Carpet, and Boeing Mach 3 VTOL fighter concept.\n\n\n\nOriginal Artwork, CLICK for HIGH RESOLUTION:\n\n\n\n\n\nBoeing Mach-3 VTOL fighter Specifications\n\nLength: 70 ft (unpacked)\n\nWingspan: 21.1 ft overall (unpacked)\n\nHeight: 19.5 ft (unpacked)\n\nSpeed: Mach 3\n\nWeight: 15,380 lb empty, 32,630 lb max\n\nPropulsion: Wright SE-105 with JP-5 fuel, 23,000 lb thrust\n\nCrew: 1\n\nArmament: tbc\n\n\n\nFate\n\nThe US Navy soon lost interest in aircraft carrying submarines and the AN-1 and other designs never made it off the drawing board.\n\n"
        },
        {
            "authors": [
                "Costas Paris"
            ],
            "title": "Maersk Ship Loses 750 Containers Overboard in Pacific Ocean",
            "contents": "A cargo ship operated by A.P. Moller-Maersk A/S lost several hundred containers in the Pacific Ocean while sailing through heavy seas from China to Los Angeles, the latest in a spate of incidents in which boxes carrying millions of dollars\u2019 worth of goods have gone overboard.\n\nThe company said the Maersk Essen, which has capacity for more than 13,000 containers, lost an estimated 750 of them on January 16 about halfway through its trans-Pacific sailing from China\u2019s Port of Xiamen.\n\n\u201cAll crew members are safe and a detailed cargo assessment is ongoing while the vessel continues on her journey,\u201d Maersk said in a statement on Thursday. \u201cThe U.S. Coast Guard, flag state and relevant authorities have been notified. We view this as a very serious situation which will be investigated promptly and thoroughly.\u201d A.P. Moller-Maersk is based in Copenhagen and the ship carries a Danish flag.\n\nNewsletter Sign-up The Logistics Report Top news and in-depth analysis on the world of logistics, from supply chain to transport and technology. PREVIEW\n\nSeveral container ships have lost large numbers of boxes overboard in recent months in a spurt of accidents that maritime industry officials say had been declining.\n\nThe One Apus container vessel, operated by Singapore-based Ocean Network Express, lost around 2,000 boxes in November when it hit a storm off Hawaii on its way to Long Beach, Calif., from Yantian, China. The ship eventually sailed to Kobe, Japan, with hundreds of tipped-over containers sitting precariously onboard and remains there for repairs and an investigation into the cause of the incident.\n\nPeople involved in the investigations said insurance claims from the One Apus could reach more than $220 million.\n\nDislodged containers on the One Apus, berthed at Kobe after losing some 2,000 containers in November. Photo: Buddhika Weerasinghe/Bloomberg News\n\nLosing boxes in harsh weather is relatively rare, but incidents this winter have been on the rise, especially in the Pacific.\n\nEarlier this month, 76 containers fell off a vessel operated by Israel\u2019s ZIM Integrated Shipping Services Ltd. en route from South Korea to North America. On Dec. 31, a boxship managed by Taiwan\u2019s Evergreen Marine Corp. Ltd. lost around 40 containers off the coast of Japan while heading across the Pacific.\n\nEngineers involved in the probes say they are looking into typical causes like failures in lashing systems that hold containers together. But as ships become bigger and containers are stacked high as multistory buildings, a vessel\u2019s stability may come under greater pressure from pitching and rolling.\n\n\u201cIt\u2019s called parametric rolling and can happen when waves don\u2019t hit the bow head-on, but at an angle. The ship goes into a rolling motion in sync with the waves which, combined with the ship\u2019s normal pitching as it steams ahead, can displace cargo,\u201d said Fotis Pagoulatos, an Athens-based naval architect.\n\nMaritime officials say ship operators are looking at installing sensors that could issue warnings on sea conditions to avoid parametric rolling.\n\n\u201cThe higher you stack the boxes on deck, the larger the forces they are subjected to when the vessel moves in waves, and this could be a contributing factor, especially as the recent demand boom has meant filling all ships to the brim,\u201d said Lars Jensen, chief executive of Denmark-based SeaIntelligence Consulting.\n\nYiannis Sgouras, a veteran Greek captain, said the threat can come without warning, even when waves aren\u2019t very high. \u201cIf you don\u2019t catch it early on and change course, the ship can roll from side to side as it steams forward and things fall over,\u201d he said.\n\nMaritime insurance executives said roughly 3,000 containers have been lost at sea over the past two months.\n\nThe World Shipping Council, a Washington-based trade body representing liner companies, said in a report last July that between 2008 and 2019 on average 1,382 containers were lost at sea each year.\n\nWrite to Costas Paris at costas.paris@wsj.com"
        },
        {
            "authors": [
                "Kenny Kerr"
            ],
            "title": "microsoft/windows-rs: Rust for Windows",
            "contents": "Rust for Windows\n\nThe windows crate lets you call any Windows API past, present, and future using code generated on the fly directly from the metadata describing the API and right into your Rust package where you can call them as if they were just another Rust module.\n\nThe Rust language projection follows in the tradition established by C++/WinRT of building language projections for Windows using standard languages and compilers, providing a natural and idiomatic way for Rust developers to call Windows APIs.\n\nGetting started\n\nStart by adding the following to your Cargo.toml file:\n\n[ dependencies ] windows = \" 0.2.1 \" [ build-dependencies ] windows = \" 0.2.1 \"\n\nThis will allow Cargo to download, build, and cache Windows support as a package. Next, specify which types you need inside of a build.rs build script and the windows crate will generate the necessary bindings:\n\nfn main () { windows :: build! ( windows :: data :: xml :: dom :: * windows :: win32 :: system_services :: {CreateEventW, SetEvent, WaitForSingleObject} windows :: win32 :: windows_programming :: CloseHandle ); }\n\nFinally, make use of any Windows APIs as needed.\n\nmod bindings { :: windows :: include_bindings! (); } use bindings :: { windows :: data :: xml :: dom :: * , windows :: win32 :: system_services :: {CreateEventW, SetEvent, WaitForSingleObject}, windows :: win32 :: windows_programming :: CloseHandle, }; fn main () -> windows:: Result <()> { let doc = XmlDocument :: new ()?; doc. load_xml ( \"<html>hello world</html>\" )?; let root = doc. document_element ()?; assert! (root. node_name ()? == \"html\" ); assert! (root. inner_text ()? == \"hello world\" ); unsafe { let event = CreateEventW ( std :: ptr :: null_mut (), true . into (), false . into (), std :: ptr :: null (), ); SetEvent (event). ok ()?; WaitForSingleObject (event, 0 ); CloseHandle (event). ok ()?; } Ok (()) }\n\nTo reduce build time, use a bindings crate rather simply a module. This will allow Cargo to cache the results and build your project far more quickly.\n\nThere is an experimental documentation generator for the Windows API. The documentation is published here. This can be useful to figure out how the various Windows APIs map to Rust modules and which use paths you need to use from within the build macro.\n\nFor a more complete example, take a look at Robert Mikhayelyan's Minesweeper. More simple examples can be found here."
        },
        {
            "authors": [],
            "title": "Intel outsources Core i3 to TSMC's 5nm process",
            "contents": "Market analyst Trendforce reports that foundry TSMC is to start making Intel's Core i3 process later in the year on a 5nm process. This follows Intel's well documented problems with its leading edge process technology at 10nm and 7nm.\n\nThe Core i3 move to a 5nm process is set to be followed by mid-range and high-end CPUs being produced for Intel by TSMC on a 3nm process in 2H22. TrendForce did not give a source for the information, simply referencing \"investigations.\"\n\nIntel has long outsourced production significant amounts of its non-CPU chips to TSMC and UMC \u2013 about 15 to 20 percent of its output, according to TrendForce. This is partly because it has often acquired fabless startups that had brought products to market using foundry. It was usually not worthwhile to re-engineer such products to Intel processes. It is also because Intel has wanted to focus on leading-edge specialist processes, although with less success in recent years.\n\nThat 15 to 20 percent outsource was likely worth $10.5 billion to $14 billion in 2020, given Intel's annual revenue of $70bn.\n\nAs a result, the world's leading processors have been produced for Apple, Hi-Silicon and Samsung on foundry manufacturing processes at 7nm and 5nm. It also means that Advanced Micro Devices, Intel's rival in the PC processor market, has been able to gain a superior position due to access to TSMC's 7nm manufacturing process. Intel's in ability to keep pace with leading foundries is also likely to have figured in Apple's decision to develop its own PC processors for the MacBook and Mac Mini, now equipped with M1 processors manufactured for Apple by TSMC.\n\nThe 5nm outsourcing move will allow Intel to compete and also maintain some existence as a major IDM making higher margin chips and maintaining in-house production. It will also",
            "published_at": "2021-01-18T16:03:47+01:00"
        },
        {
            "authors": [],
            "title": "Solar2D",
            "contents": "Cross-platform Develop for mobile, desktop, and connected TV devices with just one code base: iOS, tvOS, Android, Android TV, macOS, Windows, Linux or HTML5.\n\nPlugins for all needs Select from numerous plugins which extend the Solar2D core for features like in-app advertising, analytics, media, and much more. A vast variety of plugins is available via Solar2D free directory or third party stores, like Solar2D Marketplace and Solar2D Plugins.\n\nCall any native library If it\u2019s not already in the core or supported via a plugin, you can call any native (C/C++/Obj-C/Java) library or API using Solar2D Native.\n\nProduction ready Solar2D is official fork of Corona SDK, which has been in active development for over 10 years, and used by hundreds of thousands of apps and developers.\n\nLua-based Lua is an open source scripting language designed to be lightweight, fast, yet also powerful. Lua is currently the leading scripting language in games and has been utilized in Warcraft \u2122, Angry Birds \u2122, Civilization \u2122 and many other popular franchises.\n\nCompletely free No hidden fees, charges, or royalties. No matter if you are an indie developer or a large publisher, you will never pay for using the engine.\n\nPrivacy aware Solar2D would not track your users. No anonymized data gathering, no server calls. Nothing. Games you build only make network requests you asked for.\n\nCommunity Join a vibrant community of thousands developers using Solar2D for live games on Forums or Discord chat."
        },
        {
            "authors": [],
            "title": "Regarding CSW and the Bitcoin Whitepaper",
            "contents": "Yesterday both Bitcoin.org and Bitcoincore.org received allegations of copyright infringement of the Bitcoin whitepaper by lawyers representing Craig Steven Wright. In this letter, they claim Craig owns the copyright to the paper, the Bitcoin name, and ownership of bitcoin.org. They also claim he is Satoshi Nakamoto, the pseudonymous creator of Bitcoin, and the original owner of bitcoin.org. Bitcoin.org and Bitcoincore.org were both asked to take down the whitepaper. We believe these claims are without merit, and refuse to do so.\n\nUnfortunately, without consulting us, Bitcoin Core developers scrambled to remove the Bitcoin whitepaper from bitcoincore.org, in response to these allegations of copyright infringement, lending credence to these false claims. The Bitcoin Core website was modified to remove references to the whitepaper, their local copy of the whitepaper PDF was deleted, and with less than 2 hours of public review, this change was merged. By surrendering in this way, the Bitcoin Core project has lent ammunition to Bitcoin\u2019s enemies, engaged in self-censorship, and compromised its integrity. This surrender will no doubt be weaponized to make new false claims, like that the Bitcoin Core developers \u201cknow\u201d CSW to be Satoshi Nakamoto and this is why they acted in this way.\n\nThe Bitcoin whitepaper was included in the original Bitcoin project files with the project clearly published under the MIT license by Satoshi Nakamoto. We believe there is no doubt we have the legal right to host the Bitcoin whitepaper. Furthermore, Satoshi Nakamoto has a known PGP public key, therefore it is cryptographically possible for someone to verify themselves to be Satoshi Nakamoto. Unfortunately, Craig has been unable to do this.\n\nWe will continue hosting the Bitcoin whitepaper and won\u2019t be silenced or intimidated. Others hosting the whitepaper should follow our lead in resisting these false allegations."
        },
        {
            "authors": [],
            "title": "",
            "contents": "JavaScript is not available.\n\nWe\u2019ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.\n\nHelp Center"
        },
        {
            "authors": [],
            "title": "Sidebar for Twitter",
            "contents": "About The Twemex Philosophy A note from the developer, @geoffreylitt\n\nAs a researcher, I love using Twitter as a memex: a tool for thinking and making connections between related ideas. I've used it to find so many fascinating people, thoughts, and discussions.\n\nBut using Twitter this way has always felt like fighting the natural design of the tool. The Twitter product is trying to get me to refresh the feed, not grow my thoughts. That might make sense for most casual users, but I wanted something more powerful.\n\nSo I started building a browser extension to make Twitter a better place to think. It all started out with a lightweight search UI, so that I could seamlessly search through tweets to reference while I was writing new threads, weaving together old ideas with new ones. Then I added a way to see people's best tweets, so I could get more from coming across an interesting user's profile.\n\nI used this extension myself for about 6 months, and after a while I couldn't live without it. Other people started asking me to share it, so I decided to turn it into a product. My hope is to build Twemex into a high-quality tool that makes Twitter a better memex for everyone."
        },
        {
            "authors": [
                "Brian Dunbar"
            ],
            "title": "NASA Lends Moon Rock for Oval Office Display",
            "contents": ""
        },
        {
            "authors": [
                "Amar Desai"
            ],
            "title": "How to Write, Plan & Get Organized",
            "contents": "This week we will talk about a useful website, a productivity tool, and a physical product.\n\n1. Notion\n\nNotion is the best thing that has happened to me this year. I have been in search of the ultimate note-taking and collaboration tool for years and finally found Notion.\n\nI have used a host of note-taking apps like Evernote, Microsoft OneNote, Google Keep, and many collaboration/project management platforms like Basecamp, Confluence, Trello, etc. However, none of them was a complete package, some offered great note-taking ability but did not have collaboration or project management features and vice versa.\n\nNotion is not just a note-taking app but its a combination of note-taking, project management, collaboration, and much more. People are using it for different purposes, like managing house plants, tracking expenses, keeping track of personal projects, managing daily to-dos, and the list is endless. Even a lot of startups are using Notion to manage projects and as their internal portals.\n\nI personally use Notion as my second brain where I dump all the useful information, ideas, and thoughts which I come across daily. This way I don't forget anything and can easily refer to it. Also, I and my wife have a kanban board where we create tasks and assign them to one another. For example, my wife assigned me a task this morning to look for new car insurance.\n\nIf you want to stay organized, have everything in one place, and want to build a second brain then I highly recommend Notion.\n\n2. Launch House\n\nA couple of weeks back I stumbled upon a website named Launch House where the authors Michael, Brett, and Jacob have curated a list of useful how-to articles & videos. Resources will help you in launching your next online business or passive income stream. For example, they have a curated list of resources on starting a clothing brand, opening an Amazon seller central account, opening an Etsy shop, starting a podcast, starting a newsletter, etc.\n\nIf you are planning to launch something of your own then you should give this website a try. I am sure you will find something useful.\n\n3. Transparent Sticky Notes\n\nWho doesn\u2019t like sticky notes? They are cheap, functional, and easy to use. People at The Worcestershire Stationery Company took the same old sticky notes and made them translucent. It\u2019s a combination of tracing paper and sticky notes with a wide range of applications. So if you want to annotate something on a textbook but don't want to ruin its sell back value, you can use these sticky notes.\n\n4. Musk on the Mars\n\nWhen will Elon Musk be on Mars? Yes, physical Elon Musk on physical Mars. If this question is keeping you awake at night and you want to predict when this will happen, then visit Musk on the Mars website. On the website, you can reserve any day in the future that you think Musk will land on Mars by paying $1 and if your prediction comes true then you win the entire pot.\n\nShare\n\nIf you found this content useful and would like to get notified when I publish new posts please consider subscribing.\n\nI publish posts every week. Topics vary from Tech to Design to Social Media to Pop Culture to Politics to Space to Travel to Productivity to Outdoors to Finance to Books to TV Shows to Youtube Channels to Movies to Self Help. It\u2019s going to be a mixed bag and will help you in your career or personal life one way or another.\n\nThank you for your support! It truly means the world to me! Please let me know if you have any questions or recommendations, by responding in the comments section below. I will do my best to answer all of them.\n\nTell your friends!"
        },
        {
            "authors": [],
            "title": "truthly/pg-emoji: \ud83d\ude0d\ud83d\udc18PostgreSQL emoji encode/decode extension",
            "contents": "1. About\n\nemoji is a pure SQL PostgreSQL extension to encode/decode bytea/text to/from emoji.\n\nA lookup-table is constructed from the first 1024 emojis from [https://unicode.org/Public/emoji/13.1/emoji-test.txt], where each emoji maps to a unique 10 bit sequence.\n\nThe input data is split into 10 bit fragments, mapped to the corresponding emojis.\n\nThe first emoji in the result is a header, where the first bit is 1 if the result was zero padded, and the remaining 9 bits is a checksum based on the input data.\n\nIf the checksum is invalid during decode, NULL is returned.\n\n2. Dependencies\n\nNone.\n\n3. Installation\n\nInstall the emoji extension with:\n\n$ git clone https://github.com/truthly/pg-emoji.git $ cd pg-emoji $ make $ sudo make install $ make installcheck\n\n4. Usage\n\nUse with:\n\n$ psql # CREATE EXTENSION emoji; CREATE EXTENSION;\n\n5. API\n\nSELECT emoji . encode ( ' \\x 0123456789abcdef ' :: bytea ); encode -- -------- \ud83d\udc66\ud83d\ude00\ud83e\udd7a\ud83e\ude80\ud83e\udda0\ud83d\udd96\ud83c\udf0c\ud83e\udd5a ( 1 row)\n\nMaking a subtle change to the input data will not only change the corresponding emoji, but also the first emoji which contains a 9-bit checksum of the data, which means it will change with 99.8% confidence (511/512). Notice in the example below what happens if the last f is changed to 7.\n\nSELECT emoji . encode ( ' \\x 0123458789abcde7 ' :: bytea ); encode -- -------- \ud83d\udc9c\ud83d\ude00\ud83e\udd7a\ud83e\ude80\ud83c\udf7c\ud83d\udd96\ud83c\udf0c\ud83c\udf55 ( 1 row)\n\nSELECT emoji . decode ( ' \ud83d\udc66\ud83d\ude00\ud83e\udd7a\ud83e\ude80\ud83e\udda0\ud83d\udd96\ud83c\udf0c\ud83e\udd5a ' ); decode -- ------------------ \\x0123456789abcdef ( 1 row)\n\nThanks to the first emoji containing a 9-bit checksum of the data, failing to properly copy/paste the entire emoji string will be detected upon decoding with 99.8% confidence and NULL will be returned.\n\nSELECT emoji . decode ( ' \ud83d\udc66\ud83d\ude00\ud83e\udd7a\ud83e\ude80\ud83e\udda0\ud83d\udd96\ud83c\udf0c ' ); decode -- ------ ( 1 row)\n\nSELECT emoji . from_text ( ' Hello \ud83c\udf0e! ' ); from_text -- ---------- \ud83e\uddb3\ud83e\udd7a\ud83d\udc1e\ud83d\udd70\ud83c\udf90\ud83c\udf97\ud83d\udcf7\ud83e\uddc2\ud83c\udf96\ud83e\uded6 ( 1 row)"
        },
        {
            "authors": [
                "Mark Stenberg"
            ],
            "title": "Advertisements have emerged on Substack's 'ad-free' newsletter platform as writers look for more ways to earn. These 3 startups are trying to turn the trend into a business.",
            "contents": "The newsletter platform Substack, whose business model uses subscriptions rather than advertisements, has seen some of its most prominent users incorporate advertising into their newsletters.\n\nAt least three startups \u2014 Swapstack, Upstart.me, and Letterwell \u2014 have begun helping small newsletters find interested advertisers.\n\nVisit Business Insider's homepage for more stories.\n\nIn recent months, Substack, a newsletter platform whose appeal stems in part from its promise of an advertising-free ecosystem, has watched d.i.y. advertisements slowly find their way onto dozens of its most prominent newsletters, from Delia Cai's classifieds section to Packy McCormack's sponsorship deals.\n\nTo capitalize on the trend, a number of startups have emerged that help pair independent newsletters with interested advertisers, a development that presents a potential source of revenue for writers but could force the subscription-based Substack to respond to the phenomenon.\n\nThe founders of Substack have been vocal about their opposition to advertising since they launched the platform. On its website, the company describes how it makes money, saying: \"Substack takes a 10% fee on all paid subscriptions. Substack will never run ads or sell user data.\"\n\nNot only does Substack stake part of its identity in its eschewal of advertising, its business model relies on the decision. Substack subsidizes the cost of providing its free newsletter services by taking a 10% cut of the revenue it generates from its paid newsletters. An outbreak of advertising on its free newsletters could threaten that model if a significant chunk of newsletter writers choose to ignore the subscription component.\n\nIf users can make money without turning on their paywall, then they get free hosting and generate revenue. As an isolated event, advertising on Substack is no big deal. But if it becomes ubiquitous, the funding model of the platform could be at risk, forcing Substack to either clamp down on this cottage industry or embrace it. Substack could incorporate advertising into its product, but doing so would contradict the platform's commitment to remaining ad-free.\n\nIt's free real estate\n\nThe founders of Swapstack, Jake Schonberger and Jake Singer, originally hatched the idea as a form of audience sharing: You promote my newsletter in your newsletter, and I'll promote yours in mine. Hence, swapping Substacks.\n\nSwapstack cofounder Jake Schonberger says that advertising makes better business sense for small newsletters. Jake Schonberger\n\nBut the concept quickly morphed, as the Facebook and Amazon alumni discovered an untapped desire from newsletter writers to find advertisers for their writing.\n\n\"We realized that writers with a smaller subscriber base \u2014 a couple hundred or maybe a thousand \u2014 when they started thinking about monetization, it didn't necessarily make sense to turn on a subscription model,\" Schonberger said. \"Whereas at that same point, if you place an advertisement, you could make $200 a week.\"\n\nCompared to erecting a paywall, which slows newsletter growth and limits readership, some newsletter writers are open to advertising because it allows them to grow quickly and monetize without asking readers for money.\n\nNewsletter authors like Mary Retta, who writes the newsletter \"close but not quite,\" also want their writing to be accessible to all readers, regardless of their ability to pay.\n\n\"A lot of the people who subscribe to my newsletter are younger, students, POC, or low-income, and it didn't feel right to put up barriers that would prevent people who otherwise would like to keep up with my work from reading,\" Retta said.\n\nTo meet this need, Schonberger and Singer set up a landing page, which directs interested parties to a Slack workspace where members chat and connect with potential sponsors in the #advertisers channel. There, Schonberger announces every new advertiser the duo lands, listing the company name and a brief bio, as well as its minimum number of subscribers (typically 1,000), budget, and preferred type of newsletter subject matter.\n\nRead more: 1 woman's experiment with newsletter platforms revealed 4x traffic growth thanks to one strategy: leaving Substack\n\nThe advertisers, many of which are startups themselves, get an affordable vehicle for reaching a targeted, engaged group of readers. Because newsletters subscribers have opted in to reading an email, their attention can be valuable and cost effective, especially for startups whose marketing budgets are tight.\n\n\"People spend minutes with the content versus seconds,\" Schonberger said. \"If you can get in front of people that actually care about the type of stuff that your brand is building, that's valuable.\"\n\nAuthors of newsletters that meet the brand's requirements can fill out a brief form describing their product; if the brand is interested, the two are connected and can begin hashing out the details of their agreement. When they have reached an agreement, the advertiser pays the newsletter writer \u2014 through Venmo or CashApp, at the moment \u2014 then sends over the necessary materials and the deal goes live.\n\nSchonberger and Singer plan to make money by taking a percentage of this deal, less than 10% they say, though they haven't taken any cuts yet. The two are working to launch a standalone platform for their business, which will help them more easily take a percentage of the deals and streamline the process. So far, Swapstack has facilitated 38 deals and generated roughly $5,700 in total, according to documents reviewed by Insider.\n\nBelow the Fold creator Anum Hussain says that the key value in services like Swapstack is that they take care of business outreach, which lets writers focus on editorial. Anum Hussain\n\nAlex Cervasio, a digital consultant who runs the newsletter The Daily Coach, discovered Swapstack late last year. Since then, he has signed deals to sponsor six editions of Sunday Thinking, a weekly column of his newsletter, for between $125 and $600 per post, according to documents reviewed by Insider. Cervasio, whose newsletter has more than 25,000 subscribers, says that the main value that Swapstack provides is the connection to interested advertisers.\n\n\"The biggest issue for me \u2014 for any creator, really \u2014 is just finding the point of contact for these companies that are willing to sponsor or advertising emails,\" Cervasio said.\n\nAnum Hussain, a Hubspot alumna whose newsletter Below the Fold has more than 8,000 subscribers, said that the advertiser sourcing that Swapstack does saves her time cold emailing potential sponsors.\n\n\"There are so many potential sponsors out there, so having a vetting system that allows you to be easily introduced to them takes a lot off my plate,\" said Hussain.\n\nThe ad rush\n\nOther platforms like Letterwell and Upstart.me are also hoping to turn independent newsletter advertising into a business.\n\nWhereas Swapstack is little more than a Slack channel and a dream at the moment, Letterwell and Upstart.me both have fully developed platforms that connect advertisers and newsletter writers.\n\nLetterwell has more than 290 newsletters on its site and has helped connect \"thousands\" of brands and newsletters, according to founder Sohum Shah. The platform caters to smaller newsletters and plans to launch as a programmatic advertising platform for email newsletters in the coming months.\n\n\"Think of Google Adsense or Facebook Ads but for email newsletters,\" Shah said. \"We're essentially automating the whole advertising process, allowing advertisers to buy a specific number of views in an email newsletter campaign.\"\n\nUpstart.me, which was founded by Karim Amrani, has 2,500 advertisers and 110 newsletters on its platform, according to documents reviewed by Insider. Amrani said that the site is not an ad platform and that it won't take a percentage of revenue. It will, however, offer newsletters \"pro\" options that allow them to pay for promotion on the website.\n\n\"Upstart.me is like a community pinboard that connects advertisers with newsletters,\" Amrani said.\n\nBoth allow newsletter writers to submit their products for inclusion on the platform, free of charge, and Letterwell already has a number of high-profile clients on its roster, including Scott's Cheap Flights, a popular newsletter with 1.8 million subscribers.\n\nPain points\n\nOn Swapstack, Upstart.me, and Letterwell, advertisers all clamored for newsletters that dealt with finance, investing, venture capital, or related industries. These newsletters are more likely to have affluent readerships who are keen to spend money, which makes them more valuable than the readers of, say, a cultural newsletter.\n\nOther problems abound: Key metrics in the world of digital advertising, such as clickthrough rates, must be tracked manually. The specifics of the deals, such as price, duration, and creative, must be haggled out, which can put those unfamiliar with digital marketing practices at a disadvantage.\n\nOn a more existential level, all three startups must reckon with the fact that their business strategy is at odds with the ethos and financial model of Substack. If these services prove popular and free Substack users increasingly choose to use ads rather than subscriptions, they could force Substack into action.\n\nThe founders could renege on their promise to keep Substack an ad-free ecosystem, an enticing option for investors who might see Substack's disavowal of advertising as leaving money on the table. Or they could keep their commitment to an ad-free platform by either barring third-party advertising services or letting them continue unabated, a decision that could potentially hurt their business.\n\nThe possibility also exists that Substack could solve this problem by charging newsletter writers to use their platform, rather than offer their software for free. Other newsletter platforms, such as Revue, Ghost, and Mailchimp, cost anywhere from $5 to $300 a month and let users choose whether or not they want to run ads.\n\nNonetheless, the emergence of advertising in Substack's ad-free ecosystem raises intriguing questions about the future of the platform and of the newsletter boom more broadly. If the writers want ads, but the founders want subscriptions, who wins?\n\n\"Our goal,\" said Swapstack cofounder Singer, \"is just to make it easy for people to do something that they are already doing.\"",
            "published_at": "2021-01-20T00:00:00"
        },
        {
            "authors": [],
            "title": "Plasma 5.21 Beta",
            "contents": "Plasma 5.21 Beta We did a thing. A pretty one Plasma 5.21 is all about upgrading the looks and usability of Plasma. Thursday, 21 January 2021.\n\nPlasma 5.21 Beta Development of the upcoming version of Plasma 5.21 has, among other things, introduced many improvements into Plasma\u2019s design, utilities and themes, with the aim of providing end users with a more pleasant and accessible environment. Although most things in Plasma 5.21 beta will work fine, please note it is still beta software and released mainly for testing purposes. Using this software in production is NOT recommended. The final version of Plasma 5.21 will be available on the 16th of February. New Application Launcher Plasma 5.21 introduces a new application launcher that features a double-pane UI, improvements to keyboard and mouse navigation, better accessibility and support for languages with RTL writing. The new launcher includes an alphabetical \u201cAll Applications\u201d view, a grid-style favorites view, and power actions visible by default with their labels. Last but not least, we have fixed most of the bugs reported by users, guaranteeing a smoother access to all your stuff. The old Kickoff app launcher is still available at store.kde.org. Application theme improvements Applications using Plasma\u2019s default theme now have a refreshed color scheme and sport a brand new unified headerbar style with a clean, cool new look.\n\nBreeze Twilight Meet Breeze Twilight: a combination of a dark theme for Plasma and a light theme for applications, so you can enjoy the best of both worlds. It\u2019s available in the Global Theme settings\n\nPlasma System Monitor Plasma System Monitor is a new UI for monitoring system resources. It is built on top of Kirigami and a system statistics service called \u201cKSystemStats\u201d. It shares code with the new system monitor applets introduced in Plasma 5.19 and is designed to succeed KSysGuard. Plasma System Monitor provides many different views, offering an overview page that provides information on important core resources, like memory, disk space, network and CPU usage. It also provides a quick view of the applications consuming the most resources. If you need more details, the Applications page shows you all the running applications along with detailed statistics and graphs. A process page is also available for per process information. History shows the evolution of the use of your machine\u2019s resources over time. Finally, you can also create new customized pages using the page editor. This lets you tailor the information you get from your system to your needs.\n\nKWin and Wayland KDE is pushing to have first class support for Wayland, and Plasma 5.21 makes great headway to reach that goal. The compositing code in KWin has been extensively refactored and should reduce latency throughout all compositing operations. We have also added a control in the compositing settings so you can choose whether you prefer lower latency or smoother animations. In addition, we have also added support for mixed-refresh-rate display setups on Wayland, e.g. you can have one screen refreshing at 144Hz and another at 60Hz. Preliminary support for multiple GPUs was also added on Wayland. The virtual keyboard in Wayland has been improved and now supports GTK applications using the text-input-v3 protocol. The support for graphical tablets has also been improved and now includes all the controls that were missing in the previous version, such as pad ring and pad buttons. Apart from the numerous improvements in stability, there are quite a few Plasma components that are getting much better support in Wayland. For example, KRunner is now able to list all open windows in Wayland, and we now support features required for GTK4, so GTK4 application will now work.\n\nSystem Settings Plasma 5.21 brings a new page to the System Settings: the Plasma Firewall settings. This configuration module lets you set up and edit a Firewall for your system and is a graphical frontent for both UFW and firewalld. Multiple pre-existing configuration pages have been completely rewritten and are now cleaner and easier to use. This has been the case for the Accessibility, Desktop Session and SDDM configuration modules.\n\nApplets The Media Player applet\u2019s layout has been improved and now includes the list of applications currently playing music in the header as a tab bar. Another upgrade is that the album cover now takes up the whole width of the applet.\n\nPlasma Mobile Plasma has always been designed to be form factor flexible. It can work on a desktop but it\u2019s also easily adaptable to work on a mobile. The PinePhone KDE Community Edition is now shipping. In Plasma 5.21 we are adding two new components for mobile in the official release. The Plasma Phone Components contains the mobile shell but also specific plasma widgets adapted for Plasma Mobile.\n\nQQC2 Breeze Style is a pure Qt Quick Controls 2 style. It visually fits in with the widget based desktop Breeze theme and was optimized for lower RAM and GPU usage.\n\nOther Updates Discover Support unattended updates Support unattended updates KRunner Pin KRunner (doesn't close automatically) Pin KRunner (doesn't close automatically) Digital Clock Better support for timezones Better support for timezones Sound Applet The sound applet now displays the live microphone volume The sound applet now displays the live microphone volume\n\nA full changelog is available here\n\nFeedback You can provide feedback direct to the developers via the #Plasma IRC channel, Plasma-devel mailing list or report issues via Bugzilla. If you like what the team is doing, please let them know!\n\nSupporting KDE\n\nKDE is a Free Software community that exists and grows only because of the help of many volunteers that donate their time and effort. KDE is always looking for new volunteers and contributions, whether it is help with coding, bug fixing or reporting, writing documentation, translations, promotion, money, etc. All contributions are gratefully appreciated and eagerly accepted. Please read through the Supporting KDE page for further information or become a KDE e.V. supporting member through our Join the Game initiative.\n\nAbout KDE\n\nKDE is an international technology team that creates free and open source software for desktop and portable computing. Among KDE's products are a modern desktop system for Linux and UNIX platforms, comprehensive office productivity and groupware suites and hundreds of software titles in many categories including Internet and web applications, multimedia, entertainment, educational, graphics and software development. KDE software is translated into more than 60 languages and is built with ease of use and modern accessibility principles in mind. KDE's full-featured applications run natively on Linux, BSD, Solaris, Windows and Mac OS X.\n\nTrademark Notices.\n\n\n\nKDE\u00ae and the K Desktop Environment\u00ae logo are registered trademarks of KDE e.V.\n\nLinux is a registered trademark of Linus Torvalds. UNIX is a registered trademark of The Open Group in the United States and other countries.\n\nAll other trademarks and copyrights referred to in this announcement are the property of their respective owners.\n\nFor more information send us an email:\n\npress@kde.org",
            "published_at": "2020-01-21T15:00:00+00:00"
        },
        {
            "authors": [],
            "title": "'The Sound and the Fury' as William Faulkner imagined, in color",
            "contents": "\n\nAlthough William Faulkner won a Nobel Prize in literature, his writing is still considered particularly dense. One of his most difficult works is \"The Sound and the Fury,\" which is told from multiple points of view. It begins in the voice of Benjy, a mentally disabled man whose perception is jumbled, immediate and distinctly hard to parse.\n\nOne of the reasons Benji's narrative is hard to follow is because it jumps around in time with little indication of the change, other than italics. But when Faulkner was working on the book in the 1920s -- \"The Sound and the Fury\" was published in 1929 -- he imagined a way to make the section clearer to readers. \"I wish publishing was advanced enough to use colored ink,\" Faulkner wrote to his editor, \"as I argued with you and Hal in the Speakeasy that day.\"\n\n\"I'll just have to save the idea until publishing grows up,\" he added, inadvertently launching a challenge to future publishers. Nine decades later, the Folio Society took it up.\n\nIn a special edition, the Folio Society is publishing \"The Sound and the Fury\" in 14 colors. It's a fine press edition, quarter-bound in leather, with a slipcase and an additional volume of commentary. It also includes a color-coded bookmark that reveals which time period is designated by each color.\n\nThe Folio Society worked with two Faulkner scholars, Stephen Ross and Noel Polk, to figure out how to divide the text. Only the Benjy section is rendered in the 14 colors of ink.\n\n\"With the Benjy section the different threads are sufficiently clear that I don't feel we are distorting or compromising the novel,\" Folio's commissioning editor for limited editions Neil Titman told the Guardian. \"I found the book tremendously confusing the first time I read it, so I think that overall you have a net gain here, rather than feeling over-guided.\"\n\nThe color edition of Faulkner's \"The Sound and the Fury\" is being published July 6 in a limited edition of 1,480 and is priced at $345. One thousand preordered copies have been sold.\n\nRELATED:\n\nWilliam Faulkner's Mississippi\n\nEven William Faulkner had a day job\n\nInnovators in print books and e-books\n\n-- Carolyn Kellogg\n\nPhoto: William Faulkner's \"The Sound and the Fury\" in color. Credit: The Folio Society"
        },
        {
            "authors": [
                "Gemma Peplow",
                "Entertainment Reporter"
            ],
            "title": "Gary Numan: 'One of my songs got over a million streams - I got \u00a337'",
            "contents": "Gary Numan has told Sky News he received just \u00a337 for a hit that had been streamed more than a million times, as an inquiry into the economics of digital music services continues.\n\nThe British singer-songwriter, best known for hits including Are 'Friends' Electric?, Cars and We Are Glass in the 1970s and 80s, has sold millions of records over the years and is set to release his 18th solo studio album, Intruder, in May.\n\nWith artist revenue from touring obliterated by the COVID-19 pandemic, platforms such as Spotify, Apple Music, Amazon Music and Google Play have come under increased scrutiny in the past 12 months and are currently being looked into by a department of culture, media, and sport committee (DCMS) inquiry.\n\nImage: Nile Rodgers is among several artists who have appeared at an inquiry into streaming. Pic:John Nacion/STAR MAX/IPx/AP\n\nOn Wednesday, executives from the UK's \"big three\" major labels - Sony Music, Warner Music and Universal Music - as well as representatives from licensing bodies, appeared before MPs for the third session of the hearing, which has already seen artists such as Elbow frontman Guy Garvey, Radiohead guitarist Ed O'Brien, Chic's Nile Rodgers and singer-songwriter Nadine Shah give evidence.\n\nThe label heads told MPs their cut from streaming was a fair reward for the risks involved in developing artists, recording, marketing and distribution.\n\nAdvertisement\n\nNuman is not appearing at the inquiry, but gave his thoughts on the revenue from streaming in an interview with Sky News - putting the figures into context.\n\n\"The solution's simple,\" he said. \"The streaming companies should pay more money. They're getting it for nothing.\n\n\"I had a statement a while back and one of my songs had had over a million plays, million streams, and it was \u00a337. I got \u00a337 from a million streams.\"\n\nGiving another example, Numan continued: \"I printed out, I think it was about a year ago, a statement - my streaming statement came in and I didn't look at it, I just put it to print, and I looked over about half an hour later, it was still printing.\n\n\"It was hundreds and hundreds of pages. And the end of it was, like, \u00a3112. It was barely worth the [paper] it was printed on, and it took nearly half an hour to print. You know, it's so much stuff, so much streaming, and there's absolutely nothing in it.\"\n\nStreaming currently accounts for more than half of the global music industry's revenue and brings more than \u00a31 billion to the UK in revenue, with 114 billion music streams in the last year.\n\nBut according to the Broken Record campaign, artists receive around 16% of the total income from streams, while record companies get around 41% and streaming services around 29%.\n\nNuman said that very big artists can do well from streaming, but smaller artists struggle.\n\n\"If you're really at the top, then you can earn pretty well from streaming,\" he said. \"If you're not, you might as well forget it, it isn't even worth printing it out, printing out the statement.\"\n\nThe star's words were echoed during the latest DCMS session by Peter Leathem, chief executive of music copyright collective Phonographic Performance Ltd, who said: \"If you look at 2019, the best-selling albums were Queen, Bohemian Rhapsody, based on the film, and Abbey Road by the Beatles, its 50-year anniversary.\n\n\"If you are trying to break a new artist or trying to get your own streaming going you have got the last 50 years of the music industry to compete with.\"\n\n:: Subscribe to the Backstage podcast on Apple Podcasts, Google Podcasts, Spotify, Spreaker\n\nBut Tony Harlow, chief executive of Warner Music UK, cautioned against disrupting the system.\n\n\"This is an evolving situation,\" he said. \"It is being well-governed by a market that is efficient and nimble and it doesn't need any change.\n\n\"Any disruptions could diminish UK competitiveness at a time when I feel the UK needs to be the home of recorded music, just as it is by providing one in 10 streams around the world, by being the number two export business.\"\n\nDuring the session, it was suggested that major labels were operating like an \"oligopoly\" - to which Jason Iley, chairman and chief executive of Sony Music UK and Ireland, replied: \"The independent sector is a brilliant sector and signs some of the best acts.\n\n\"There is more opportunity for artists to either sign to a major label or sign to an independent label or distribute their own records. There are more avenues today than I have ever seen in my time doing this job.\""
        },
        {
            "authors": [
                "Tim Bray"
            ],
            "title": "ongoing by Tim Bray \u00b7 When You Know",
            "contents": "I\u2019m a person who knows a lot about how computers and software work, is generally curious, and reads fast. I\u2019ve been wrong about lots of things over the years. But there have a been a few times when a combination of technology-literacy and just paying attention to the world have made me 100% sure that I was seeing something coming that many others weren\u2019t. Here are a few of those stories. The reason I\u2019m telling them is that I\u2019m in another of those moments, seeing something obvious that not enough other people have, and I want to offer credentials before I share it. Also, some of the stories are entertaining.\n\nUnix in the Eighties \u00b7 As an undergrad, I used Unix V6 on a PDP-11/34 back in 1979, but when I graduated the dinosaurs that stomped the earth had labels like \u201cVMS\u201d and \u201cMVS\u201d painted on the side, and mega millions of investment and marketing behind them. (If you don\u2019t know what those labels stood for, that\u2019s OK). But from time to time I got my hands on a Unix command line and kept thinking \u201cThis is just better\u201d and then one time I wrote code that did networking and understood the power of fork and exec.\n\nSo I started going around telling all these IT management types that Unix was better than what they were using, and got blank looks, and when I got really insistent was eventually told to shut up. At that point I was young enough that I was convinced that, well maybe, I was just crazy, after all these were guys who\u2019d been doing IT for decades. The rest is history.\n\nJava in the Nineties \u00b7 I was a C and FORTRAN guy, then in 1996 I was helping design XML and told myself that it\u2019d be cool if XML had a working XML parser. In 1997 Java was The New Hotness so I decided to learn it and use it. I actually used Microsoft\u2019s Visual J++ which, for the time, was pretty great.\n\nEventually I published Lark, the world\u2019s first XML parser, then a couple of years later gave up on it because Microsoft and Sun both had their own parsers and who was I to compete with titans? I regret that because Lark was faster and had a nice API and if I\u2019d maintained it, it\u2019d probably be popular to this day.\n\nBy the time I\u2019d done Lark, I\u2019d seen the advantages of a programming language that came with a good standard library, ran on a VM, had garbage collection, and had a reasonably clean, minimal design.\n\nSo I started telling everyone I knew that they should do their next project in Java. Everyone I knew blew me off and said that Java was too slow compared to C++, had a primitive GUI compared to Visual Basic, didn\u2019t have government buy-in compared to Ada, didn\u2019t have a mainframe story compared to PL/1, or didn\u2019t let you go fast and loose compared to Perl.\n\nBy this time my ego had expanded and I didn\u2019t shut up and I think I may have actually changed a few people\u2019s minds.\n\nThe Web in the Nineties \u00b7 This was the one that was most irritating. By the late Nineties, the Web had expanded out of the geek-enthusiast space and Open Text, a company I co-founded, had done a nice IPO based on Web search and a Web document-management UI. I remember like yesterday a presentation at one of the early Web meetups, an engineering lead for a (then) big computer company. She said \u201cThis is so great. Our interfaces used to have to be full of sliders and dials and widgets or people would say we were amateurs. But now with the Web, there\u2019s so much less you can do, but the important things are easier, and that\u2019s what people want!\u201d She was right.\n\nBetween 1996 and 1999 I was an indie consultant, trading off my ill-gotten fame as a Web Search pioneer and XML co-inventor. Everyone who hired me got told that they should damn well invest in Web delivery and stop investing in anything else. I heard \u201cBut native GUI is a much richer environment\u201d and \u201cThe network will never be fast enough\u201d and \u201cYeah, that stuff is just toys for kids, we\u2019re serious Enterprise Analysts here.\u201d\n\nThe change came, as they always do, maddeningly slow then frighteningly fast. My recollection is that the advent of fedex.com was very influential. Even the most non-technical business person could glance at it, realize \u201cAll I have to do paste in a tracking number, didn\u2019t have to install any software, and there\u2019s my answer.\u201d\n\nWS-* in the Naughties \u00b7 Once the Web had become everyone\u2019s favorite GUI, people started to notice that it was pretty easy to set up a network-facing API using HTTP. (And at that time XML, which made things harder, but it was still pretty easy.) Way easier than the incumbent technologies like CORBA and DCOM and so on. Meanwhile, Roy Fielding was working on his doctoral dissertation which established the key concepts of REST.\n\nFor some reason that I\u2019ve never understood, IBM and Microsoft chose this time to launch a land-grab. In a really annoying and unprincipled way, they rallied behind the banner of \u201cXML Web Services\u201d and jammed a huge number of mammoth, stupidly-complex \u201cWS-*\u201d specifications through compliant standards organizations. Even the most foundational of these, for example WSDL, was deeply broken.\n\nA few people (including me) thought the Web didn\u2019t need WS-layering, and that what we would come to call REST was astonishingly simpler and already known to work. We recoiled in horror and became avid anti-WS-* campaigners. Of all the things in my life that I\u2019ve found myself against, WS-* was the softest target, because it basically just didn\u2019t work very well. I remember with glee publishing blog pieces like WS-Pagecount and (especially) WS-Stardate 2005.10. Because the best way to attack a soft target is to make people laugh at it.\n\nWS-* actually survives, last time I checked, as Microsoft WCF. But nobody cares.\n\nAndroid in 2010 \u00b7 When the iPhone launched in 2007, I was working at Sun, i.e. Java World Headquarters. Like everyone else, I was captivated by the notion of a pocketable general-purpose computer with a built-in GPS and phone and camera and so on. Unlike most, I was appalled by the App Store axiom that I could write code for the thing, but I couldn\u2019t publish it unless Apple said I could. Also, I was (and remain) not crazy about Objective C.\n\nSo when Android arrived on the scene, it got my attention. Among other things, you programmed it with what felt like pretty ordinary mainstream Java, which I and a lot of people already knew. And if I didn\u2019t want to use the Google store, I could post my app on my website and anyone could use it.\n\nSo in 2008, I wrote the Android Diary series, describing my experiences in getting an Android phone and writing my first app, which was so much fun. I discovered that the development environment, while immature, was basically clean and well-designed, and accessible instantly to anyone who knew Java.\n\nPeople laughed at me, saying the iPhones were faster (true), had better UI design (true), and were uncatchably-far ahead, measured by unit sales (not true). Eventually Oracle bought Sun and I left and I got a nice job in the Android group at Google. When I joined, there were roughly ten thousand Android devices being sold per day. When I left, it was over a million.\n\nWhat I see now: Run screaming from Bitcoin \u00b7 It is completely unambiguously obvious to me that Bitcoin, a brilliant achievement technically, is functioning as a Ponzi scheme, siphoning money from the pockets of rubes and into those of exchange insiders and China-based miners. I\u2019m less alone in this position than I was in some of those others, I think a high proportion of tech insiders know perfectly well that this is a looming financial disaster.\n\nI am not going to re-iterate all the arguments as to why this is the case. If you want to find out, follow Amy Castor.\n\nOK, let me add one additional argument for why Bitcoin is not and can never be \u201creal\u201d money. You know what real money is? Money you can use to pay your taxes. The USA, in 2018, had about 140 million taxpayers. Suppose 10% of them wanted to use Bitcoin to pay their taxes. Let\u2019s say the global Bitcoin network can process ten transactions per second (it can\u2019t, it\u2019s slower than that). By my arithmetic, at 10/second it would take the whole network, running flat out, not doing anything else, over five months to process those payments and refunds. This is just Federal Income Tax.\n\nDon\u2019t get into Bitcoin. If you\u2019re in, get your money out while you still can.\n\nTrust me on this.",
            "published_at": "2021-01-20T00:00:00"
        },
        {
            "authors": [
                "Jakob Greenfeld"
            ],
            "title": "How management by metrics leads us astray",
            "contents": "Let\u2019s say my goal this year is to get to 10k monthly recurring revenue (MRR).\n\nMRR first, everything else second. I would ruthlessly cut out all activities that don\u2019t lead to a measurable MRR increase. For example, I would stop writing blog posts like the one you\u2019re currently reading. I certainly wouldn\u2019t create fun projects like What to Tweet or take the time for non-transactional Zoom calls.\n\nI would focus on my revenue generating projects, crank up the advertising machinery and do everything I can think of that potentially increases my MRR. I\u2019m fairly confident that I would reach 10k MRR by the end of the year. But at what price?\n\nLet\u2019s take a step back for a second.\n\nYou know who manages by metrics? Big companies like Google, Amazon and LinkedIn.\n\nAnd what do they have in common?\n\nTheir core product got notably worse over time.\n\nGoogle\u2019s search results are dominated by ads and many users now use workarounds to find what they\u2019re looking for (\u201cBest headphone reddit\u201d). LinkedIn looks like Minesweeper. Facebook was a fun place to meet friends. And if you search for an electronics product on Amazon, you immediately feel like you\u2019re at a flea market in the middle of Shenzhen. This is the result of hundreds of decisions that were motivated by a short-term focus on specific metrics like revenue and click rates. And while these decisions most likely optimized the metrics, they made the user experience worse.\n\nThe problem is that we don\u2019t have the technology to measure the right thing. Or maybe the \u201cright thing\u201d is inherently immeasurable.\n\nOr, and that\u2019s my personal favorite, we\u2019re dealing with a Schr\u00f6dinger\u2019s cat like situation. The \u201cright thing\u201d is only the right thing as long as we\u2019re not trying to optimize it. This is known as Goodhart\u2019s law. \u201cWhen a measure becomes a target, it ceases to be a good measure.\u201c\n\nLet\u2019s consider another example.\n\nWho else manages by metrics? Politicians!\n\nInstead of some boring historic example (there are plenty, just read \u201cSeeing like a State\u201d), I\u2019m going to spice things up by talking about something that\u2019s happening right now. It\u2019s not hard to anticipate what I want to talk about since there\u2019s precisely one thing that\u2019s currently happening: Covid-19.\n\nPoliticians try to manage the situation by focusing on metrics like the \u201cR number\u201d. At least here in Germany the motto is: let\u2019s lower the R number at all costs. Turns out the cost of this strategy is incredibly high. It\u2019s just not immediately measurable.\n\nFor example, everyone\u2019s getting fatter since all gyms and sport clubs are closed. Most people are developing unhealthy eating habits because there\u2019s simply nothing else to do. Social connections are rapidly degregating even though everyone knows how important they\u2019re for the human wellbeing.\n\nI\u2019m not a doctor and don\u2019t play one on the internet, but I can easily imagine that there are lots of further unintended consequences of the current lockdown policy. For example, since most people now spend most of their time indoors alone and wear masks when they leave their homes, their immune system isn\u2019t stressed regularly like it usual is. Hence I wouldn\u2019t be surprised if many people will get sick of other diseases once the lockdown ends as a result of weakened immune systems.\n\nJust to clarify, I\u2019m all for taking Covid seriously. In fact, I\u2019ve been hoarding masks since 2009. I spent the summer of 2009 with a bunch of friends in Lloret de Mar (a party hotspot in Spain) just when news of the swine flu started to emerge. Long story short, everyone in our group got sick (20+ people) except for my girlfriend and me because we\u2019ve been wearing masks. Now the most famous example how focusing on metrics can have unhealthy consequences is the cobra effect. I\u2019ll just quote from Wikipedia:\n\n\u201cThe term cobra effect originated in an anecdote that describes an occurrence during India under British rule. The British government was concerned about the number of venomous cobras in Delhi. The government therefore offered a bounty for every dead cobra. Initially, this was a successful strategy; large numbers of snakes were killed for the reward. Eventually, however, enterprising people began to breed cobras for the income. When the government became aware of this, the reward program was scrapped. When cobra breeders set their now-worthless snakes free, the wild cobra population further increased.\u201d\n\nThere is one more example I always have to talk about when the topic comes up: academia.\n\nStagnation apologists love to argue that the lack of progress in science is simply a result of the fact that all low-hanging fruit are gone.\n\nThat\u2019s complete nonsense. There\u2019s still plenty of low-hanging fruit. It\u2019s just that everyone is so busy chasing meaningless metrics that they can\u2019t see them.\n\nIn academia the metric of choice usually has something to do with the number of citations. But just like Google\u2019s backlink-driven algorithm (which was in fact inspired by academia\u2019s citation metrics) was quickly gamed by savvy webmasters, academia\u2019s algorithm is gamed by careerists.\n\nAnd I\u2019m not talking about some tiny minority here. If you want to survive in academia, you have to play the citation game. And if everyone is cheating, at the very least you have to do the same to stand a chance.\n\nYou have to partner up with others because five people can write five times as many papers. You agree to constantly cite each other. Likewise, you focus on incremental additions to established ideas because that\u2019s the safest way to new publications regularly. You work on the stuff everyone else is working on because how else are you going to get citations?\n\nAnd one thing you absolutely have to avoid like the plague is the risky and deep kind of research that leads to real progress in the field.\n\nThe problem isn\u2019t that citations are a bad metric, and we need to come up with smarter ones. Instead, it\u2019s just Goodhart\u2019s law in action. Every attempt to manage academia makes it worse.\n\nAs soon a new metric is introduced it will misdirect the attention of scientists towards playing the game, instead of progressing science.\n\nI talked about three areas, but the same pattern occurs almost everywhere. For example, just remember when everyone started minimizing fat in their diet or how grades are used to measure \u201clearning\u201d.\n\nCrucially the problem is not the specific metric that\u2019s being used. Whatever new metric gets introduced will soon again be made useless by Goodhart\u2019s law. The metrics game will always be akin to Whac-A-Mole.\n\nNow where does this leave us? If management by metrics doesn\u2019t work, what else should we do?\n\nFirst of all, I think that looking at metrics can be helpful. They just shouldn\u2019t be the sole yardstick decisions are measured against.\n\nAnd while the map never will be the territory, the picture you\u2019re looking at gets more accurate the more metrics you consider. For example, if politicians would not just consider the R number but hundreds of metrics that take different aspects of a population\u2019s wellbeing into account, the decisions would be better ones.\n\nThe obvious problem is that decisions become much harder if you consider more than one metric.\n\nBut more importantly there are so many factors we can\u2019t measure that nevertheless should be taken into account.\n\nThe more I think about it the more I become convinced that it all boils down to talking to people.\n\nAnd I\u2019m not talking about surveys that distill thousands of standardized \u201cconversations\u201d into a few numbers or analytics tools that do similar things for behavior. I\u2019m talking about real one-on-one interactions.\n\nJust imagine if Jeff Bezos would talk to hundreds of real (non pre-vetted) Amazon customers each month and take what he hears in these conversations seriously. Or if politicians would have long conversations with regular people from all wakes of life. Or if academic committees would actually talk to applicants before filtering them out based on citation metrics.\n\nIn our hyper rational world it\u2019s extremely hard to justify decisions based on the mere gut feeling you got from a few conversations. But this is exactly what people should be doing.\n\nEngagement metrics will never tell you if users are genuinely happy when they use your product. But a few genuine conversations will.\n\nA resume will never tell you if a person is genuinely interested in uncovering unknown truths about nature. But a 30-minute conversation will.\n\nYes, it\u2019s so much easier to reach a consensus if you just hire the candidate with the highest citation metrics or implement the feature that will lead to the largest revenue increase in the next quarter. But while it\u2019s the easiest decision it\u2019s usually not the best one.\n\nAnd I think the same applies not just to managing organizations but also to managing ourselves.\n\nJust by listening to my own body I\u2019ll always know better than any fitness tracker if I slept well, better than any blood test if my diet is healthy, and better than my bank account if I\u2019m happy. That doesn\u2019t mean that I\u2019ll not use a fitness tracker, make regular blood tests, or check my bank account. But I certainly won\u2019t allow them to dictate my decisions.\n\nI\u2019ll look at all the facts and take everything in, even the stuff that can\u2019t be measured, and then I\u2019ll just go with my gut."
        },
        {
            "authors": [],
            "title": "lmatteis/torrent-net: Distributed search engines using BitTorrent and SQLite",
            "contents": "TorrentNet\n\nDistributed search engines using BitTorrent and SQLite\n\nAbout\n\nDistributed sites have gained much attention lately with systems such as ZeroNet and IPFS, which seem to improve on older systems like Freenet.\n\nBuilding search engines on top of these distributed systems is not quite feasible yet as users need to download the entire site database (usually several hundreds of gigabytes large) before running queries against it.\n\nTorrentNet allows you to build distributed search engines by leveraging the BitTorrent network and the SQLite database. Users do not have to wait until the site is fully downloaded before they can query it.\n\nSite owners create an SQLite database (.db file) and create a torrent from this file. They then proceed to seed this torrent -- just like they would seed any other file.\n\nSite users then start downloading the site torrent, but, rather than downloading pieces of the torrent in \"rarest first\" order, they download pieces based on the search query they performed.\n\nIn other words, given a search query such as \"indiana jones\", TorrentNet knows to download pieces of the torrent where data for \"indiana jones\" is likely to be stored at.\n\nHence, results are given to users in a timely manner, without having to wait until the entire database is downloaded.\n\nFurthermore, since search queries are just regular torrent piece downloads, a search query can be satisfied by many different peers in a swarm, effectively allowing for distributed search engines.\n\nMain features\n\nDistributed torrent sites, queryable on demand by prioritizing specific piece downloads.\n\nCreate distributed torrent sites accessible via your public key (eg. 33cwte8iwWn7uhtj9MKCs4q5Ax7B ) that are shared and kept alive using the BitTorrent network.\n\nUpdate your site using the Mutable Torrents extension (BEP46) and let your users know about site updates via the DHT network.\n\nBuild interactive sites that are queryable on demand using the sqltorrent technique. Things like search engines, or complex browsable experiences are possible.\n\nPros & cons\n\nPros Hosting your site is as simple as seeding a torrent. No need to buy a domain name or a hosting server. You can easily host your torrent site on your home network and let users visiting your site help you with the hosting. Since you control your address (public key), which is broadcast via the DHT, it's much harder for governments and institution to block the content you're sharing. Via sqltorrent you drive your users experience by letting them only downloading pieces of the torrent that are relevant to the users' interaction. Essentially you could create a search engine, and program interactions so that submissions of a search form result in queries to the underlying SQLite database - which prioritizes pieces based on the query.\n\nCons Read only sites for now. ZeroNet seems to have a solution to this problem but I'm not convinced yet. Your site doesn't have a pretty name. Sharing your public key can be much harder than sharing the name of a DNS site. No standards for doing distributed sites. Many different solutions (IPFS, ZeroNet, Freenet, etc..). TorrentPeek at least follows most of the BitTorrent specifications (using the powerful libtorrent library) and provides unique SQL querying capabilities.\n\n\n\nsqltorrent\n\nBy putting an SQLite database file (.db) inside a torrent, we can query its content -- by prioritizing pieces based on the SQL query -- and quickly peek at the content of the database without downloading it entirely.\n\nTorrentPeek was inspired by sqltorrent.\n\nA video is worth 1000 words: https://www.youtube.com/watch?v=EKttt8PYu5M&feature=youtu.be\n\nInstall\n\nThis currently only works on Mac OS X."
        },
        {
            "authors": [],
            "title": "profullstack/bitchin.net",
            "contents": "My dad used to say \"Bitchin'\" a lot when I was a kid. This site is dedicated to my dad who died in 2012 due to parkinsons.\n\nI'm learning v and am going to be doing some more with it very soon.\n\nYou can see the running version at https://bitchin.net\n\ninstall\n\nclone it. install v\n\nfor development run v run app.v\n\nTo compile run v app.v then push ./app to your server.\n\nYou can modify .env\n\nLICENSE: do whatever the fuck you want license."
        },
        {
            "authors": [
                "Greg Miller",
                "Senior Editor",
                "Greg Miller Covers Maritime For Freightwaves",
                "American Shipper. After Graduating Cornell University",
                "He Fled Upstate New York'S Harsh Winters For The Island Of St. Thomas",
                "Where He Rose To Editor-In-Chief Of The Virgin Islands Business Journal. In The Aftermath Of Hurricane Marilyn",
                "He Moved To New York City",
                "Where He Served As Senior Editor Of Cruise Industry News. He Then Spent Years At The Shipping Magazine Fairplay In Various Senior Roles",
                "Including Managing Editor. He Currently Resides In Manhattan With His Wife",
                "Two Shih Tzus."
            ],
            "title": "Inside California\u2019s colossal container-ship traffic jam",
            "contents": "In the movie \u201cFalling Down,\u201d the character played by Michael Douglas is stranded in a Los Angeles traffic jam. He abandons his car, starts walking with briefcase in hand and ultimately has a mental breakdown. Cargo shippers trying to get their containers through the ports of Los Angeles and Long Beach can relate.\n\nThe pileup of ships offshore in San Pedro Bay and congestion onshore at the terminals have reached epic proportions.\n\nAnd the situation could become even more maddening in the weeks ahead.\n\n32 container ships at anchor\n\nAmerican Shipper interviewed Kip Louttit, executive director of the Marine Exchange of Southern California, to get the latest on ships in San Pedro Bay.\n\nHe reported that as of midday Wednesday, 91 ships were in port: 46 at berth and 45 at anchor. Of those, there were 56 container ships: 24 at berth and 32 at anchor. Between Wednesday and Saturday, 19 more container ships will arrive, with the same number due to depart.\n\nContainer ship locations, Jan. 13 (Map: MarineTraffic)\n\nThere were a few more container ships at anchor on Friday \u201437 in total. Yet Louttit said \u201cthere has been no significant change between the first of January and today.\u201d\n\nLouttit confirmed that ships have effectively filled all of the usable anchorages off Los Angeles and Long Beach. Ships have also taken six of the 10 contingency anchorages off Huntington, the next town south.\n\nIf all the anchorages and contingency anchorages fill up, ships will be placed in so-called \u201cdrift boxes\u201d in deeper water. These are actually circles not boxes. Unlike ships at anchorages in shallower water, ships in drift boxes would not anchor, they\u2019d drift. \u201cWhen you drift out of the circle, which has a radius of 2 miles, you start your engine and go back to the middle of the circle,\u201d explained Louttit.\n\nHistorical perspective on traffic jam\n\nAnchored container ships stretch off into the distance (Photo: Kip Louttit)\n\nGiven the drift-box option, container ships are not about to hit any kind of maximum capacity offshore of California. Nor is there a higher safety risk. \u201cThere are a lot of ships, but they\u2019re all very carefully watched and managed,\u201d affirmed Louttit.\n\nThe significance of so many anchored ships is what they reveal about the extent of the logistics logjam on shore.\n\nThe most recent comparable anchorage level occurred during the labor dispute between the International Longshore and Warehouse Union (ILWU) and their employers in 2014-15.\n\n\u201cOn March 14, 2015, there were 28 container ships at anchor. We\u2019ve blown through that record,\u201d said Louttit. The all-time record for ships at anchorage off California occurred in 2004 during a rail staffing shortage.\n\n\u201cNormally, if you want a baseline, there\u2019d be a dozen and rarely are they container ships,\u201d he said.\n\nSignal still flashing red\n\nThe Marine Exchange does not look past the coming four days\u2019 arrivals. But there are other ways to see what\u2019s headed this way across the Pacific.\n\nIt takes two to three weeks for containers to cross the ocean from China to California. The Port of Los Angeles developed The Signal, a daily digital tool powered by Port Optimizer, to indicate what\u2019s en route. The system uses manifest data from nine of the top 10 carriers calling in Los Angeles.\n\nThe Signal data updated on Wednesday showed no letup in sight. Imports are expected to rise from 143,776 twenty-foot equivalent units (TEUs) this week to 157,763 TEUs next week to 182,953 TEUs the week of Jan. 24-30.\n\n(Chart: The Port of Los Angeles Signal, Jan. 13)\n\nImportantly, the data does not solely include TEUs arriving in a particular week. It also includes TEUs arriving in prior weeks that the port expects to handle in the stated week.\n\nConsequently, the data provides an indirect indicator of how much cargo is getting delayed. For example, on Monday, Jan. 4, The Signal indicated the port would handle 165,000 TEUs that week. But by Friday, Jan. 8, the assessment for that same week had plunged to 99,785 TEUs \u2014 implying that over 65,000 TEUs were pushed to the following week (i.e., this week). This pattern also suggests that the forecast for 182,953 TEUs the week of Jan. 24-30 will ultimately be revised downward.\n\nCongestion causes\n\nIn an alert to customers this week, carrier Hapag-Lloyd reported, \u201cAll terminals [at Los Angeles/Long Beach] continue to be congested due to the spike in import volumes and [this] is expected to last until February.\n\n\u201cTerminals are working with limited labor and split shifts,\u201d it said, asserting that this is related to COVID. \u201cThis labor shortage affects all terminals\u2019 TAT [turnaround time] for truckers, inter-terminal transfers and the number of daily appointments available for gate transactions and delays our vessel operations.\u201d\n\nAs a result of \u201clack of terminal space\u201d to service vessels, \u201cthere is a constant switching of terminals that must be kept in mind\u201d given that containers are ending up \u201cin the wrong terminal,\u201d said Hapag-Lloyd.\n\nCongestion woes are now spreading well beyond California ports, confirmed Hapag-Lloyd. The carrier reported \u201cheavy congestion\u201d in Canada and \u201cberth congestion at Maher Terminal and APM Terminals [in the Port of New York and New Jersey] impacting all services with delays of several days being experienced upon arrival.\u201d\n\nLittle relief ahead\n\nLiner companies traditionally cancel numerous sailings during the Chinese New Year period to account for decreased Chinese exports. If they did so in 2021, it would allow U.S. terminals time to clear some of the inbound congestion. Unfortunately for terminals, liners are opting against canceling sailings during the Chinese holiday period next month.\n\nPorts could also see congestion relief if U.S. consumer demand slowed. However, that does not appear to be happening.\n\nAnalysts believe the \u201cblue sweep\u201d scenario \u2014 with Democrats winning the presidency as well as both houses of Congress \u2014 will spur $1 trillion-$2 trillion in new stimulus during the first half of this year.\n\nInvestment bank Evercore ISI predicted, \u201cAdditional checks will reach consumers at a time when unemployment is lower [than during the 2020 stimulus round], mobility has significantly improved, the overall willingness to spend of the general public is up significantly, confidence levels are higher, housing is strong and the savings rate is still extremely high. That is a set-up for a consumer boom.\u201d Click for more FreightWaves/American Shipper articles by Greg Miller\n\nMORE ON CONTAINERS: \u2018Blue wave\u2019 could spur stimulus on top of stimulus: see story here. Liners highly unlikely to slash service for Chinese New Year: see story here. Container shipping 2021: hangover or party on? See story here.",
            "published_at": "2021-01-14T02:02:25+00:00"
        },
        {
            "authors": [],
            "title": "The effect of early treatment with ivermectin on viral load, symptoms and humoral response in patients with non-severe COVID-19: A pilot, double-blind, placebo-controlled, randomized clinical trial",
            "contents": "Abstract\n\nBackground Ivermectin inhibits the replication of SARS-CoV-2 in vitro at concentrations not readily achievable with currently approved doses. There is limited evidence to support its clinical use in COVID-19 patients. We conducted a Pilot, randomized, double-blind, placebo-controlled trial to evaluate the efficacy of a single dose of ivermectin reduce the transmission of SARS-CoV-2 when administered early after disease onset.\n\nMethods Consecutive patients with non-severe COVID-19 and no risk factors for complicated disease attending the emergency room of the Cl\u00ednica Universidad de Navarra between July 31, 2020 and September 11, 2020 were enrolled. All enrollments occurred within 72 h of onset of fever or cough. Patients were randomized 1:1 to receive ivermectin, 400 mcg/kg, single dose (n = 12) or placebo (n = 12). The primary outcome measure was the proportion of patients with detectable SARS-CoV-2 RNA by PCR from nasopharyngeal swab at day 7 post-treatment. The primary outcome was supported by determination of the viral load and infectivity of each sample. The differences between ivermectin and placebo were calculated using Fisher's exact test and presented as a relative risk ratio. This study is registered at ClinicalTrials.gov: NCT04390022.\n\nFindings All patients recruited completed the trial (median age, 26 [IQR 19\u201336 in the ivermectin and 21\u201344 in the controls] years; 12 [50%] women; 100% had symptoms at recruitment, 70% reported headache, 62% reported fever, 50% reported general malaise and 25% reported cough). At day 7, there was no difference in the proportion of PCR positive patients (RR 0\u00b792, 95% CI: 0\u00b777\u20131\u00b709, p = 1\u00b70). The ivermectin group had non-statistically significant lower viral loads at day 4 (p = 0\u00b724 for gene E; p = 0\u00b718 for gene N) and day 7 (p = 0\u00b716 for gene E; p = 0\u00b718 for gene N) post treatment as well as lower IgG titers at day 21 post treatment (p = 0\u00b724). Patients in the ivermectin group recovered earlier from hyposmia/anosmia (76 vs 158 patient-days; p < 0.001).\n\nInterpretation Among patients with non-severe COVID-19 and no risk factors for severe disease receiving a single 400 mcg/kg dose of ivermectin within 72 h of fever or cough onset there was no difference in the proportion of PCR positives. There was however a marked reduction of self-reported anosmia/hyposmia, a reduction of cough and a tendency to lower viral loads and lower IgG titers which warrants assessment in larger trials.",
            "published_at": "2021-01-19T00:00:00"
        },
        {
            "authors": [
                "Claire Miller"
            ],
            "title": "Trader Joe's, Dollar General And Others Are Paying Workers To Get Vaccines",
            "contents": "Trader Joe's, Dollar General And Others Are Paying Workers To Get Vaccines\n\nEnlarge this image toggle caption Nati Harnik/AP Nati Harnik/AP\n\nA growing number of grocers are adopting a novel approach in the race to get their workers vaccinated against COVID-19: providing pay incentives.\n\nAldi this week became the latest grocery chain to offer employees compensation for getting vaccinated, saying it would provide workers with two hours of pay for each of the two vaccine doses.\n\nThe grocer also promised workers receiving vaccines that they would not lose pay for missed hours from work and that it would help pay for the shots.\n\n\"Providing accommodations so employees can receive this critical vaccine is one more way we can support them and eliminate the need to choose between earning their wages and protecting their well-being,\" said Jason Hart, CEO of Aldi U.S., in a statement.\n\nWith its announcement, the U.S. unit of the German grocer joined Trader Joe's and Dollar General in offering to pay employees extra hourly wages to get the COVID-19 vaccine. Online grocery-delivery firm Instacart, meanwhile, is offering a $25 stipend for eligible workers and contractors.\n\nThe payments come as governments and companies grapple with how to get people vaccinated amid significant skepticism about the doses.\n\nDr. Anthony Fauci, the nation's top infectious disease expert, says the United States will likely need a vaccination level of between 70% and 90% to reach herd immunity.\n\nPaying people is an idea that is winning adherents, but it also has its critics, who believe that offering to pay people could actually reinforce skepticism about the vaccine by making it seem risky.\n\nEmployers have the legal right to require that their workers get the COVID-19 vaccine, with some exceptions, according to experts.\n\nBut it can be tricky. Workers also generally have the right to request medical or religious exemptions to vaccines under federal anti-discrimination laws, and companies are so far wading carefully.\n\nDollar General, for example, made clear it would not force its employees to get vaccinated when announcing its pay incentives last week.\n\n\"We understand the decision to receive the COVID-19 vaccination is a personal choice,\" the chain said. \"And although we are encouraging employees to take it, we are not requiring them to do so.\"",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [],
            "title": "",
            "contents": "JavaScript is not available.\n\nWe\u2019ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.\n\nHelp Center"
        },
        {
            "authors": [
                "Netflix Technology Blog"
            ],
            "title": "The case of the extra 40 ms",
            "contents": "By: John Blair, Netflix Partner Engineering\n\nThe Netflix application runs on hundreds of smart TVs, streaming sticks and pay TV set top boxes. The role of a Partner Engineer at Netflix is to help device manufacturers launch the Netflix application on their devices. In this article we talk about one particularly difficult issue that blocked the launch of a device in Europe.\n\nThe mystery begins\n\nTowards the end of 2017, I was on a conference call to discuss an issue with the Netflix application on a new set top box. The box was a new Android TV device with 4k playback, based on Android Open Source Project (AOSP) version 5.0, aka \u201cLollipop\u201d. I had been at Netflix for a few years, and had shipped multiple devices, but this was my first Android TV device.\n\nAll four players involved in the device were on the call: there was the large European pay TV company (the operator) launching the device, the contractor integrating the set-top-box firmware (the integrator), the system-on-a-chip provider (the chip vendor), and myself (Netflix).\n\nThe integrator and Netflix had already completed the rigorous Netflix certification process, but during the TV operator\u2019s internal trial an executive at the company reported a serious issue: Netflix playback on his device was \u201cstuttering.\u201d, i.e. video would play for a very short time, then pause, then start again, then pause. It didn\u2019t happen all the time, but would reliably start to happen within a few days of powering on the box. They supplied a video and it looked terrible.\n\nThe device integrator had found a way to reproduce the problem: repeatedly start Netflix, start playback, then return to the device UI. They supplied a script to automate the process. Sometimes it took as long as five minutes, but the script would always reliably reproduce the bug.\n\nMeanwhile, a field engineer for the chip vendor had diagnosed the root cause: Netflix\u2019s Android TV application, called Ninja, was not delivering audio data quickly enough. The stuttering was caused by buffer starvation in the device audio pipeline. Playback stopped when the decoder waited for Ninja to deliver more of the audio stream, then resumed once more data arrived. The integrator, the chip vendor and the operator all thought the issue was identified and their message to me was clear: Netflix, you have a bug in your application, and you need to fix it. I could hear the stress in the voices from the operator. Their device was late and running over budget and they expected results from me.\n\nThe investigation\n\nI was skeptical. The same Ninja application runs on millions of Android TV devices, including smart TVs and other set top boxes. If there was a bug in Ninja, why is it only happening on this device?\n\nI started by reproducing the issue myself using the script provided by the integrator. I contacted my counterpart at the chip vendor, asked if he\u2019d seen anything like this before (he hadn\u2019t). Next I started reading the Ninja source code. I wanted to find the precise code that delivers the audio data. I recognized a lot, but I started to lose the plot in the playback code and I needed help.\n\nI walked upstairs and found the engineer who wrote the audio and video pipeline in Ninja, and he gave me a guided tour of the code. I spent some quality time with the source code myself to understand its working parts, adding my own logging to confirm my understanding. The Netflix application is complex, but at its simplest it streams data from a Netflix server, buffers several seconds worth of video and audio data on the device, then delivers video and audio frames one-at-a-time to the device\u2019s playback hardware.\n\nFigure 1: Device Playback Pipeline (simplified)\n\nLet\u2019s take a moment to talk about the audio/video pipeline in the Netflix application. Everything up until the \u201cdecoder buffer\u201d is the same on every set top box and smart TV, but moving the A/V data into the device\u2019s decoder buffer is a device-specific routine running in its own thread. This routine\u2019s job is to keep the decoder buffer full by calling a Netflix provided API which provides the next frame of audio or video data. In Ninja, this job is performed by an Android Thread. There is a simple state machine and some logic to handle different play states, but under normal playback the thread copies one frame of data into the Android playback API, then tells the thread scheduler to wait 15 ms and invoke the handler again. When you create an Android thread, you can request that the thread be run repeatedly, as if in a loop, but it is the Android Thread scheduler that calls the handler, not your own application.\n\nTo play a 60fps video, the highest frame rate available in the Netflix catalog, the device must render a new frame every 16.66 ms, so checking for a new sample every 15ms is just fast enough to stay ahead of any video stream Netflix can provide. Because the integrator had identified the audio stream as the problem, I zeroed in on the specific thread handler that was delivering audio samples to the Android audio service.\n\nI wanted to answer this question: where is the extra time? I assumed some function invoked by the handler would be the culprit, so I sprinkled log messages throughout the handler, assuming the guilty code would be apparent. What was soon apparent was that there was nothing in the handler that was misbehaving, and the handler was running in a few milliseconds even when playback was stuttering.\n\nAha, Insight\n\nIn the end, I focused on three numbers: the rate of data transfer, the time when the handler was invoked and the time when the handler passed control back to Android. I wrote a script to parse the log output, and made the graph below which gave me the answer.\n\nFigure 2: Visualizing Audio Throughput and Thread Handler Timing\n\nThe orange line is the rate that data moved from the streaming buffer into the Android audio system, in bytes/millisecond. You can see three distinct behaviors in this chart:\n\nThe two, tall spiky parts where the data rate reaches 500 bytes/ms. This phase is buffering, before playback starts. The handler is copying data as fast as it can. The region in the middle is normal playback. Audio data is moved at about 45 bytes/ms. The stuttering region is on the right, when audio data is moving at closer to 10 bytes/ms. This is not fast enough to maintain playback.\n\nThe unavoidable conclusion: the orange line confirms what the chip vendor\u2019s engineer reported: Ninja is not delivering audio data quickly enough.\n\nTo understand why, let\u2019s see what story the yellow and grey lines tell.\n\nThe yellow line shows the time spent in the handler routine itself, calculated from timestamps recorded at the top and the bottom of the handler. In both normal and stutter playback regions, the time spent in the handler was the same: about 2 ms. The spikes show instances when the runtime was slower due to time spent on other tasks on the device.\n\nThe real root cause\n\nThe grey line, the time between calls invoking the handler, tells a different story. In the normal playback case you can see the handler is invoked about every 15 ms. In the stutter case, on the right, the handler is invoked approximately every 55 ms. There are an extra 40 ms between invocations, and there\u2019s no way that can keep up with playback. But why?\n\nI reported my discovery to the integrator and the chip vendor (look, it\u2019s the Android Thread scheduler!), but they continued to push back on the Netflix behavior. Why don\u2019t you just copy more data each time the handler is called? This was a fair criticism, but changing this behavior involved deeper changes than I was prepared to make, and I continued my search for the root cause. I dove into the Android source code, and learned that Android Threads are a userspace construct, and the thread scheduler uses the epoll() system call for timing. I knew epoll() performance isn\u2019t guaranteed, so I suspected something was affecting epoll() in a systematic way.\n\nAt this point I was saved by another engineer at the chip supplier, who discovered a bug that had already been fixed in the next version of Android, named Marshmallow. The Android thread scheduler changes the behavior of threads depending whether or not an application is running in the foreground or the background. Threads in the background are assigned an extra 40 ms (40000000 ns) of wait time.\n\nA bug deep in the plumbing of Android itself meant this extra timer value was retained when the thread moved to the foreground. Usually the audio handler thread was created while the application was in the foreground, but sometimes the thread was created a little sooner, while Ninja was still in the background. When this happened, playback would stutter.\n\nLessons learned\n\nThis wasn\u2019t the last bug we fixed on this platform, but it was the hardest to track down. It was outside of the Netflix application, in a part of the system that was outside of the playback pipeline, and all of the initial data pointed to a bug in the Netflix application itself.\n\nThis story really exemplifies an aspect of my job I love: I can\u2019t predict all of the issues that our partners will throw at me, and I know that to fix them I have to understand multiple systems, work with great colleagues, and constantly push myself to learn more. What I do has a direct impact on real people and their enjoyment of a great product. I know when people enjoy Netflix in their living room, I\u2019m an essential part of the team that made it happen.",
            "published_at": "2020-12-14T10:35:16.226000+00:00"
        },
        {
            "authors": [
                "The Raspberry Pi Foundation"
            ],
            "title": "Buy a Raspberry Pi Pico \u2013 Raspberry Pi",
            "contents": "From controlling appliances to operating a light display, Raspberry Pi Pico puts the technology that underpins countless everyday operations into your hands.\n\nProgrammable in C and MicroPython, Pico is adaptable to a vast range of applications and skill levels, and getting started is as easy as dragging and dropping a file.\n\nMore experienced users can take advantage of Raspberry Pi Pico\u2019s rich peripheral set, including SPI, I2C, and eight Programmable I/O (PIO) state machines for custom peripheral support."
        },
        {
            "authors": [
                "David Foster"
            ],
            "title": "I no longer trust The Great Suspender",
            "contents": "I no longer trust The Great Suspender\n\nI know a number of folks use The Great Suspender to automatically suspend inactive browser tabs in Chrome. Apparently recent versions of this extension have been taken over by a shady anonymous entity and is now flagged by Microsoft as malware. Notably the most recent version of the extension (v7.1.8) has added integrated analytics that can track all of your browsing activity across all sites. Yikes.\n\nRecommendations for users of The Great Suspender (7.1.8):\n\nTemporary easy fix\n\nDisable analytics tracking by opening the extension options for The Great Suspender and checking the box \u201cAutomatic deactivation of any kind of tracking\u201d.\n\nPray that the shady developer doesn\u2019t issue a malicious update to The Great Suspender later. (There\u2019s no sensible way to disable updates of an individual extension.)\n\nPermanent harder fix (\ud83d\udc48 Recommended!)\n\nClose as many unneeded tabs as you can.\n\nUnsuspend all remaining tabs. \u23f3\n\nUninstall The Great Suspender.\n\nDownload the latest good version of The Great Suspender (7.1.6) from GitHub, and move it to some permanent location outside your Downloads folder. (It should be commit 9730c09.)\n\nLoad your downloaded copy as an unpacked extension. (This copy will not auto-update to future untrusted versions of the extension.)\n\nAll done! \ud83c\udf89\n\nCaveat: My understanding is that installing an unpacked extension in this way will cause Chrome to issue a new kind of security prompt every time it is launched, which you\u2019ll have to ignore. \ud83d\ude15\n\nOther options\n\nOther browser extensions for suspending tabs exist, as mentioned in the Hacker New discussion for this article. However I have not conducted my own security review on any of those other extensions, so buyer beware.",
            "published_at": "2021-01-20T00:00:00"
        },
        {
            "authors": [],
            "title": "Mibo \u2014 the fun way to meet!",
            "contents": ""
        },
        {
            "authors": [
                "Adi Robertson",
                "Jan"
            ],
            "title": "Twitter\u2019s decentralized social network project takes a baby step forward",
            "contents": "Bluesky, Twitter\u2019s decentralized social networking effort, has announced its first major update since 2019. The Bluesky team released a review of the decentralized web ecosystem and said it\u2019s hoping to find a team lead in the coming months. The review follows Twitter CEO Jack Dorsey discussing Bluesky earlier this month, when he called it a \u201cstandard for the public conversation layer of the internet.\u201d\n\nThe review outlines a variety of known decentralized systems. It includes ActivityPub, known for powering the social network Mastodon; the messaging standard XMPP, which powers WhatsApp and the now-defunct Google Talk; and Solid, a decentralization project led by World Wide Web creator Sir Tim Berners-Lee. The report covers how these systems handle key social network elements like discoverability, moderation, and privacy, as well as how services based on them can scale up, interoperate, and make money.\n\nWorking virtually through the pandemic, this group self-organized, invited additional experts, and created a review of the ecosystem around protocols for social media. You can read it here: https://t.co/U5DczWX1qb \u2014 bluesky (@bluesky) January 21, 2021\n\nThis doesn\u2019t tell us how Bluesky itself might operate. Dorsey said that Bluesky would \u201ctake time to build.\u201d If it results in a protocol, that system might be created from scratch, or it might build on an existing standard like ActivityPub \u2014 a possibility Dorsey mentioned in 2019 upon unveiling the initiative. \u201cWe are trying to do our part by funding an initiative around an open decentralized standard for social media. Our goal is to be a client of that standard for the public conversation layer of the internet,\u201d he tweeted simply.\n\nHowever, the report offers a snapshot of who\u2019s been working on Bluesky. It was authored by Jay Graber, creator of event-organizing platform Happening. Other contributors include Mastodon developer Eugen Rochko, peer-to-peer Beaker Browser co-creator Paul Frazee, ActivityPub standard co-editor Christopher Lemmer Webber, and InterPlanetary File System project lead Molly Mackinlay.\n\nIt also hints at the fact that decentralization often isn\u2019t profitable. The report focuses on monetization options like membership fees and cryptocurrency microtransactions, but it also notes that \u201cmany decentralized projects run on volunteer work and donations\u201d \u2014 something that isn\u2019t ideal for a platform supporting commercial networks like Twitter. That said, platforms have worked on systems like the Brave browser\u2019s Basic Attention Token, a cryptocurrency-based system for sharing ad revenue.\n\nGiven the long gap between Bluesky\u2019s announcement and this early step, we may not see Twitter\u2019s plans for a universal standard anytime soon, let alone see Twitter adopt that standard. As some decentralized developers mentioned in 2019, Twitter could also potentially do more good by contributing resources to existing projects \u2014 not trying to spearhead a unified standard. However, the new paper signals that Bluesky is trying to build on what people in the field have already discovered... even if we don\u2019t know where it might go from there.",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [
                "Paul Alcorn",
                "Math Geek"
            ],
            "title": "Intel Says it Fixed 7nm Process, But Will Still Outsource Some Parts",
            "contents": "Intel isn't giving up on making its own chips. The company reported its fourth quarter earnings today, with incoming CEO Pat Gelsinger and Omar Ishrak, Chairman of Intel's board, joining outgoing CEO Bob Swan on the call. Intel posted yet another strong quarter and record year, but the promised update on its 7nm process and outsourcing strategy, which will see the company contract out production of some of its leading-edge products for the first time in its history, hung thick in the air.\n\nIncoming Intel CEO Pat Gelsinger spoke on the call, saying that he has personally reviewed progress on the company's 7nm process over the last week and that he is pleased with the \"health and recovery of the 7nm program.\" Gelsinger also said that, given the breadth of the company's portfolio, Intel will expand its use of external foundries for some products. However, he is confident that the majority of Intel's 2023 CPU products will come from the company's own factories. The company plans to share more details after Gelsinger takes the helm in February.\n\nNotably, the 2023 timeline is still delayed relative to Intel's original timeline for its 7nm process, but is in line with the six-month delay the company outlined when it announced that its 7nm process was broken. Gelsinger noted that while his cursory 7nm investigation took place over the last week, it's based on data collected over the last six months as Intel investigated the yield issues.\n\nIntel CEO Bob Swan weighed in with more details, saying that the company's yield issues with the 7nm process stemmed from difficulties with a sequence of steps in the company's production process, which introduced defects. \"By rearchitecting these steps, we have been able to resolve the defects,\" Swan said. Swan also said that the company has simplified and streamlined the 7nm process to better ensure that the company can deliver on its 2023 roadmap, implying there may be some significant changes to 7nm's performance and/or design targets.\n\nIntel chips based on the 7nm process will debut in 2023, with client processors coming in the first half and server products following. That timeline still leaves Intel's competitors, like TSMC and Samsung, with a process node advantage in the 2023 time frame. TSMC projects it will be in full production of its 3nm node in 2023, explaining Intel's continued need to outsource some products. Intel plans to leverage its packaging technology and disaggregated design philosophy to integrate externally-produced chips into its own products.\n\nGelsinger also said the company remains committed to re-establishing its lead in process node technology, saying he's \"not interested in closing the gaps...but being the unquestioned leader in process technology.\"\n\nSwan also reiterated the company's commitment to preserving its IDM advantage, echoing his comments in our recent interview, meaning the company will still focus on producing its own chips in-house. Intel's CFO also said the company is increasing its spending on 7nm tooling for internal production.\n\nGelsinger noted that the company wouldn't share its detailed plans until after he takes the CEO position next month. Gelsinger commented that he will make key leadership changes and that other leaders will come back to the company, much like the news that Glenn Hinton has re-joined Intel.\n\nGelsinger also commented that \"we are committed to innovation and delivering the best products in every market we compete in.\" Gelsinger says his overhaul of the company will focus on four key areas: Leadership products, 'maniacal execution,' innovation, and restoring the \"transparent, data-driven culture\" cultivated by former Intel CEO Andy Grove.\n\nThe company is also delaying its full-year outlook until its next earnings call. Intel's press release also indicates that the company has quadrupled its '10nm supply unit growth', which has long been plagued by low yields, but didn't provide a frame of reference as to how much actual 10nm volume is in production.",
            "published_at": "2021-01-21T22:28:45+00:00"
        },
        {
            "authors": [],
            "title": "Fastmail status",
            "contents": "The issue accessing our web services is now resolved. No mail has been lost.",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [],
            "title": "edge-sql",
            "contents": "A serverless edge worker embedding SQLite using Cloudflare Workers and WASM.\n\nCheck out the source on GitHub and/or follow me on Twitter.\n\nThe data used in production is a reduced version (only EUR, JPY, GBP and CHF currencies) of the European Central Bank Forex Rates on Humdata. It is distributed under the license CC-BY.\n\nExample queries\n\nDays GBP was the highest and lowest\n\nSELECT *,1/EUR,1/JPY,1/GBP,1/CHF FROM forex WHERE GBP = ( SELECT max(GBP) FROM forex) OR GBP = ( SELECT min(GBP) FROM forex)\n\nUse SQLite built-in and extra functions\n\nSELECT getdata('country') AS country, random() AS rnd, date('now') AS now\n\nTry\n\nSELECT * FROM forex ORDER BY Date DESC LIMIT 10;\n\nQuery"
        },
        {
            "authors": [
                "Nilesh Christopher"
            ],
            "title": "WhatsApp is at risk of losing its most loyal customers: Indian uncles and aunties",
            "contents": "For the last two years, Mala Jadwani, a 31-year-old teacher, has received daily messages from her 60-year-old chachi. On their family WhatsApp group, her aunt, who started using a smartphone a few years ago, posts classic older-relative content: garden-variety motivational quotes, prayer emojis, home remedies for body aches, and \u201cgood morning,\u201d \u201cgood afternoon,\u201d and \u201cgood evening\u201d greetings accompanied by pictures of babies and roses. On days when her chachi\u2019s messages were particularly prolific, Jadwani\u2019s phone would freeze, forcing her to disable WhatsApp\u2019s automatic photo download setting.\n\nBut last week, her chachi\u2019s WhatsApp messages suddenly stopped.\n\nWorried, Jadwani called her aunt. It turned out that she had read a chain of viral messages on WhatsApp, which falsely said the platform\u2019s messaging system was no longer private. She was one of millions of users who received a push notification from WhatsApp earlier this month, explaining that the company was updating its privacy policy, mostly in superficial ways. After February 8, 2021, \u201cyou\u2019ll need to accept these changes to continue using WhatsApp,\u201d the update read. The thought of her WhatsApp messages being exposed frightened Jadwani\u2019s aunt so much, she said, that she stopped sending messages on WhatsApp altogether.\n\nIndia is WhatsApp\u2019s largest consumer market by many measures, but backlash against the recent privacy policy change is now threatening its future in the country. Many people have debated the update with their friends and family, and it\u2019s also been featured in nightly news shows and on the front page of local newspapers. Last week, Facebook, which owns WhatsApp, took out full-page advertisements and plugged interviews with the heads of WhatsApp in newspapers, in an effort to mitigate the fallout.\n\nThe change isn\u2019t actually a departure from how WhatsApp has treated user data for years. The platform has shared metadata \u2014 phone numbers, how often the app is opened, and operating system information \u2014 with Facebook since 2016. The pop-up notification millions of users received on January 8 sought explicit permission to continue these existing practices. But unlike four years ago, \u201cthere was no opt-out option to refuse sharing information with Facebook companies,\u201d said Apar Gupta, executive director of the nonprofit Internet Freedom Foundation (IFF).\n\nThe new changes are related to how businesses communicate with WhatsApp users. The update made it clearer that commercial activity on the messaging platform has the potential to spill over into other Facebook apps: \u201ca handbag you browse on WhatsApp could pop up later in your Instagram app,\u201d as The New York Times explained. Over 175 million people already message companies on WhatsApp, and the updated terms of service would make it \u201ceasier to chat with businesses who may use Facebook business products,\u201d reads a blog post from WhatsApp\u2019s communications team. The post claims that the update would enable customer service features like purchase receipts.\n\nThe confusing change triggered a mass exodus of WhatsApp users, who moved to other messaging platforms like Signal and Telegram. After the disastrous rollout, Facebook later announced it was delaying updating its privacy policy by three months, but for loyal WhatsApp users, the damage has already been done.\n\nWhatsApp began running promotional ads to explain its privacy updates and prevent further fallout. WhatsApp\n\nFacebook\u2019s privacy practices and policy decisions in India have long been criticized. What\u2019s different this time are the demographics of the people who are outraged: uncles and aunties. For the first time, India\u2019s most loyal and engaged users are questioning the intentions of an app that has grown to encompass nearly every aspect of their daily lives.\n\nWhatsApp groups are an indispensable part of South Asian families: they\u2019re a forum for gossip, dodgy forwarded messages, boisterous political debates, and generally the primary way for relatives to stay in touch. Many of these WhatsApp family groups \u2014 including my own \u2014 erupted with anger and confusion in the days after the platform sent the pop-up message announcing its new policy.\n\nSheshadri Narayanane, a friend from high school, is a part of two family groups. He said his 70-year-old aunt falsely believed WhatsApp had been granted access to her bank accounts. WhatsApp recently launched payments in India, allowing users to send and receive money by connecting their bank accounts to the messaging app. The privacy update came shortly after WhatsApp Pay was launched, which might have caused some of the confusion. \u201cBad timing is my theory,\u201d Narayanane speculated.\n\nOn prime-time talk shows, news hosts likened the update to digital dadagiri, or \u201cdigital bullying,\u201d by WhatsApp. \u201cThey want to sell your chats \u2014 your data from WhatsApp private chats \u2014 to business enterprises,\u201d declared one news anchor.\n\nThe debates often took on a political tone. Some TV hosts asked questions about the government\u2019s ability to protect India\u2019s sovereignty. Gaurav Bhatia, the national spokesperson of the ruling Bharatiya Janata Party (BJP) weighed in, saying his party would go to any lengths necessary to protect the privacy of its citizens. On January 19, the Ministry of Electronics and Communication Technology (MeitY) sent a request to Facebook to withdraw the new changes made to its privacy policy.\n\n\u201cWhy is it that these social media giants, who need the India market, are having two different policies for different countries?\u201d Bhatia said on a nightly news show, referencing the fact that WhatsApp\u2019s policy update does not apply to the European Union. Some accused WhatsApp of treating Indians as \u201csecond-rate citizens.\u201d Local technology entrepreneurs who compete with Big Tech companies like Facebook soon piled on.\n\nFacebook has repeatedly emphasized that WhatsApp can\u2019t see the content of end-to-end encrypted messages, which can be viewed by only the sender and receiver. Alternatives like Telegram don\u2019t necessarily offer the same protections as WhatsApp. Digital liberties activists and advocacy groups like IFF recommend using Signal instead for sensitive communications, and the app quickly became the number one downloaded app in India earlier this month. Signal commemorated its victory on Twitter. \u201cLook at what you\u2019ve done,\u201d read a tweet from the app\u2019s official account, accompanied by an emoji of the Indian flag.\n\nThough users are downloading Signal and other encrypted alternatives in high numbers, switching might not be as simple for aunties and uncles who spent years becoming proficient in using WhatsApp. Vivek K, a 28-year-old tech entrepreneur, tried making the case to his mother that she should switch over to Signal and was met with an unenthusiastic response: \u201cShe said, \u2018Now, you\u2019re making me learn one more app? I can\u2019t do it; I\u2019m too old for this,\u2019\u201d explained Vivek.\n\nFor now, with the privacy update postponed until at least May, WhatsApp is working to mitigate the backlash. But people like Jadwani\u2019s chachi are still staying away from the messaging app. Jadwani said she hasn\u2019t missed the lack of daily greetings. \u201cNo wonder my phone hasn\u2019t had issues,\u201d she said with a laugh.",
            "published_at": "2021-01-20T06:30:00+00:00"
        },
        {
            "authors": [],
            "title": "The Lakota: A Human Story",
            "contents": "The Lakota: A Human Story\n\nLakota America: A New History of Indigenous Power\n\nby Pekka H\u00e4m\u00e4l\u00e4inen.\n\nYale University Press, 2019.\n\nHardcover, 544 pages, $35.\n\nRevivewed by Santi Ruiz\n\nOn this year\u2019s Indigenous People\u2019s Day I encountered a curious phenomenon. My social circles are largely college-educated, left-leaning Gen Zers and Millennials, whom I assumed would be most likely to celebrate indigenous people as an underrepresented and integral part of the American fabric. Opening Twitter and Instagram, however, the zone was flooded with variations on the pastel infographic aesthetic that showcases one\u2019s excellent politics. One wag\u2019s tweet captured the dynamic: \u201chope everyone had a good time partaking in that beloved Indigenous People\u2019s Day tradition: posting online about Indigenous People\u2019s Day to own your dad.\u201d\n\nThis state of affairs highlights the declining role of Native Americans in what Ross Douthat has called the American imaginarium. For most of the nation\u2019s history, Native Americans figured largely in our self-conception: as the Pilgrims\u2019 saviors, as retrograde savages, as Gaian mystics, opponents to progress, consummate trackers, and the last obstacle to the white man\u2019s dominion over the New World. In the late 1800s, \u201cWild West Show Indians\u201d became one of the country\u2019s premier tourist attractions, by turns mawkish and genuinely sentimental.\n\nIt\u2019s understandable why these forms of cultural representation, like the Cowboys and Indians television genre or the crude caricatures that once marked our sports teams, died out in an era more concerned with racial reconciliation. But it\u2019s hard to point to more humane representations that have taken their place. What remains is generalized ignorance, punctuated by occasional invocations of Native Americans in a litany of oppressed peoples. Our culture seems satisfied by avoiding obvious offensiveness.\n\nJumping into this cultural void is Pekka H\u00e4m\u00e4l\u00e4inen\u2019s 2019 opus Lakota America, which presents the sweeping, unvarnished history of the Lakota tribe (often referred to as Sioux) from 1695 to today. H\u00e4m\u00e4l\u00e4inen tells the deeply American story of a people at once oppressors and oppressed, tricksters who are themselves deceived, who over three hundred years traverse almost the entire continent.\n\nThroughout the narrative, the Lakota remain stubbornly illegible to the white settlers they encounter. H\u00e4m\u00e4l\u00e4inen does a sterling job of setting the stage by highlighting the gulf in societal and family life. The seven tribes that made up the Lakota had no centralized governance structure, and more than once the Americans suspect they have resolved tensions by cutting a deal with a regional leader who has no authority over other tribes. Society was tied together into \u201cthiospaye,\u201d groups of around twenty households, with each household bound through dense kinship ties to families within and without the thiospaye. These decentralized networks are systematically dismantled by the enforced move to reservations.\n\nThe Lakota begin in New England, but disease, settlers, war, and famine continually force them west. Crossing through the Midwest in the winter of 1703, the Lakota traverse a frozen lake, and discover that a tremendous herd of bison has been trapped beneath the ice. The natural refrigerator feeds them all winter. The book contains multiple moments of this kind of shocking, strange beauty, as when in a major conference women drop pieces of buffalo fat into their tipi fires at night, turning lodges into gigantic lanterns in a \u201csea of sparkling light.\u201d\n\nBut the book\u2019s focus is on the grand historical narrative, as the Lakota continually adapt to adverse circumstances. Over the course of the eighteenth century, the Lakota become a horse people. It takes generations, a huge expenditure of wealth, and day-long rides clinging onto wild and bucking horses until they break, but they do it, earlier than other tribes. It\u2019s a technological cheat code. The plant biomass of the Great Plains may have been a thousand times greater than that of its animals: the horse unlocks all that energy and converts it to muscle power. H\u00e4m\u00e4l\u00e4inen brilliantly documents the transition from a largely hunter-gatherer-farmer society, settled in Mississippi River valleys, into a fully nomadic one. It\u2019s a staggering success while it lasts; by the late 1860s, the average family owns upwards of twenty horses. The empire these nomads build is rapacious; more than once, the Lakota are forced to move westward because they have looted the neighboring tribes out of existence and are therefore out of farmed food. Over several more generations, the Lakota shift fitfully west, until a series of visions reveals the Black Hills as their final home.\n\nH\u00e4m\u00e4l\u00e4inen treats with as much detail as he can their brutal war with the Crow tribes for control of the hills, though the historical record is largely confined to the extensive buffalo hide pictographs made each winter by tribal elders. Despite his attempts to demystify Lakota society, it is these intra-Indian interactions that contain the most mystery. We know that in 1835, Lakota chief Lame Deer shot a Crow warrior twice with the same arrow. We have little idea why.\n\nOne theory holds that Lame Deer is \u201ccounting coup\u201d\u2014a form of ritual point-scoring in battle that holds massive ceremonial and cultural weight among Lakota and Crow alike. For all its qualities, the book never quite unpacks this central societal behavior, describing it more as a game for the wealthy than as a driving feature of Lakota life. For those interested in this side of Great Plains culture, Jonathan Lear\u2019s Radical Hope: Ethics in the Face of Cultural Devastation is a superb account of the practice among the Crow.\n\nH\u00e4m\u00e4l\u00e4inen spends ample time on the diseases that tore apart Native societies, and the sheer numbers are a useful reminder. Eighty percent of New England Indians died in a 1633 smallpox outbreak. Leading Massachusetts colonist John Winthrop sees it as divine intervention: \u201cGods hand hath so pursued them, as for three hundred miles space, the greatest parte of them are swept awaye by the small poxe, which still continues among them: So as God hathe hereby cleered our title to this place.\u201d The various poxes overturn intra-native relations: at various points on their journey, the Lakota narrowly dodge or are less brutally affected by diseases that ravage the tribes already present in the land, making conquest that much easier.\n\nBut it\u2019s the slower, more nuanced encounters between Lakota and settlers that get the most attention, and to which H\u00e4m\u00e4l\u00e4inen brings the most clarity. For instance, the buffalo hide trade brings massive wealth to the Lakota. But as it scales, gender dynamics quickly spin out of control. As killing buffalo (a man\u2019s job) takes much less time than skinning and tanning (women\u2019s work), rates of polygamy skyrocket, with successful men taking on multiple wives, essentially as at-home laborers. The resultant spike in young, single men with little hope of finding a wife leads to macho posturing, widespread drinking, and violence.\n\nIn similar fashion, the advances of the steam train and of trading posts at first cement the Lakota position as rulers of the Plains. As the frontier pushes west, Lakota society becomes completely riven over how to deal with white intrusion, with the economic ties to the posts limiting the possible responses.\n\nWell into the last years of the nineteenth century, the Lakota continue to mount spirited resistance to federal control. Little Big Horn and Wounded Knee both get ample attention, with H\u00e4m\u00e4l\u00e4inen not shy to describe the brutality of warfighting on both sides. Wounded Knee in 1890 marks an end to the Lakota empire: many winter counts end for good there, and those that remain are a grim catalogue of suicides, domestic violence, and the herding of the tribes onto reservation land. Before the great leader\u2019s death, Red Cloud puts it poignantly: \u201cThink of it! I, who used to own rich soil in a well-watered country so extensive that I could not ride through it in a week on my fastest pony, am put down here \u2026 Now I, who used to control five thousand warriors, must tell Washington when I am hungry. I must beg for that which I own.\u201d\n\nToday, it\u2019s hard to find remnants of the empire that once ruled the Mississippi, then the Great Plains, then the American Northwest. But it is as integral and vibrant a part of American history as any. If we Americans tend to confront our relationship with Native Americans by either whitewashing our story, or by recognizing them chiefly as an oppressed contingent of society, books like Lakota America offer us a third way: simply telling their particular, human story, one that is just as important as the story of European settlement and pillaging. If anything, reading Native history helps us engage with contemporary political issues more honestly: how, for example, the Indian Bureau\u2019s bags of flour and sugar helped create the modern diabetes wave on reservations. Books like Lakota America help us begin to see our shared America, in all its guts and glory.\n\nSanti Ruiz is a writer living in Washington DC. You can reach him on Twitter at @rSanti97.",
            "published_at": "2021-01-17T10:15:34+00:00"
        },
        {
            "authors": [],
            "title": "Ask HN: How to find the energy for side projects after day job?",
            "contents": "I started doing a remote job and as I don't have to commute anymore, I have some time left after work hours. I'm a software developer, and most of the days are fully packed. Considering the few hours I'm getting, I am thinking about being productive on side projects after my day job, but most of the days I feel too drained to do anything. People who are successfully at being highly productive in side hustles after demanding jobs, how do you do this? Any actionable advice?"
        },
        {
            "authors": [],
            "title": "",
            "contents": "JavaScript is not available.\n\nWe\u2019ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.\n\nHelp Center"
        },
        {
            "authors": [],
            "title": "Launch HN: Manara (YC W21) \u2013 Connect Middle East engineers with global companies",
            "contents": "Hey everyone! My name is Laila and with my co-founder Iliana I\u2019m building Manara (https://www.manara.tech/). We support software engineers in the Middle East and North Africa (MENA) region to get great jobs at tech companies worldwide. These companies appreciate being connected to skilled talent that is diverse and inclusive (50% of our engineers are women). I grew up in a refugee camp in Gaza. My dream was to become a Silicon Valley software engineer. Eventually I hacked my way there successfully, becoming a software engineer at Nvidia. I like to joke that the hardest part wasn\u2019t escaping Gaza in the middle of the 2014 war, but rather, my first interviews... which I totally bombed. ;) Once I got to Silicon Valley, I was surprised at the lack of women. In Gaza, more women study computer science than men! I was also surprised to learn how hard it was for companies in Silicon Valley to attract the talent we needed. During interviews with candidates I\u2019d often think, \u201cI wish I could hire my friends in Gaza. They\u2019d be great.\u201d That\u2019s when I re-connected with Iliana. She and I had met in Gaza when she was running Gaza Sky Geeks (GSG), the first startup accelerator in Gaza. Her work was widely covered and has a few threads on HN including https://news.ycombinator.com/item?id=11858963. Iliana asked me \u201cHow can we produce more success stories like yours?\u201d I told her that engineers in the MENA region don't lack talent, but they lack other ingredients. They're mostly not aware of opportunities outside their region, and even if they are aware, they think you have to be a genius to work at a company like Google. Also, they have no idea what sorts of resumes recruiters want to see and don't have brand names to put on them. They don't have referral networks to get their foot in the door. And they're completely unprepared for the style of interviews that tech companies go for. As we talked further, it became clear that all of these problems would be fixable with the right kind of coaching and support, and that bringing this growing talent pool to the global job marketplace would benefit both sides (accelerating the success of global companies, while redistributing wealth to the region). We developed an approach to address those gaps - and it worked. Just last week, 67% of the people we referred to Google for internships made it past the hiring committee (they\u2019re now waiting for their job offers, so if you work at Google and have internship headcount, let us know!) We\u2019ve heard Google interviewers say several times, \u201cThis is the best junior engineering interview I\u2019ve ever done.\u201d I want to emphasize that we are not a zero-to-hero bootcamp. Manara is a career accelerator for skilled software engineers at all levels with a focus on junior engineers. Students learn the technical and soft skills they need to pass interviews and get introductions to companies with jobs that are either remote or on-site (in Europe or Canada). We charge an affordable fee to both candidates and companies, only if a successful match is made. We focus on MENA (and specifically Arabic-speaking countries in the region) for a few reasons. On the business side, that's where we're from and where our connections are, so we understand the dynamics and have comparative advantage there. Second, the region has a huge opportunity: the youngest population in the world, 2x more university graduates than 10 years ago, women studying computer science at high rates (in some countries more women study CS than men: 52% in Palestine, 62% in Tunisia, 70% in Qatar), and so on. Third, it lends itself to scale. Our graduates have a high sense of affiliation and loyalty to the region, which means that as soon as we place 1 candidate at a company that\u2019s growing, s/he comes back to us looking for 3 more to hire. But we\u2019re not building Manara just for business reasons; rather, we were motivated to launch Manara for social impact reasons. The unemployment rate for recent college grads is ~60%; for women who studied CS, it can be as high as 83%. It pains us personally to see highly talented friends of ours struggling to find (meaningful) work. We originally planned to build Manara as a non-profit, but after lots of research, we realized that a social enterprise approach would better support our mission: the pressure of becoming self-sustainable forces sharper thinking and execution, and will make it possible for us to deliver this solution at scale. A powerful part of our impact is the community we are building. Students study in cohorts. Within each cohort, they compete to see who can solve more coding problems, and form strong bonds and support each other. Students also meet volunteers from tech companies like Google, Amazon, Facebook, Wayfair, Stripe, etc for mentorship and mock interviews once they achieve certain milestones (e.g., 100 questions on Leetcode). This leads to high motivation and retention. It also gives them access to professional networks like those Americans have when graduating from universities like Stanford. Often these networks later help them with their job hunting: just last week, a candidate got an interview at Uber thanks to a referral from one of our volunteers who works there. Our volunteers love the chance to use their professional skills to mentor engineers from untraditional backgrounds. Several told us that they spent years looking for an effective way to contribute. One recently wrote to us, \u201cI'm in awe of the work Manara is doing. I love interacting with my mentee and providing mock interviews - so thank you for giving me a platform to be able to support these students.\u201d If you're hiring, check out https://www.manara.tech/hire-engineers. If you'd like to get involved or join our newsletter, check out https://www.manara.tech/get-involved. Most importantly, we can't wait to hear what you think, wherever in the world you might be. Over to you, HN!"
        },
        {
            "authors": [
                "Elizabeth Kolbert",
                "Elizabeth Kolber"
            ],
            "title": "Have We Already Been Visited by Aliens?",
            "contents": "On October 19, 2017, a Canadian astronomer named Robert Weryk was reviewing images captured by a telescope known as Pan-STARRS1 when he noticed something strange. The telescope is situated atop Haleakal\u0101, a ten-thousand-foot volcanic peak on the island of Maui, and it scans the sky each night, recording the results with the world\u2019s highest-definition camera. It\u2019s designed to hunt for \u201cnear-Earth objects,\u201d which are mostly asteroids whose paths bring them into our planet\u2019s astronomical neighborhood and which travel at an average velocity of some forty thousand miles an hour. The dot of light that caught Weryk\u2019s attention was moving more than four times that speed, at almost two hundred thousand miles per hour.\n\nWeryk alerted colleagues, who began tracking the dot from other observatories. The more they looked, the more puzzling its behavior seemed. The object was small, with an area roughly that of a city block. As it tumbled through space, its brightness varied so much\u2014by a factor of ten\u2014that it had to have a very odd shape. Either it was long and skinny, like a cosmic cigar, or flat and round, like a celestial pizza. Instead of swinging around the sun on an elliptical path, it was zipping away more or less in a straight line. The bright dot, astronomers concluded, was something never before seen. It was an \u201cinterstellar object\u201d\u2014a visitor from far beyond the solar system that was just passing through. In the dry nomenclature of the International Astronomical Union, it became known as 1I/2017 U1. More evocatively, it was dubbed \u2018Oumuamua (pronounced \u201coh-mooah-mooah\u201d), from the Hawaiian, meaning, roughly, \u201cscout.\u201d\n\nEven interstellar objects have to obey the law of gravity, but \u2018Oumuamua raced along as if propelled by an extra force. Comets get an added kick thanks to the gases they throw off, which form their signature tails. \u2018Oumuamua, though, didn\u2019t have a tail. Nor did the telescopes trained on it find evidence of any of the by-products normally associated with outgassing, like water vapor or dust.\n\n\u201cThis is definitely an unusual object,\u201d a video produced by NASA observed. \u201cAnd, unfortunately, no more new observations of \u2018Oumuamua are possible because it\u2019s already too dim and far away.\u201d\n\nAs astronomers pored over the data, they excluded one theory after another. \u2018Oumuamua\u2019s weird motion couldn\u2019t be accounted for by a collision with another object, or by interactions with the solar wind, or by a phenomenon that\u2019s known, after a nineteenth-century Polish engineer, as the Yarkovsky effect. One group of researchers decided that the best explanation was that 1I/2017 U1 was a \u201cminiature comet\u201d whose tail had gone undetected because of its \u201cunusual chemical composition.\u201d Another group argued that \u2018Oumuamua was composed mostly of frozen hydrogen. This hypothesis\u2014a variation on the mini-comet idea\u2014had the advantage of explaining the object\u2019s peculiar shape. By the time it reached our solar system, it had mostly melted away, like an ice cube on the sidewalk.\n\nBy far the most spectacular account of 1I/2017 U1 came from Avi Loeb, a Harvard astrophysicist. \u2018Oumuamua didn\u2019t behave as an interstellar object would be expected to, Loeb argued, because it wasn\u2019t one. It was the handiwork of an alien civilization.\n\nIn an equation-dense paper that appeared in The Astrophysical Journal Letters a year after Weryk\u2019s discovery, Loeb and a Harvard postdoc named Shmuel Bialy proposed that \u2018Oumuamua\u2019s \u201cnon-gravitational acceleration\u201d was most economically explained by assuming that the object was manufactured. It might be the alien equivalent of an abandoned car, \u201cfloating in interstellar space\u201d as \u201cdebris.\u201d Or it might be \u201ca fully operational probe\u201d that had been dispatched to our solar system to reconnoitre. The second possibility, Loeb and Bialy suggested, was the more likely, since if the object was just a piece of alien junk, drifting through the galaxy, the odds of our having come across it would be absurdly low. \u201cIn contemplating the possibility of an artificial origin, we should keep in mind what Sherlock Holmes said: \u2018when you have excluded the impossible, whatever remains, however improbable, must be the truth,\u2019 \u201d Loeb wrote in a blog post for Scientific American.\n\nNot surprisingly, Loeb and Bialy\u2019s theory received a lot of attention. The story raced around the world almost at the speed of \u2018Oumuamua. TV crews crowded into Loeb\u2019s office, at the Harvard-Smithsonian Center for Astrophysics, and showed up at his house. Film companies vied to make a movie of his life. Also not surprisingly, much of the attention was unflattering.\n\n\u201cNo, \u2018Oumuamua is not an alien spaceship, and the authors of the paper insult honest scientific inquiry to even suggest it,\u201d Paul M. Sutter, an astrophysicist at Ohio State University, wrote.\n\n\u201cCan we talk about how annoying it is that Avi Loeb promotes speculative theories about alien origins of \u2018Oumuamua, forcing [the] rest of us to do the scientific gruntwork of walking back these rumors?\u201d Benjamin Weiner, an astronomer at the University of Arizona, tweeted.\n\nFar from being deterred, Loeb doubled down. Together with Thiem Hoang, a researcher at the Korea Astronomy and Space Science Institute, he blasted the frozen-hydrogen theory. In another equation-packed paper, the pair argued that it was fantastical to imagine solid hydrogen floating around outer space. And, if a frozen chunk did manage to take shape, there was no way for a block the size of \u2018Oumuamua to survive an interstellar journey. \u201cAssuming that H 2 objects could somehow form,\u201d Hoang and Loeb wrote, \u201csublimation by collisional heating\u201d would vaporize them before they had the chance to, in a manner of speaking, take off.\n\nLoeb has now dispensed with the scientific notation and written \u201cExtraterrestrial: The First Sign of Intelligent Life Beyond Earth\u201d (Houghton Mifflin Harcourt). In it, he recounts the oft-told story of how Galileo was charged with heresy for asserting that Earth circled the sun. At his trial in Rome, in 1633, Galileo recanted and then, legend has it, muttered, sotto voce, \u201cEppur si muove\u201d (\u201cAnd yet it moves\u201d). Loeb acknowledges that the quote is probably apocryphal; still, he maintains, it\u2019s relevant. The astronomical establishment may wish to silence him, but it can\u2019t explain why \u2018Oumuamua strayed from the expected path. \u201cAnd yet it deviated,\u201d he observes.\n\nIn \u201cExtraterrestrial,\u201d Loeb lays out his reasoning as follows. The only way to make sense of \u2018Oumuamua\u2019s strange acceleration, without resorting to some sort of undetectable outgassing, is to assume that the object was propelled by solar radiation\u2014essentially, photons bouncing off its surface. And the only way the object could be propelled by solar radiation is if it were extremely thin\u2014no thicker than a millimetre\u2014with a very low density and a comparatively large surface area. Such an object would function as a sail\u2014one powered by light, rather than by wind. The natural world doesn\u2019t produce sails; people do. Thus, Loeb writes, \u201c \u2018Oumuamua must have been designed, built, and launched by an extraterrestrial intelligence.\u201d\n\nThe first planet to be found circling a sunlike star was spotted in 1995 by a pair of Swiss astronomers, Michel Mayor and Didier Queloz. Its host star, 51 Pegasi, was in the constellation Pegasus, and so the planet was formally dubbed 51 Pegasi b. By a different naming convention, it became known as Dimidium.\n\nDimidium was the \u2018Oumuamua of its day\u2014a fantastic discovery that made headlines around the world. (For their work, Mayor and Queloz were eventually awarded a Nobel Prize.) The planet turned out to be very large, with a mass about a hundred and fifty times that of Earth. It was whipping around its star once every four days, which meant that it had to be relatively close to it and was probably very hot, with a surface temperature of as much as eighteen hundred degrees. Astronomers hadn\u2019t thought such a large body could be found so close to its parent star and had to invent a whole new category to contain it; it became known as a \u201chot Jupiter.\u201d\n\nMayor and Queloz had detected Dimidium by measuring its gravitational tug on 51 Pegasi. In 2009, NASA launched the Kepler space telescope, which was designed to search for exoplanets using a different method. When a planet passes in front of its star, it reduces the star\u2019s brightness very slightly. (During the last transit of Venus, in 2012, viewers on Earth could watch a small black dot creep across the sun.) Kepler measured variations in the brightness of more than a hundred and fifty thousand stars in the vicinity of the constellations Cygnus and Lyra. By 2015, it had revealed the existence of a thousand exoplanets. By the time it stopped operating, in 2018, it had revealed sixteen hundred more.\n\nNASA\u2019s ultimate goal for the telescope was to work out a figure known as eta-Earth, or \u03b7\u2295. This is the average number of rocky, roughly Earth-size planets that can be found orbiting an average sunlike star at a distance that might, conceivably, render them habitable. After spending two years analyzing the data from Kepler, researchers recently concluded that \u03b7\u2295 has a value somewhere between .37 and .6. Since there are at least four billion sunlike stars in the Milky Way, this means that somewhere between 1.5 billion and 2.4 billion planets in our galaxy could, in theory, harbor life. No one knows what fraction of potentially habitable planets are, in fact, inhabited, but, even if the proportion is trivial, we\u2019re still talking about millions\u2014perhaps tens of millions\u2014of planets in the galaxy that might be teeming with living things. At a public event a few years ago, Ellen Stofan, who at the time was NASA\u2019s chief scientist and is now the director of the National Air and Space Museum, said that she believed \u201cdefinitive evidence\u201d of \u201clife beyond earth\u201d would be found sometime in the next two decades.\n\n\u201cIt\u2019s definitely not an \u2018if,\u2019 it\u2019s a \u2018when,\u2019 \u201d Jeffrey Newmark, a NASA astrophysicist, said at the same gathering.",
            "published_at": "2021-01-25T00:00:00"
        },
        {
            "authors": [],
            "title": "New Year, new Red Hat Enterprise Linux programs: Easier ways to access RHEL",
            "contents": "This post highlights new, simplified and low-/no-cost options for deploying RHEL. These are the first of many new programs. To immediately go to the program that interests you:\n\nOn December 8, 2020, Red Hat announced a major change to the enterprise Linux ecosystem: Red Hat will begin shifting our work from CentOS Linux to CentOS Stream on December 31, 2021. We and the CentOS Project governing board believe that CentOS Stream represents the best way to further drive Linux innovation. It will give everyone in the broader ecosystem community, including open source developers, hardware and software creators, individual contributors, and systems administrators, a closer connection to the development of the world\u2019s leading enterprise Linux platform.\n\nWhen we announced our intent to transition to CentOS Stream, we did so with a plan to create new programs to address use cases traditionally served by CentOS Linux. Since then, we have gathered feedback from the broad, diverse, and vocal CentOS Linux user base and the CentOS Project community. Some had specific technical questions about deployment needs and components, while others wondered what their options were for already- or soon-to-be deployed systems. We\u2019ve been listening. We know that CentOS Linux was fulfilling a wide variety of important roles.\n\nWe made this change because we felt that the Linux development models of the past 10+ years needed to keep pace with the evolving IT world. We recognize the disruption that this has caused for some of you. Making hard choices for the future isn\u2019t new to Red Hat. The introduction of Red Hat Enterprise Linux and the deprecation of Red Hat Linux two decades ago caused similar reactions. Just as in the past, we\u2019re committed to making the RHEL ecosystem work for as broad a community as we can, whether it\u2019s individuals or organizations seeking to run a stable Linux backend; community projects maintaining large CI/Build systems; open source developers looking toward \"what\u2019s next;\" educational institutions, hardware, and software vendors looking to bundle solutions; or enterprises needing a rock-solid production platform.\n\nToday we\u2019re sharing details about some of the new no- and low-cost programs we\u2019re adding to RHEL. These are the first of many new programs.\n\nNo-cost RHEL for small production workloads\n\nWhile CentOS Linux provided a no-cost Linux distribution, no-cost RHEL also exists today through the Red Hat Developer program. The program\u2019s terms formerly limited its use to single-machine developers. We recognized this was a challenging limitation.\n\nWe\u2019re addressing this by expanding the terms of the Red Hat Developer program so that the Individual Developer subscription for RHEL can be used in production for up to 16 systems. That\u2019s exactly what it sounds like: for small production use cases, this is no-cost, self-supported RHEL. You need only to sign in with a free Red Hat account (or via single sign-on through GitHub, Twitter, Facebook, and other accounts) to download RHEL and receive updates. Nothing else is required. This isn\u2019t a sales program and no sales representative will follow up. An option will exist within the subscription to easily upgrade to full support, but that\u2019s up to you.\n\nYou can also use the expanded Red Hat Developer program to run RHEL on major public clouds including AWS, Google Cloud Platform, and Microsoft Azure. You have to pay only the usual hosting fees charged by your provider of choice; the operating system is free for both development and small production workloads.\n\nThe updated Individual Developer subscription for RHEL will be available no later than February 1, 2021.\n\nNo-cost RHEL for customer development teams\n\nWe recognized a challenge of the developer program was limiting it to an individual developer. We\u2019re now expanding the Red Hat Developer program to make it easier for a customer\u2019s development teams to join the program and take advantage of its benefits. These development teams can now be added to this program at no additional cost via the customer\u2019s existing subscription, helping to make RHEL more accessible as a development platform for the entire organization. Through this program, RHEL can also be deployed via Red Hat Cloud Access and is accessible on major public clouds including AWS, Google Cloud Platform and Microsoft Azure at no additional costs except for the usual hosting fees charged by your cloud provider of choice.\n\nBringing RHEL to additional use cases\n\nWe know that these programs don\u2019t address every CentOS Linux use case, so we aren\u2019t done delivering more ways to get RHEL easily. We\u2019re working on a variety of additional programs for other use cases, and plan to provide another update in mid-February.\n\nWe want to make RHEL easier to use and are removing many barriers that stand in the way, working to keep pace with the evolving needs of Linux users, our customers and our partners. This requires us to continuously examine our development and business models to meet these changing needs. We believe that these new programs -- and those to follow -- work toward that goal.\n\nWe\u2019re making CentOS Stream the collaboration hub for RHEL, with the landscape looking like this:\n\nFedora Linux is the place for major new operating system innovations, thoughts, and ideas - essentially, this is where the next major version of Red Hat Enterprise Linux is born.\n\nCentOS Stream is the continuously delivered platform that becomes the next minor version of RHEL.\n\nRHEL is the intelligent operating system for production workloads, used in nearly every industry in the world, from cloud-scale deployments in mission-critical data centers and localized server rooms to public clouds and out to far-flung edges of enterprise networks.\n\nWe aren\u2019t done with this work. We want to hear from you, whether or not your needs fall into one of the use cases described here."
        },
        {
            "authors": [],
            "title": "Caitlin Green: Another eleventh-century medieval Chinese coin found in England",
            "contents": "This is the blog of Dr Caitlin Green FSA. It features posts on my main academic research foci alongside other topics that I'm currently working on, including drafts of papers, ideas and similar\u2014these are usually identifiable by the presence of footnotes. You're free to cite these drafts if they are of interest, and are reminded that academic blogs are indeed citable under most citation systems. In addition, the current site also houses posts relating to my personal interests, including long-distance trade, migration and contacts; landscape and coastal history; early literature and legends; and the history, archaeology, place-names and legends of Lincolnshire and Cornwall. For further details of this website & how to contact me, please see the ' About ' page or @caitlinrgreen on Twitter."
        },
        {
            "authors": [],
            "title": "Ultraleap",
            "contents": "Where the focal point is positioned in 3D space is programmable in real time. It can change position from instant to instant.\n\nWe use a hand tracking device (usually a Leap Motion Controller) to track the exact position of your hand and position the focal point at a spot on it."
        },
        {
            "authors": [],
            "title": "Statement from the Press Secretary Regarding Executive Grants of Clemency",
            "contents": "President Donald J. Trump granted pardons to 73 individuals and commuted the sentences of an additional 70 individuals.\n\nTodd Boulanger \u2013 President Trump granted a full pardon to Todd Boulanger. Mr. Boulanger\u2019s pardon is supported by numerous friends, as well as by past and present business associates. In 2008, Mr. Boulanger pled guilty to one count of conspiracy to commit honest services fraud. He has taken full responsibility for his conduct. Mr. Boulanger is a veteran of the United States Army Reserves and was honorably discharged. He has also received an award from the City of the District of Columbia for heroism for stopping and apprehending an individual who assaulted an elderly woman with a deadly weapon on Capitol Hill. Mr. Boulanger is known as a model member of his community. In addition, he is remorseful for his actions and would like to leave his mistakes behind him.\n\nAbel Holtz \u2013 President Trump granted a full pardon to Abel Holtz. This pardon is supported by Representative Mario Diaz-Balart and friends and business colleagues in his community. Mr. Holtz is currently 86 years old. In 1995, he pled guilty to one count of impeding a grand jury investigation and was sentenced to 45 days in prison. Before his conviction, Mr. Holtz, who was the Chairman of a local bank, never had any legal issues and has had no other legal issues since his conviction. Mr. Holtz has devoted extensive time and resources to supporting charitable causes in South Florida, including substantial donations to the City of Miami Beach.\n\nRepresentative Rick Renzi \u2013 President Trump granted a full pardon to Representative Rick Renzi of Arizona. Mr. Renzi\u2019s pardon is supported by Representative Paul Gosar, Representative Tom Cole, former Representative Tom DeLay, former Representative Jack Kingston, former Representative Todd Tiahrt, former Representative John Doolittle, former Representative Duncan Hunter Sr., former Representative Richard Pombo, former Representative Charles Taylor, former Representative Dan Burton, Larry Weitzner, National Institute of Family and Life Advocates, and numerous other members of his community. In 2013, Mr. Renzi was convicted of extortion, bribery, insurance fraud, money laundering, and racketeering. He was sentenced to 2 years in Federal prison, 2 years of supervised release, and paid a $25,000 fine. Before his conviction, Mr. Renzi served three terms in the House of Representatives. His constituents considered him a strong advocate for better housing, quality education, and improved healthcare\u2014especially for the underprivileged and Native Americans. He is the father of 12 children and a loving and devoted husband.\n\nKenneth Kurson \u2013 President Trump granted a full pardon to Kenneth Kurson. Prosecutors have charged Mr. Kurson with cyberstalking related to his divorce from his ex-wife in 2015. In a powerful letter to the prosecutors, Mr. Kurson\u2019s ex-wife wrote on his behalf that she never wanted this investigation or arrest and, \u201crepeatedly asked for the FBI to drop it\u2026 I hired a lawyer to protect me from being forced into yet another round of questioning. My disgust with this arrest and the subsequent articles is bottomless\u2026\u201d This investigation only began because Mr. Kurson was nominated to a role within the Trump Administration. He has been a community leader in New York and New Jersey for decades. In addition, Mr. Kurson is a certified foster parent, a successful business owner, and is passionate about various charitable causes. Mr. Kurson is an upstanding citizen and father to five beautiful children.\n\nCasey Urlacher \u2013 President Trump granted a full pardon to Casey Urlacher. This pardon is supported by his friends and family, and countless members of his community. Mr. Urlacher has been charged with conspiracy to engage in illegal gambling. Throughout his life, Mr. Urlacher has been committed to public service and has consistently given back to his community. Currently, Mr. Urlacher serves as the unpaid Mayor of Mettawa, Illinois. He is a devoted husband to his wife and a loving father to his 17-month old daughter.\n\nCarl Andrews Boggs \u2013 President Trump granted a full pardon to Carl Andrews Boggs. This pardon is supported by the Honorable David Lee and South Carolina Department of Transportation Chairman Tony Cox. In 2013, Mr. Boggs pled guilty to two counts of conspiracy. Since his release, Mr. Boggs has rebuilt his company, has employed hundreds of people, and has dedicated countless hours and financial resources to his community.\n\nJaime A. Davidson \u2013 President Trump commuted the sentence of Jaime A. Davidson. This commutation is supported by Mr. Davidson\u2019s family and friends, Alice Johnson, and numerous others. In 1993, Mr. Davidson was convicted and sentenced to life imprisonment in relation to the murder of an undercover officer. Notably, witnesses who testified against Mr. Davidson later recanted their testimony in sworn affidavits and further attested that Mr. Davidson had no involvement. Although Mr. Davidson has been incarcerated for nearly 29 years, the admitted shooter has already been released from prison. Following the commutation of his sentence, Mr. Davidson will continue legal efforts to clear his name. In addition, while incarcerated, Mr. Davidson mentored and tutored over 1,000 prisoners to help them achieve their GED certificates. Mr. Davidson has earned praise from prison officials for his dedication to helping others.\n\nJames E. Johnson, Jr. \u2013 President Trump granted a full pardon to James E. Johnson, Jr. In 2008, Mr. Johnson pled guilty to charges related to migratory birds. Mr. Johnson received 1 year probation, was barred from hunting during that period, and a $7,500 fine was imposed. Throughout his life, Mr. Johnson has made numerous contributions for the conservation of wildlife.\n\nTommaso Buti \u2013 President Trump granted a full pardon to Tommaso Buti. Mr. Buti is an Italian citizen and a respected businessman. He is the Chief Operating Officer of a large Italian company and has started a successful charitable initiative to raise funds for UNICEF. More than 20 years ago, Mr. Buti was charged with financial fraud involving a chain of restaurants. He has not, however, been convicted in the United States.\n\nBill K. Kapri \u2013 President Trump granted a commutation to Bill Kapri, more commonly known as Kodak Black. Kodak Black is a prominent artist and community leader. This commutation is supported by numerous religious leaders, including Pastor Darrell Scott and Rabbi Schneur Kaplan. Additional supporters include Bernie Kerik, Hunter Pollack, Gucci Mane, Lil Pump, Lil Yachty, Lamar Jackson of the Baltimore Ravens, Jack Brewer formerly of the National Football League, and numerous other notable community leaders. Kodak Black was sentenced to 46 months in prison for making a false statement on a Federal document. He has served nearly half of his sentence. Before his conviction and after reaching success as a recording artist, Kodak Black became deeply involved in numerous philanthropic efforts. In fact, he has committed to supporting a variety of charitable efforts, such as providing educational resources to students and families of fallen law enforcement officers and the underprivileged. In addition to these efforts, he has paid for the notebooks of school children, provided funding and supplies to daycare centers, provided food for the hungry, and annually provides for underprivileged children during Christmas. Most recently while still incarcerated, Kodak Black donated $50,000 to David Portnoy\u2019s Barstool Fund, which provides funds to small businesses affected by the COVID-19 pandemic. Kodak Black\u2019s only request was that his donation go toward restaurants in his hometown.\n\nJawad A. Musa \u2013 President Trump commuted the sentence of Jawad A. Musa. In 1991, Mr. Musa was sentence to life imprisonment for a non-violent, drug-related offense. Mr. Musa\u2019s sentencing judge and the prosecutor on the case have both requested clemency on his behalf. He is currently 56-years old. During his time in prison, Mr. Musa has strengthened his faith and taken dozens of educational courses. Mr. Musa is blessed with a strong supportive network in Baltimore, Maryland and has numerous offers of employment.\n\nAdriana Shayota \u2013 President Trump commuted the sentence of Adriana Shayota. Ms. Shayota has served more than half of her 24 month sentence. The Deputy Mayor of Chula Vista, California, John McCann, supports this commutation, among other community leaders. Ms. Shayota is a mother and a deeply religious woman who had no prior convictions. She was convicted of conspiracy to traffic in counterfeit goods, commit copyright infringement, and introduce misbranded food into interstate commerce. During her time in prison, Ms. Shayota mentored those who wanted to improve their lives and demonstrated an extraordinary commitment to rehabilitation.\n\nGlen Moss \u2013 President Trump granted a full pardon to Glen Moss. After pleading guilty in 1998, Mr. Moss has been a vital member of his community. Mr. Moss has been committed to numerous philanthropic efforts at the national level, including St Jude\u2019s Hospital for Children, Breast Cancer Awareness, and the Colon Cancer Foundation. Within his community, he has contributed to Danbury Hospital and Ann\u2019s Place, a community-based cancer support center.\n\nAnthony Levandowski \u2013 President Trump granted a full pardon to Anthony Levandowski. This pardon is strongly supported by James Ramsey, Peter Thiel, Miles Ehrlich, Amy Craig, Michael Ovitz, Palmer Luckey, Ryan Petersen, Ken Goldberg, Mike Jensen, Nate Schimmel, Trae Stephens, Blake Masters, and James Proud, among others. Mr. Levandowski is an American entrepreneur who led Google\u2019s efforts to create self-driving technology. Mr. Levandowski pled guilty to a single criminal count arising from civil litigation. Notably, his sentencing judge called him a \u201cbrilliant, groundbreaking engineer that our country needs.\u201d Mr. Levandowski has paid a significant price for his actions and plans to devote his talents to advance the public good.\n\nAviem Sella \u2013 President Trump granted a full pardon to Aviem Sella. Mr. Sella is an Israeli citizen who was indicted in 1986 for espionage in relation to the Jonathan Pollard case. Mr. Sella\u2019s request for clemency is supported by the Prime Minister of Israel Benjamin Netanyahu, Israeli Ambassador to the United States Ron Dermer, the United States Ambassador to Israel David Friedman, and Miriam Adelson. The State of Israel has issued a full and unequivocal apology, and has requested the pardon in order to close this unfortunate chapter in U.S.-Israel relations.\n\nMichael Liberty \u2013 President Trump granted a full pardon to Michael Liberty. Mr. Liberty\u2019s request for clemency is supported by Representative Susan Austin, Matthew E. Sturgis, and Anthony Fratianne. In 2016 Mr. Liberty was convicted for campaign finance violations and later was indicted for related offenses. Mr. Liberty is the father of 7 children and has been involved in numerous philanthropic efforts.\n\nGreg Reyes \u2013 President Trump granted a full pardon to Greg Reyes. This pardon is supported by Shon Hopwood, former United States Attorney Brett Tolman, and numerous others. Mr. Reyes was the former CEO of Brocade Communications. Mr. Reyes was convicted of securities fraud. The Ninth Circuit Court of Appeals, however, threw out his convictions, finding prosecutorial misconduct. He was later retried, convicted, and sentenced to 18 months in Federal prison. Mr. Reyes has accepted full responsibility for his actions and has been out of prison for more than 8 years.\n\nFerrell Damon Scott \u2013 President Trump commuted the sentence of Ferrell Damon Scott. This commutation is supported by former Acting United States Attorney Sam Sheldon, who prosecuted his case and wrote that he \u201c\u2026 strongly does not believe that [Mr. Scott] deserves a mandatory life sentence.\u201d Ms. Alice Johnson, the CAN-DO Foundation, and numerous others also support clemency for Mr. Scott. Mr. Scott has served nearly 9 years of a life imprisonment sentence for possession with intent to distribute marijuana. Under today\u2019s sentencing guidelines, it is likely that Mr. Scott would not have received such a harsh sentence.\n\nJerry Donnell Walden \u2013 President Trump commuted the sentence of Jerry Donnell Walden. Mr. Walden has served 23 years of a 40-year prison sentence. He is known as a model inmate who completed his GED while incarcerated, as well as various other education classes.\n\nJeffrey Alan Conway \u2013 President Trump granted a full pardon to Jeffrey Alan Conway. Mr. Conway\u2019s pardon is strongly supported by his business partners Gary N. Solomon and Ely Hurwitz, members of law enforcement, and numerous other members of the community. Since his release from prison, Mr. Conway has led a successful life and currently runs 10 restaurant businesses that employ nearly 500 people. Mr. Conway is active in his community and in various philanthropic efforts.\n\nBenedict Olberding \u2013 President Trump granted a full pardon to Benedict Olberding. Mr. Olberding was convicted on one count of bank fraud. Mr. Olberding is an upstanding member of the community who has paid his debt to society. After completing his sentence, he purchased two aquarium stores, as well as a consulting business to train prospective mortgage brokers.\n\nSyrita Steib-Martin \u2013 President Trump granted a full pardon to Syrita Steib-Martin. This clemency is supported by Ben Watson formerly of the National Football League, Judge Sandra Jenkins of the Louisiana state courts, and Sister Marjorie Herbert, who serves as President and CEO of Catholic Charities Archdiocese of New Orleans, among many others. Ms. Steib-Martin was convicted at the age of 19 and sentenced to 10 years in prison and nearly $2 million in restitution for the use of fire to commit a felony. After her release from prison, she became an advocate for criminal justice reform and founded Operation Restoration, which helps transition women prisoners after incarceration by providing education opportunities and job placement. With today\u2019s pardon, Ms. Steib-Martin is relieved of the crushing restitution she incurred at such a young age.\n\nMichael Ashley \u2013 President Trump commuted the sentence of Michael Ashley. This commutation is supported by Professor Alan Dershowitz, Pastor Darrel Scott, Rabbi Zvi Boyarski, The Aleph Institute, Rabbi Hirschy Zarchi, Gary Apfel, and Bradford Cohen. Mr. Ashley was convicted and sentenced to 3 years in prison for bank fraud. Notably, Mr. Ashley\u2019s sentencing judge said, \u201cI don\u2019t have any concern that you are not truly remorseful. I know that you are a changed man.\u201d Since his conviction, Mr. Ashley has spent time caring for his ailing mother and paying his debt back to society.\n\nLou Hobbs \u2013 President Trump commuted the sentence of Lou Hobbs. Mr. Hobbs has served 24 years of his life sentence. While incarcerated, Mr. Hobbs completed his GED as well as various other education classes. Mr. Hobbs is dedicated to improving his life and is focused on his family and friends who have assisted him during difficult times.\n\nMatthew Antoine Canady \u2013 President Trump commuted the sentence of Matthew Antoine Canady. This commutation is supported by Acting Attorney General Jeffrey Rosen and the Office of the Pardon Attorney. Mr. Canady had an unstable childhood and all of his prior drug-related convictions occurred during his teenage years. Mr. Canady worked hard to move beyond his challenging circumstances and has demonstrated extraordinary rehabilitation while in custody. He has maintained clear conduct while incarcerated and has notably taken advantage of significant vocational programs, including an electrical apprenticeship. He receives \u201coutstanding\u201d work reports and is described as \u201chardworking\u201d and \u201crespectful\u201d by the Bureau of Prisons staff. Mr. Canady takes full responsibility for his criminal actions and would like to find gainful employment to help support his children.\n\nMario Claiborne \u2013 President Trump commuted the sentence of Mario Claiborne. This commutation is supported by Acting Attorney General Jeffrey Rosen and the Office of the Pardon Attorney. Mr. Claiborne is serving life imprisonment and has already served more than 28 years in prison. For more than 20 years, Mr. Claiborne has maintained clear conduct. Mr. Claiborne currently works for a UNICOR facility and has completed rehabilitative programming, including drug education.\n\nRodney Nakia Gibson \u2013 President Trump commuted the sentence of Rodney Nakia Gibson. This commutation is supported by Acting Attorney General Jeffrey Rosen and the Office of the Pardon Attorney. In 2009, Mr. Gibson was convicted of trafficking drugs. Mr. Gibson is a first time, non-violent offender who has been a \u201cmodel inmate\u201d for more than 11 years in custody. In addition, he has maintained clear conduct and works with other inmates to help them obtain the important benefits of a GED. He has an impressive list of programming accomplishments, including apprenticeships and professional certifications which will readily translate into employable skills upon release. Mr. Gibson accepts responsibility for his actions.\n\nTom Leroy Whitehurst \u2013 President Trump commuted the sentence of Tom Leroy Whitehurst from life to 30 years. This clemency is supported by Acting Attorney General Jeffrey Rosen and the Office of the Pardon Attorney. Mr. Whitehurst led a conspiracy to manufacture at least 16.7 kilograms of methamphetamine and possessed numerous firearms during the course of the conspiracy. The court sentenced him to life imprisonment under the then-mandatory Sentencing Guidelines. Mr. Whitehurst has served nearly 24 years in prison. While incarcerated, he has demonstrated exemplary prison conduct by incurring just a single disciplinary infraction over two decades ago and holding a UNICOR position for much of his incarceration.\n\nMonstsho Eugene Vernon \u2013 President Trump commuted the sentence of Monstsho Eugene Vernon. This commutation is supported by Acting Attorney General Jeffrey Rosen and the Office of the Pardon Attorney. Mr. Vernon has served over 19 years in prison for committing a string of armed bank robberies in Greenville, South Carolina. Evidence showed that numerous of these offenses involved him carrying BB guns rather than genuine firearms. While incarcerated, Mr. Vernon has worked steadily, programmed well, and recovered from a bout of cancer.\n\nLuis Fernando Sicard \u2013 President Trump commuted the sentence of Luis Fernando Sicard. This commutation is supported by Acting Attorney General Jeffrey Rosen and the Office of the Pardon Attorney. Mr. Sicard was sentenced in 2000 for conspiracy to possess with intent to distribute cocaine and possession of a firearm during and in furtherance of a drug trafficking crime. He has served 20 years with clear conduct. Mr. Sicard has participated in substantial programming, including a number of vocational courses. Currently, Mr. Sicard works in the camp vehicular factory and previously worked in UNICOR earning \u201coutstanding\u201d work reports, and he also volunteers in the inmate puppy program. Importantly, Mr. Sicard takes full responsibility for his criminal actions. Mr. Sicard is a former Marine and father of two girls.\n\nDeWayne Phelps \u2013 President Trump commuted the sentence of DeWayne Phelps. This commutation is supported by Acting Attorney General Jeffrey Rosen and the Office of the Pardon Attorney. Mr. Phelps has served 11 years in prison for conspiracy to distribute methamphetamine. He has served over a decade in prison with clear conduct, has trained as a dental apprentice, participated in UNICOR, and is noted as being a reliable inmate capable of being assigned additional responsibilities. Most notably, Mr. Phelps\u2019s sentence would unquestionably be lower today under the First Step Act.\n\nIsaac Nelson \u2013 President Trump commuted the sentence of Isaac Nelson. This commutation is supported by Acting Attorney General Jeffrey Rosen and the Office of the Pardon Attorney. Mr. Nelson is serving a mandatory 20 year sentence for conspiracy to possess with intent to distribute and distribution of 5 kilograms or more of cocaine and 50 grams or more of crack cocaine. Following the First Step Act\u2019s changes to the definition of serious drug felony, Mr. Nelson would no longer receive a mandatory minimum term of 20 years\u2019 imprisonment. Instead, he would likely face a 10-year sentence. He has already served more than 11 years in prison. Throughout his incarceration, he appears to have demonstrated commendable adjustment to custody.\n\nTraie Tavares Kelly \u2013 President Trump commuted the sentence of Traie Tavares Kelly. This commutation is supported by Acting Attorney General Jeffrey Rosen and the Office of the Pardon Attorney. Mr. Kelly was convicted of conspiracy to possess with intent to distribute and to distribute 50 grams or more of cocaine base and 5 kilograms or more of cocaine. He has served over 14 years in prison, but if he were sentenced today, he would likely be subject only to 10-year mandatory minimum. Moreover, Mr. Kelly has substantial work history while incarcerated and his notable accomplishments in education and programming demonstrate that he has used his time to maximize his chance at being a productive citizen upon release.\n\nJavier Gonzales \u2013 President Trump commuted the sentence of Javier Gonzales. This commutation is supported by Acting Attorney General Jeffrey Rosen and the Office of the Pardon Attorney. Mr. Gonzales was convicted of conspiracy to possess with intent to distribute methamphetamine and distribution of methamphetamine in 2005. He has served over 14 years in prison, which is 4 years longer than the 10-year sentence he would likely receive today. He has a demonstrated record of rehabilitation during his incarceration, including steady employment, with substantial UNCIOR experience, and participation in vocational programming and training to facilitate his successful reintegration into the workforce upon release. He also has no history of violent conduct. Mr. Gonzales has actively addressed his admitted substance abuse issues with nonresidential drug treatment and participation in the residential program.\n\nEric Wesley Patton \u2013 President Trump granted a full pardon to Eric Wesley Patton. This pardon is supported by former Deputy Attorney General Rod Rosenstein and the Office of the Pardon Attorney. Mr. Patton was convicted of making a false statement on a mortgage application in 1999. In the 20 years since his conviction, Mr. Patton has worked hard to build a sterling reputation, been a devoted parent, and made solid contributions to his community by quietly performing good deeds for friends, neighbors, and members of his church.\n\nRobert William Cawthon \u2013 President Trump granted a full pardon to Robert William Cawthon. His pardon is supported by former Deputy Attorney General Rod Rosenstein and the Office of the Pardon Attorney. Mr. Cawthon was convicted in 1992 for making a false statement on a bank loan application and was sentenced to 3 years\u2019 probation, conditioned upon 180 days\u2019 home confinement. Mr. Cawthon has accepted responsibility for his offense, served his sentence without incident, and fulfilled his restitution obligation. His atonement has been exceptional, and since his conviction he has led an unblemished life while engaging in extensive, praiseworthy community service.\n\nHal Knudson Mergler \u2013 President Trump granted a full pardon to Hal Knudson Mergler. This pardon is supported by former Deputy Attorney General Rod Rosenstein and the Office of the Pardon Attorney. Mr. Mergler was convicted of conspiracy to possess with intent to distribute and distribution of lysergic acid diethylamide (LSD) in 1992. He received 1 month imprisonment, 3 years supervised release, and ordered to pay restitution. Since his conviction, Mr. Mergler has lived a productive and law-abiding life, including by earning a college degree, creating a successful business career, and starting a family. He has made significant contributions to his community and has helped to build a new school for a non-profit charitable organization. He is uniformly praised as a hardworking and ethical businessman and a caring father.\n\nGary Evan Hendler \u2013 President Trump granted a full pardon to Gary Evan Hendler. This pardon is supported by former Deputy Attorney General Rod Rosenstein and the Office of the Pardon Attorney. In 1984, Mr. Hendler was convicted of conspiracy to distribute and dispense controlled substances and served 3 years\u2019 probation for his crime. He is remorseful and has taken full responsibility for his criminal actions. In the 40 years since his conviction, Mr. Hendler has lived a law-abiding life and has positively contributed to his community. He is financially stable and owns a successful real estate business. Most notably, he has helped others recover from addiction. Since 1982, he has organized and led weekly AA meetings. He also has mentored many individuals on their journey to sobriety with his radio broadcasts. His former probation officer noted that Mr. Hendler had become \u201cintegral\u201d in the lives of many members of the community who were dealing with substance abuse issues. Further, his efforts in addiction and recovery have been recognized by Pennsylvania Governor Tom Wolf, who recently appointed him to a state advisory council on drug and alcohol abuse.\n\nJohn Harold Wall \u2013 President Trump granted a full pardon to John Harold Wall. This pardon is supported by former Deputy Attorney General Rod Rosenstein, the former United States Attorney for the District of Minnesota Andrew M. Luger, and the Office of the Pardon Attorney. Mr. Wall was convicted of aiding and abetting possession with intent to distribute methamphetamine in 1992. He completed a 60 month prison sentence with 4 years\u2019 supervised release.\n\nSteven Samuel Grantham \u2013 President Trump granted a full pardon to Steven Samuel Grantham. This pardon is supported by Mr. Grantham\u2019s friends and family who praise his moral character, Acting Attorney Jeffrey Rosen, former Deputy Attorney General Rod Rosenstein, and the Office of the Pardon Attorney. Mr. Grantham was convicted in 1967 for stealing a vehicle. He received 18-months imprisonment, and 2 years\u2019 probation. Since his conviction and release from prison, he has demonstrated remorse and accepted responsibility for his crime, which he committed approximately 50 years ago when he was just 19 years old. Mr. Grantham has lived a law-abiding and stable life. Most notably, he stepped in and assumed custody of his grandchild when the child\u2019s parents were unable to care for him. He now seeks a pardon for forgiveness and to restore his gun rights.\n\nClarence Olin Freeman \u2013 President Trump granted a full pardon to Clarence Olin Freeman. This pardon is supported by former Deputy Attorney General Rod Rosenstein and the Office of the Pardon Attorney. Mr. Freeman was convicted in 1965 for operating an illegal whiskey still. He received 9 months imprisonment and 5 years\u2019 probation. Since his conviction and release from prison, Mr. Freeman has led a law-abiding life. He has expressed sincere remorse for his illegal activity and remains mindful of the valuable lesson his conviction taught him. In the approximately 55 years since his conviction, he has built a stable marriage, founded a thriving business, and contributed positively to his community. He has earned a reputation for honesty, hard work, and generosity.\n\nFred Keith Alford \u2013 President Trump granted a full pardon to Fred Keith Alford. This pardon is supported by former Deputy Attorney General Rod Rosenstein and the Office of the Pardon Attorney. Mr. Alford was convicted in 1977 for a firearm violation and served 1 year\u2019s unsupervised probation. Since his conviction, he has established a stable and law-abiding life and earned a commendable reputation in his small town as a man of great skill, dedication, and integrity.\n\nJohn Knock \u2013 President Trump commuted the sentence of John Knock. This commutation is supported by his family. Mr. Knock is a 73 year-old man, a first-time, non-violent marijuana only offender, who has served 24 years of a life sentence. Mr. Knock has an exemplary prison history, during which he completed college accounting classes and has had zero incident reports.\n\nKenneth Charles Fragoso \u2013 President Trump commuted the sentence of Kenneth Charles Fragoso. Mr. Fragoso is a 66 year-old United States Navy veteran who has served more than 30 years of a life sentence for a nonviolent drug offense. Mr. Fragoso has an exemplary prison history and has worked for UNICOR for over 20 years, learned new trades, and has mentored fellow inmates.\n\nLuis Gonzalez \u2013 President Trump commuted the sentence of Luis Gonzalez. Mr. Gonzalez is a 78 year-old non-violent drug offender who has served more than 27 years of a life sentence. Under the First Step Act, Mr. Fragoso would not have been subject to a mandatory life sentence. Mr. Gonzalez has an upstanding prison record and has worked for UNICOR for over 20 years producing military uniforms.\n\nAnthony DeJohn \u2013 President Trump commuted the sentence of Anthony DeJohn. Mr. DeJohn has served more than 13 years of a life sentence for conspiracy to distribute marijuana. Mr. DeJohn has maintained a clear disciplinary record and has been recognized for his outstanding work ethic while incarcerated. Mr. DeJohn has employment and housing available to him upon release.\n\nCorvain Cooper \u2013 President Trump commuted the sentence of Mr. Corvain Cooper. Mr. Cooper is a 41 year-old father of two girls who has served more than 7 years of a life sentence for his non-violent participation in a conspiracy to distribute marijuana.\n\nWay Quoe Long \u2013 President Trump commuted the sentence of Way Quoe Long. Mr. Long is a 58 year-old who has served nearly half of a 50-year sentence for a non-violent conviction for conspiracy to manufacture and distribute marijuana. Mr. Long has spent his incarceration striving to better himself through English proficiency classes and by obtaining his GED. Upon release, Mr. Long will reunite with his family and will be strongly supported as he integrates back into the community.\n\nMichael Pelletier \u2013 President Trump commuted the sentence of Michael Pelletier. Mr. Pelletier is a 64 year-old who has served 12 years of a 30 year sentence for conspiracy to distribute marijuana. Mr. Pelletier has maintained a clear disciplinary record, has thrived as an artist working with oil paints on canvas, and has taken several courses to perfect his skill while incarcerated. Upon his release, Mr. Pelletier will have a meaningful place of employment and housing with his brother.\n\nCraig Cesal \u2013 President Trump commuted the sentence of Craig Cesal. Mr. Cesal is a father of two, one of whom unfortunately passed away while he was serving his life sentence for conspiracy to distribute marijuana. Mr. Cesal has had an exemplary disciplinary record and has become a paralegal assistant and a Eucharistic Minister in the Catholic Church to assist and guide other prisoners. Upon his release, Mr. Cesal looks forward to reintegrating back into society and to contributing to his community while living with his daughter with whom he has remained close. Mr. Cesal hopes to be a part of her upcoming wedding.\n\nDarrell Frazier \u2013 President Trump commuted the sentence of Darrell Frazier. Mr. Frazier is a 60 year-old who has served 29 years of a life sentence for non-violent conspiracy to distribute and possess with intent to distribute cocaine. Mr. Frazier has had an exemplary disciplinary record in prison and has spent his time creating the Joe Johnson Tennis Foundation, a 501(c)(3) that provides free tennis lessons to hundreds of children in underserved communities. Upon his release, Mr. Frazier will have a meaningful place of employment and housing with his mother.\n\nLavonne Roach \u2013 President Trump commuted the sentence of Lavonne Roach. Ms. Roach has served 23 years of a 30-year sentence for non-violent drug charges. She has had an exemplary prison record and has tutored and mentored other prisoners. Ms. Roach has a strong family support system to help her transition back into the community.\n\nBlanca Virgen \u2013 President Trump commuted the sentence of Blanca Virgen. Ms. Virgen has served 12 years of a 30 year sentence. Rather than accept a plea offer of 10 years, Ms. Virgen exercised her constitutional right to trial and received triple the amount of time the government offered her to plead. She has received countless achievement awards from her educational programming in prison. Upon her release, Ms. Virgen will return home to Mexico to care for her four children.\n\nRobert Francis \u2013 President Trump commuted the sentence of Robert Francis. Mr. Francis has served 18 years of a life sentence for non-violent drug conspiracy charges. Mr. Francis has a spotless disciplinary record in prison and has been active in his efforts toward rehabilitation. Upon release, Mr. Francis, a father of 3, will live with his sister in Houston, Texas.\n\nBrian Simmons \u2013 President Trump commuted the sentence of Brian Simmons. Mr. Simmons has served 5 years of a 15 year sentence for a non-violent conspiracy to manufacture and distribute marijuana. Mr. Simmons has had an exemplary prison record and upon release will have strong support from his fianc\u00e9e and his community.\n\nDerrick Smith \u2013 President Trump commuted the sentence of Derrick Smith. Mr. Smith is a 53 year-old who has served more than 20 years of a nearly 30 year sentence for distribution of drugs to a companion who passed away. Mr. Smith is deeply remorseful for his role in this tragic death and has had an exemplary record while incarcerated. Mr. Smith intends to secure a construction job, care for his mother and his son, and rebuild his relationship with his two other children.\n\nRaymond Hersman \u2013 President Trump commuted the sentence of Raymond Hersman. Mr. Hersman is a 55 year-old father of two who has served more than 9 years of a 20 year sentence. While incarcerated, Mr. Hersman has maintained a spotless disciplinary record, worked steadily, and participated in several programming and educational opportunities. Upon release, he looks forward to transitioning back into the community and leading a productive life with strong family support.\n\nDavid Barren \u2013 President Trump commuted the sentence of David Barren. Mr. Barren is a father of 6 children. He has served 13 years of his life sentence in addition to 20 years for a non-violent drug conspiracy charge. Mr. Barren has maintained an exemplary prison record. Upon release, Mr. Barren looks forward to returning home to his family.\n\nJames Romans \u2013 President Trump commuted the sentence of James Romans. Mr. Romans is a father and a grandfather who received a life sentence without parole for his involvement in a conspiracy to distribute marijuana. Mr. Romans has had an exemplary disciplinary record for the more than 10 years he has served, and has completed a long list of courses. He has already secured job opportunities that will help him successfully re-enter society.\n\nJonathon Braun \u2013 President Trump commuted the sentence of Jonathan Braun. Mr. Braun has served 5 years of a 10-year sentence for conspiracy to import marijuana and to commit money laundering. Upon his release, Mr. Braun will seek employment to support his wife and children.\n\nMichael Harris \u2013 President Trump commuted the sentence of Michael Harris. Mr. Harris is a 59 year old who has served 30 years of a 25 year to life sentence for conspiracy to commit first-degree murder. Mr. Harris has had an exemplary prison record for three decades. He is a former entrepreneur and has mentored and taught fellow prisoners how to start and run businesses. He has completed courses towards business and journalism degrees. Upon his release, Mr. Harris will have a meaningful place of employment and housing with the support of his family.\n\nKyle Kimoto \u2013 President Trump commuted the sentence of Kyle Kimoto. Mr. Kimoto is a father of six who has served 12 years of his 29 year sentence for a non-violent telemarketing fraud scheme. Mr. Kimoto has been an exemplary prisoner, has held numerous jobs, shown remorse, and mentored other inmates in faith. Upon his release, he has a job offer and will help care for his six children and three grandchildren.\n\nChalana McFarland \u2013 President Trump commuted the sentence of Chalana McFarland. Ms. McFarland has served 15 years of a 30-year sentence. Though she went to trial, Ms. McFarland actually cooperated with authorities by informing them of a potential attack on the United States Attorney. Her co-defendants who pled guilty, however, received lesser sentences ranging from 5 to 87 months. Ms. McFarland was a model inmate and is now under home confinement.\n\nEliyahu Weinstein \u2013 President Trump commuted the sentence of Eliyahu Weinstein. This commutation is supported by former U.S. Attorney Brett Tolman, former Representative Bob Barr, former U.S. Attorney Joseph Whittle, Professor Alan Dershowitz, Representative Mark Walker, Representative Scott Perry, Representative Jeff Van Drew, Jessica Jackson of the Reform Alliance, The Tzedek Association, Dr. Danny Feuer, and numerous victims who have written in support. Mr. Weinstein is the father of seven children and a loving husband. He is currently serving his eighth year of a 24-year sentence for a real estate investment fraud and has maintained an exemplary prison history. Upon his release, he will have strong support from his community and members of his faith.\n\nJohn Estin Davis \u2013 President Trump commuted the sentence of John Estin Davis. This commutation is supported by Caroline Bryan, Luke Bryan, Ellen Boyer, Amy Davis, Kim Davis, Brandon McWherter, Sheila McWherter, Dr. Jeff Hall, Dr. Brad Maltz, Brent Ford, Mark Lotito, Keri Rowland, Mark Rowland, and Stephen Stock. Mr. Davis has spent the last 4 months incarcerated for serving as Chief Executive Office of a healthcare company with a financial conflict of interest. Notably, no one suffered financially as a result of his crime and he has no other criminal record. Prior to his conviction, Mr. Davis was well known in his community as an active supporter of local charities. He is described as hardworking and deeply committed to his family and country. Mr. Davis and his wife have been married for 15 years, and he is the father of three young children.\n\nAlex Adjmi \u2013 President Trump granted a full pardon to Alex Adjmi. This pardon is supported by Haim Chera on behalf of his late father Stanley, Robert Cayre, the Sitt family and numerous other community leaders. In 1996, Mr. Adjmi was convicted of a financial crime and served 5 years in prison. Following his release, he has dedicated himself to his community and has supported numerous charitable causes, including support for children with special needs and substance recovery centers.\n\nElliott Broidy \u2013 President Trump granted a full pardon to Elliott Broidy. Mr. Broidy is the former Deputy National Finance Chair of the Republican National Committee. This pardon is supported by Representative Devin Nunes, Representative Ken Calvert, Representative Jack Bergman, Representative George Holding, Ambassador Ric Grenell, Bernie Marcus, Malcolm Hoenlein, Eric Branstad, Tom Hicks, Saul Fox, Lee Samson, Rabbi Steven Leder, Dr. Alveda King, Father Frank Pavone, Major General Clayton Hutmacher, Lieutenant General Bennet Sacolick, Mr. Bruce Brereton, Rabbi Steven Burg, Rabbi Pini Dunner, Rabbi Meyer May, and Rabbi Mordechai Suchard. Mr. Broidy was convicted on one count of conspiracy to serve as an unregistered agent of a foreign principal. Mr. Broidy is well known for his numerous philanthropic efforts, including on behalf of law enforcement, the military and veterans programs, and the Jewish community.\n\nStephen K. Bannon \u2013 President Trump granted a full pardon to Stephen Bannon. Prosecutors pursued Mr. Bannon with charges related to fraud stemming from his involvement in a political project. Mr. Bannon has been an important leader in the conservative movement and is known for his political acumen.\n\nDouglas Jemal \u2013 President Trump granted a full pardon to Douglas Jemal. Mr. Jemal is an American businessman and philanthropist credited with rebuilding many urban inner cities in the United States. In 2008, Mr. Jemal was convicted of fraud. In addition, Mr. Jemal was instrumental to various other charitable causes, including the rebuilding of churches prior to his conviction. Notably, at his trial the presiding judge told prosecutors that he thought it \u201cinconceivable\u201d to send Mr. Jemal to prison.\n\nNoah Kleinman \u2013 President Trump commuted the sentence of Noah Kleinman. Mr. Kleinman is a 45-year old father of two children. The mother of his children unfortunately passed away during Mr. Kleinman\u2019s incarceration. Mr. Kleinman has served 6 years of a nearly 20 year sentence for a non-violent crime to distribute marijuana. Mr. Kleinman has had an exemplary prison history and has worked to remain close to his children and his father. Upon release, he looks forward to living with his father, working for the family business, and caring for his children.\n\nDr. Scott Harkonen \u2013 President Trump granted a full pardon Dr. Scott Harkonen. Dr. Harkonen was convicted of fraud based on a misleading caption in a press release with respect to a treatment for a disease. Dr. Harkonen is world renowned for his discovery of a new kidney disease, as well as its cause and treatment. Dr. Harkonen looks forward to returning to medicine.\n\nJohnny D. Phillips, Jr. \u2013 President Trump granted a full pardon to Johnny D. Phillips, Jr. This pardon is supported by Senator Rand Paul, the former United States Attorney for the Middle District of Tennessee, and numerous members of his community. In 2016, Mr. Phillips was convicted of conspiracy to commit wire fraud and mail fraud. Mr. Phillips is known as an upstanding citizen and is a valued member of his community. He dedicates his time to his three young children and is an advocate for Type 1 diabetes research.\n\nDr. Mahmoud Reza Banki \u2013 President Trump granted a full pardon to Dr. Mahmoud Reza Banki. This pardon is supported by many elected officials of stature, including the late Representative John Lewis, Senator Diane Feinstein, and other Members of Congress. Dr. Banki is an Iranian American citizen who came to the United States when he was 18 years old. He graduated from the University of California, Berkeley, before obtaining a PhD from Princeton University and an MBA from the University of California, Los Angeles. In 2010 Dr. Banki was charged with monetary violations of Iranian sanctions and making false statements. The charges related to sanctions violations were subsequently overturned by the United States Court of Appeals for the Second Circuit. However, the felony charges for making false statements have prevented Dr. Banki from resuming a full life. In the years since his conviction, Dr. Banki has dedicated himself to his community and maintained a sincere love and respect for the United States.\n\nTena Logan \u2013 President Trump commuted the sentence of Tena Logan. Ms. Logan has served 8 years of a 14-year sentence for a non-violent drug offense. She had an exemplary prison record with extensive work and programming, and has assumed several leadership positions. In addition, Ms. Logan was authorized to work outside the perimeter of the prison, and was granted home confinement under the CARES Act last summer. Today, Ms. Logan lives with her husband and works fulltime at a major retail store.\n\nMaryAnne Locke \u2013 President Trump commuted the sentence of MaryAnne Locke. Ms. Locke has served roughly 11 years of a nearly 20 year sentence for a non-violent drug offense. Despite the difficulties of beginning her sentence just 6 weeks after having a Caesarean section, her prison record has been exemplary, with extensive programming and work. Ms. Locke was authorized to work outside the perimeter of the prison, and was granted home confinement under the CARES Act last summer. Today, she lives with her father, is building a relationship with her children, and works fulltime at a major retail store.\n\nApril Coots \u2013 President Trump commuted the sentence of April Coots. Ms. Coots has served more than 10 years of her 20-year sentence for a non-violent drug offense. Throughout her incarceration, she has been an exemplary inmate, obtained an HVAC license, and completed the PAWS apprenticeship program. During the 18 months before the trial, Ms. Coots started a business, completed her GED, and took two semesters of community college classes. Importantly, Ms. Coots has a supportive family and church community that will help her transition and create a stable network for her post-incarceration.\n\nCaroline Yeats \u2013 President Trump commuted the sentence of Caroline Yeats. Ms. Yeats was a first-time, non-violent drug offender who has served nearly 7 years of a 20-year sentence. She has been an exemplary inmate who spends her time training service dogs as part of the PAWS program, mentoring other inmates, and she has been a committed member of her faith community. Upon her release, she plans on spending time with her husband of 30 years who suffers from multiple sclerosis.\n\nJodi Lynn Richter \u2013 President Trump commuted the sentence of Jodi Lynn Richter. Ms. Richter has served 10 years of a 15-year sentence for a non-violent drug offense. Ms. Richter has an exemplary prison record, and spends her time training service dogs in the PAWS program, tutoring other inmates in pursuit of their GED, and learning to operate a range of heavy machinery. Her parents have continued to support her and she has various employment opportunities available.\n\nKristina Bohnenkamp \u2013 President Trump commuted the sentence of Kristina Bohnenkamp. Notably, her warden recommended her for home confinement under the CARES Act. Ms. Bohnenkamp has served more than 10 years of a 24 year sentence for a non-violent drug offense. Ms. Bohnenkamp has been an exemplary inmate, with an excellent record of programming and UNICOR work, and she is authorized to work outside the prison perimeter. Upon her release, she is planning on spending time with her sister and brother-in-law and she has various employment opportunities available.\n\nMary Roberts \u2013 President Trump commuted the sentence of Mary Roberts. Ms. Roberts has served 10 years of a 19-year sentence for a non-violent drug offense. She has maintained an exemplary disciplinary record, and a strong programming and work history, including as a part of the PAWS program, UNICOR and food service, and she is authorized to work outside the prison perimeter. Upon her release, Ms. Roberts plans to spend time with her daughter and enjoys strong support from her family. In addition, she has various employment opportunities available.\n\nCassandra Ann Kasowski \u2013 President Trump commuted the sentence of Cassandra Ann Kasowski. Notably, her warden recommended her for home confinement under the CARES Act. Ms. Kasowski has served more than7 years of a 17 year sentence for a non-violent drug offense. She has been an exemplary inmate and has worked extensively, including as a part of the PAWS program and in UNICOR. Upon her release, she plans to spend time with her son and seek employment.\n\nLerna Lea Paulson \u2013 President Trump commuted the sentence of Lerna Lea Paulson. Notably, Ms. Paulson\u2019s warden recommended her for home confinement under the CARES Act. Ms. Paulson has served nearly 7 years of a 17-year sentence for a non-violent drug offense. During her time in prison, she has maintained an exemplary disciplinary record, has worked full-time in UNICOR, and served as a mental health counselor. In addition, she has served an inmate companion as well as a suicide watch companion. She is also authorized to work outside the prison perimeter. Upon her release, she plans on spending time with her family and seek employment.\n\nAnn Butler \u2013 President Trump commuted the sentence of Ann Butler. Ms. Butler has served more than 10 years of a nearly 20-year sentence for a non-violent offense. She has an exemplary prison record, with extensive programming and work history and has garnered outstanding evaluations. In addition, she is extraordinarily devoted to her faith. At the time of her arrest, Ms. Butler was caring for five children and held two minimum-wage jobs. Upon her release, Ms. Butler wishes to reunite with her family and seek employment.\n\nSydney Navarro \u2013 President Trump commuted the sentence of Sydney Navarro. Ms. Navarro has served nearly 8 years of a 27-year sentence for a non-violent drug offense. She has an exemplary prison record. In addition, Ms. Navarro obtained her GED, participated in extensive program work, and earned excellent work evaluations. Notably, Ms. Navarro was chosen to speak to at-risk youth in the community through the SHARE program. Upon her release, Ms. Navarro wishes to reunite with her daughter and seek employment.\n\nTara Perry \u2013 President Trump commuted the sentence of Tara Perry. Ms. Perry has served nearly 7 years of a 16-year sentence for a non-violent drug offense. She has maintained an exemplary prison record and has obtained her nursing certification. Ms. Perry also enjoys singing during the prison religious services. Upon her release, Ms. Perry plans to spend time with her mother and seek employment.\n\nJohn Nystrom \u2013 President Trump granted a full pardon to John Nystrom, who, other than this conviction, was described by his sentencing judge as a \u201cmodel citizen.\u201d His clemency is supported by Governor Kristi Noem and Senator Michael Rounds. Over 10 years ago, while working as a contractor on a school reconstruction project, Mr. Nystrom failed to alert the proper authorities when he learned that a subcontractor was receiving double payments for work performed. Mr. Nystrom took full responsibility for this oversight and even tried to pay the Crowe Creek Tribe, who was paying for the work, restitution before he pled guilty. Mr. Nystrom has since paid his restitution in full. Mr. Nystrom teaches Sunday school and volunteers for the Knights of Columbus and Habitat for Humanity, among other organizations, and has previously served as County Commissioner.\n\nGregory Jorgensen, Deborah Jorgensen, Martin Jorgensen \u2013 President Trump granted full pardons to Gregory and Deborah Jorgensen, and a posthumous pardon to Martin Jorgensen. Governor Kristi Noem and Senator Mike Rounds support clemency for this family, which has an exemplary record of service to their community. In the 1980\u2019s, Gregory and his father, Martin, gathered a group of South Dakota cattle producers to market and sold processed beef. The Jorgensen\u2019s marketed their beef under the Dakota Lean brand and sold the premium product as heart-healthy and antibiotic- and hormone-free. When demand outstripped supply, Gregory, Deborah, and Martin mixed in inferior, commercial beef trim and knowingly sold misbranded beef. Since their convictions in 1996, the Jorgensen\u2019s have served their community devotedly. Gregory was elected twice to the Tripp County Board of Commissioners and spearheaded infrastructure projects to improve access for Native American communities. Deborah is a lifelong member of a non-profit dedicated to promoting educational opportunities for women. And Martin was named National Beef Cattleman\u2019s Association Businessman of the Year. The Jorgensens have shown remorse for their previous action, and in light of decades of exemplary public service, they are well deserving of these pardons.\n\nJessica Frease \u2013 President Trump granted a full pardon to Jessica Frease. This pardon is supported by Governor Kristi Noem, South Dakota State Senator Lynne Hix-DiSanto, the United States Probation Officer responsible for Ms. Frease\u2019s supervision, and many in her community. Ms. Frease was 20 years old when she was convicted after converting stolen checks and negotiating them through the bank where she worked as a teller. Upon her arrest, however, she immediately relinquished the stolen funds to the authorities. After serving her two year sentence, she was granted early termination of her supervised release due to her commendable conduct. Currently, Ms. Frease is studying to become an Emergency Medical Technician and devotes her time and energy to raising funds for cancer patients.\n\nRobert Cannon \u201cRobin\u201d Hayes \u2013 President Trump granted a full pardon to Robert Cannon \u201cRobin\u201d Hayes. The former North Carolina Congressman is serving a 1-year term of probation for making a false statement in the course of a Federal investigation. In addition to his years in Congress, Mr. Hayes has served as Chairman of the North Carolina Republican Party and Chair of the National Council of Republican Party Chairs. Senator Thom Tillis and several members of the North Carolina Congressional delegation strongly support clemency for Mr. Hayes.\n\nThomas Kenton \u201cKen\u201d Ford \u2013 President Trump granted a full pardon to Ken Ford, a 38-year veteran of the coal industry and currently the General Manager of a coal company. Mr. Ford\u2019s pardon is supported by members of the coal mining community, including those with extensive experience in mining operations, safety, and engineering, who describe Mr. Ford as a \u201cmodel manager\u201d who conducts himself with the utmost professionalism and integrity. Twenty years ago, Mr. Ford made a material misstatement to Federal mining officials. Mr. Ford pled guilty and served a sentence of 3 years\u2019 probation. In the decades since, Mr. Ford has been an upstanding member of his community and has used this experience and his decades of expertise to keep miners safe, including promoting truthfulness and integrity with Federal mining officials, for whom Mr. Ford states that he has the \u201cutmost respect.\u201d\n\nJon Harder \u2013 President Trump commuted the sentence of Jon Harder, former President and CEO of Sunwest Management Inc., who has served nearly 5 years of a 15-year prison sentence. Notable figures, including the Honorable Michael Hogan who served as the Federal judge overseeing Sunwest\u2019s bankruptcy and receivership, Ford Elsaesser who served as counsel to Sunwest\u2019s creditors in receivership, and multiple other individuals involved in the litigation support Mr. Harder\u2019s commutation. Mr. Harder was serving as president and CEO of Sunwest Management Inc., a large management company overseeing residential senior care facilities, when he misused investment funds during the real estate crisis. Mr. Harder fully accepted responsibility, pled guilty, and cooperated with the government\u2019s civil and criminal actions against him at great personal cost. According to former Chief Judge Hogan, Mr. Harder\u2019s full cooperation \u201cagainst his substantial financial and penal interests\u201d helped secure the sale of the company\u2019s assets, ensuring that Sunwest\u2019s investors recovered more of their investment, seniors could continue living in their facilities, and employees could retain their livelihoods. Mr. Elsaesser stated that \u201cof all the financial wrongdoers that [the court and the Government] dealt with during the real estate crash of 2008, Mr. Harder acted more responsibly than any of his \u2018peers.\u2019\u201d President Trump commends Mr. Harder for choosing to put his employees, investors, and the senior citizens residing in Sunwest\u2019s homes above himself.\n\nScott Conor Crosby \u2013 President Trump granted a full pardon to Scott Conor Crosby. Mr. Crosby is supported by Senator Martha McSally, the Mayor and Vice Mayor of Mesa, Arizona, and the Bishop of his church, all of whom attest to Mr. Crosby\u2019s service to his community and upstanding character. In 1992, Mr. Crosby made a \u201c\u2018spur of the moment\u2019 poor decision\u201d to participate in a co-worker\u2019s plan to commit a bank robbery. Mr. Crosby was arrested the same day and cooperated with the authorities. Since his release from prison, he has spent significant time volunteering at his church, mentoring youth, and has earned a certification as an Emergency Medical Technician. Mr. Crosby\u2019s civil rights were restored by the State of Arizona in 2003, and this action restores his Federal civil rights.\n\nChris Young \u2013 President Trump commuted the remaining sentence of Chris Young. This commutation is supported by the Honorable Kevin H. Sharp, Mr. Young\u2019s sentencing judge, former law enforcement officials and Federal prosecutors, and multitudes of criminal justice reform advocates, including Alice Johnson, Kevin Ring, Jessica Jackson Sloan, Topeka Sam, Amy Povah, the Aleph Institute, Mark Holden, Doug Deason, and David Safavian, among others. Mr. Young, who is 32 years old, has served over 10 years of a 14 year sentence for his role in a drug conspiracy. Although initially sentenced to a mandatory life sentence that Judge Sharp called \u201cnot appropriate in any way, shape, or form,\u201d Mr. Young has made productive use of his time in prison by taking courses and learning coding skills. He also has maintained a spotless disciplinary record. Mr. Young\u2019s many supporters describe him as an intelligent, positive person who takes full responsibility for his actions and who lacked a meaningful first chance in life due to what another Federal judge called an \u201cundeniably tragic childhood.\u201d With this commutation, President Trump provides Mr. Young with a second chance.\n\nAdrianne Miller \u2013 President Trump commuted the remaining sentence of Adrianne Miller. This commutation is supported by former U.S. Attorney Brett Tolman and the Clemency for All Non-Violent Drug Offenders (CAN-DO) Foundation. Ms. Miller has served 6 years of a 15-year sentence after pleading guilty to conspiracy to possess with intent to distribute methamphetamine and possession of a list I chemical. Ms. Miller, who has struggled with drug addiction, has fully committed to rehabilitation while in prison. In addition, she has taken numerous courses including drug education, life management, and has participated in the Life Connections Program, an intensive, multi-phase re-entry program offered by the Bureau of Prisons. She is extremely remorseful, regrets her \u201cdestructive choices\u201d and has taken full responsibility for her actions.\n\nLynn Barney \u2013 President Trump granted a full pardon to Lynn Barney. This pardon is supported by Senator Mike Lee, as well as numerous notable members of the Utah business community. Mr. Barney was sentenced to 35 months in prison for possessing a firearm as a previously convicted felon, after having previously been convicted for distributing a small amount of marijuana. Since his release from prison, Mr. Barney has been a model citizen and has devoted himself to his work and children. He is described by his employer as an exceedingly hard worker and a role model to other employees.\n\nJoshua J. Smith \u2013 President Trump granted a full pardon to Joshua J. Smith. Tennessee Governor Bill Lee, Representative Tim Burchett, Commissioner of the Tennessee Department of Corrections Tony Parker, Director of the Tennessee Bureau of Investigation David Rausch, and numerous other community and faith leaders support the pardon of Mr. Smith. Since his release from prison in 2003 for conspiracy to possess drugs with intent to distribute, Mr. Smith has dedicated his life to his faith and to his community. He is now a successful businessman and has used his financial success to establish Fourth Purpose, a non-profit organization devoted to making prison \u201ca place of transformation.\u201d He has mentored incarcerated individuals and taught business classes to those in prison\u2014including at the prison where he was incarcerated. Mr. Smith has also been heavily involved in mission trips throughout Latin America.\n\nAmy Povah \u2013 President Trump granted a full pardon to Amy Povah, the founder of the CAN-DO (Clemency for All Non-violent Drug Offenders) Foundation. In the 1990s, Ms. Povah served 9 years of a 24 year sentence for a drug offense before President Clinton commuted her remaining prison sentence in 2000. Since her release, she has become a voice for the incarcerated, a champion for criminal justice reform, and was a strong advocate for the passage of the First Step Act. Those assisted by Ms. Povah\u2019s organization include Ms. Adrianne Miller, whose remaining prison sentence the President commuted.\n\nDr. Frederick Nahas \u2013 President Trump granted a full pardon to Frederick Nahas. This pardon is supported by Representative Jeff Van Drew. Dr. Nahas is a talented surgeon with a practice in New Jersey. In the 1990s, Dr. Nahas became aware of a Federal investigation into his billing practices. Although the 6-year investigation uncovered no underlying billing fraud, Dr. Nahas did not fully cooperate and ultimately pled guilty to one count of obstructing justice in a health care investigation. Dr. Nahas spent 1 month in prison in 2003 and has spent the subsequent 18 years working tirelessly to regain the trust and admiration of his patients, colleagues, and community.\n\nDavid Tamman \u2013 President Trump granted a full pardon to David Tamman. Mr. Tamman\u2019s pardon is supported by the Aleph Institute, former Federal Bureau of Investigation Director Louis Freeh, and former United States Attorney Kendall Coffey. Mr. Tamman was a partner at a major American law firm when he doctored financial documents that were the subject of a Federal investigation. These actions were done at the behest of a client who was perpetrating a Ponzi scheme upon unsuspecting investors. Mr. Tamman was convicted of his crimes following a bench trial and completed his seven-year sentence in 2019. Mr. Tamman accepts full responsibility for his actions and numerous friends and colleagues have attested that he is a decent man who experienced a terrible lapse in judgment for which he has already paid a significant price.\n\nDr. Faustino Bernadett \u2013 President Trump granted a full pardon to Dr. Faustino Bernadett. In approximately early 2008, Dr. Bernadett failed to report a hospital kickback scheme of which he became aware. Notably, he was not part of the underlying scheme itself, and unaffiliated himself with the hospital shortly thereafter. This conviction is the only major blemish on Dr. Bernadett\u2019s record. Although now retired, Dr. Bernadett has spent the past year devoted to helping protect his community from COVID-19, including by: procuring PPE and medical supplies for nurses; advising hospitals on expanding patient capacity and continuing prenatal services; identifying care facilities for first responders and the homeless; providing meals and books to underprivileged students; funding online educational resources for a distressed Catholic elementary school in Dr. Bernadett\u2019s neighborhood; and helping to ensure that senior citizens maintain social connections by training volunteer callers to speak with nursing home residents. In addition, Dr. Bernadett has been deeply involved in philanthropic efforts in his community and he has supported numerous non-profits that provide help to underprivileged communities, support medical research, and promote youth education programs. President Trump determined that it is in the interests of justice and Dr. Bernadett\u2019s community that he may continue his volunteer and charitable work.\n\nPaul Erickson \u2013 President Trump has issued a full pardon to Paul Erikson. This pardon is supported by Kellyanne Conway. Mr. Erickson\u2019s conviction was based off the Russian collusion hoax. After finding no grounds to charge him with any crimes with respect to connections with Russia, he was charged with a minor financial crime. Although the Department of Justice sought a lesser sentence, Mr. Erickson was sentenced to 7 years\u2019 imprisonment\u2014nearly double the Department of Justice\u2019s recommended maximum sentence. This pardon helps right the wrongs of what has been revealed to be perhaps the greatest witch hunt in American History.\n\nKwame Kilpatrick \u2013 President Trump commuted the sentence of the former Mayor of Detroit, Kwame Malik Kilpatrick. This commutation is strongly supported by prominent members of the Detroit community, Alveda King, Alice Johnson, Diamond and Silk, Pastor Paula White, Peter Karmanos, Representative Sherry Gay-Dagnogo of the Michigan House of Representatives, Representative Karen Whitsett of the Michigan House of Representatives, and more than 30 faith leaders. Mr. Kilpatrick has served approximately 7 years in prison for his role in a racketeering and bribery scheme while he held public office. During his incarceration, Mr. Kilpatrick has taught public speaking classes and has led Bible Study groups with his fellow inmates.\n\nFred \u201cDave\u201d Clark \u2013 President Trump commuted Dave Clark\u2019s remaining term of incarceration after serving over 6 years in Federal prison for a first-time, non-violent offense. Mr. Clark\u2019s commutation is supported by Professor Alan Dershowitz, Ken Starr, the Aleph Institute, his family of seven children, and former business colleagues and investors, among others. While in prison, Mr. Clark has lead Bible Study and developed a \u201cPromising People\u201d program to teach inmates technical skills and connect them with faith-based support.\n\nTodd Farha, Thaddeus Bereday, William Kale, Paul Behrens, Peter Clay \u2013 President Trump granted full pardons to Todd Farha, Thaddeus Bereday, William Kale, Paul Behrens, and Peter Clay, former executives of a healthcare maintenance organization. Widely cited as a case study in overcriminalization, these men have attracted a broad range of support, including from the CATO Institute, the Reason Foundation, the National Association of Criminal Defense Lawyers, and various scholars and law professors. In 2008, Messrs. Farha, Bereday, Kale, Behrens, and Clay were criminally prosecuted for a state regulatory matter involving the reporting of expenditures to a state health agency. The expenditures reported were based on actual monies spent, and the reporting methodology was reviewed and endorsed by those with expertise in the state regulatory scheme. Notably, there was no evidence that any of the individuals were motivated by greed. And in fact, the sentencing judge called the likelihood that there was any personal financial motivation \u201cinfinitesimal.\u201d The judge imposed a range of sentences from probation to 3 years\u2019 imprisonment, reflecting the conduct as an aberration from these individuals\u2019 otherwise law-abiding lives. Messrs. Farha, Bereday, Kale, Behrens, and Clay are described as devoted to their family and their communities, and have weathered their convictions without complaint.\n\nDavid Rowland \u2013 President Trump granted a full pardon to David Rowland. This pardon is supported by Senator Lindsey Graham. Mr. Rowland\u2019s asbestos removal license had lapsed when he agreed to remove asbestos found in an elementary school. He completed the work in compliance with all other regulations, but received 2 years\u2019 probation for a violation of the Clean Air Act. Mr. Rowland accepts responsibility and is remorseful. In addition, he has given back to his community by continuing to work with the Make-A-Wish Foundation after the completion of his mandatory community service.\n\nRandall \u201cDuke\u201d Cunningham \u2013 President Trump granted a conditional pardon to Randall \u201cDuke\u201d Cunningham who was released from prison in 2013. Former Speaker of the House Newt Gingrich strongly supports this pardon. Mr. Cunningham, a former California Congressman, was sentenced to over 8 years\u2019 imprisonment for accepting bribes while he held public office. During his time in prison, Mr. Cunningham tutored other inmates to help them achieve their GED. Mr. Cunningham is a combat veteran, an ace fighter pilot, and a member of the Military Order of Purple Hearts. Although combat-disabled, he continues to serve his community by volunteering with a local fire department and is active in Bible Study.\n\nWilliam Walters \u2013 President Trump commuted the sentence of William Walters. This commutation is supported by former Majority Leader Harry Reid, former Governor Jim Gibbons, former Representative Shelley Berkley, former Clark County Sheriff William Young, former Police Commissioner Bernie Kerik, numerous professional golfers including Butch Harmon, David Feherty, Peter Jacobsen, and Phil Mickelson, and former 60 minutes correspondent Lara Logan. Mr. Walters was sentenced to 5 years imprisonment for insider trading. Since his conviction, Mr. Walters has served nearly 4 years of his prison sentence and has paid $44 million in fines, forfeitures, and restitution. In addition to his established reputation in the sports and gaming industry, Mr. Walters is well known for his philanthropic efforts and was previously named Las Vegas\u2019 Philanthropist of the Year.\n\nDwayne Michael Carter Jr. \u2013 President Trump granted a full pardon to Dwayne Michael Carter Jr., also known as \u201cLil Wayne.\u201d Mr. Carter pled guilty to possession of a firearm and ammunition by a convicted felon, owing to a conviction over 10 years ago. Brett Berish of Sovereign Brands, who supports a pardon for Mr. Carter, describes him as \u201ctrustworthy, kind-hearted and generous.\u201d Mr. Carter has exhibited this generosity through commitment to a variety of charities, including donations to research hospitals and a host of foodbanks. Deion Sanders, who also wrote in support of this pardon, calls Mr. Wayne \u201ca provider for his family, a friend to many, a man of faith, a natural giver to the less fortunate, a waymaker, [and] a game changer.\u201d\n\nStephen Odzer \u2013 President Trump granted a conditional pardon to Stephen Odzer. This pardon is supported by former Acting Attorney General Matthew Whitaker, Sigmund \u201cSig\u201d Rogich, Jason Greenblatt, Michael Steinhardt, Wayne Allyn Root, Salvador Moran, the Aleph Institute, and numerous members of Mr. Odzer\u2019s religious community. Mr. Odzer pled guilty to conspiracy and bank fraud, for which he was sentenced to 18 months in prison. Numerous individuals testify to his substantial philanthropic and volunteer activities. His philanthropic endeavors include providing personal protective equipment to front-line workers in New York City hospitals; visiting sick children in hospitals; and donating religious materials to prison inmates and U.S. Service Members around the world. He has also dedicated resources to support and build synagogues in memory of his late cousin who was kidnapped and killed by Muslim terrorists while in Israel. The pardon requires Mr. Odzer to pay the remainder of his restitution order.\n\nJames Brian Cruz \u2013 President Trump commuted the remaining sentence of James Brian Cruz. Mr. Cruz\u2019s many supporters include Alice Johnson, Dr. Robert Jeffress, Pastor of the First Baptist Church in Dallas, Texas, Kelly Shackelford of the First Liberty Institute, several former inmates who Mr. Cruz mentored or ministered, Mr. Cruz\u2019s work supervisor, and several business owners and managers. Mr. Cruz, who has served approximately half of a 40-year sentence for a drug crime, has truly reformed and has worked to better his life and the lives of other inmates while in prison. Several former inmates credit Mr. Cruz, whom they met while incarcerated, as someone who helped changed their life, as \u201ca great source of comfort\u201d for many, and one who helps others without looking for anything in return. Mr. Cruz\u2019s work supervisor describes him as a dependable and hard-working employee, who has \u201cgained the respect of many staff workers and inmates alike\u201d and who helps arguing inmates \u201cmake peace.\u201d Mr. Cruz writes that he recognizes the effect drugs have on people, families, and the community, and desires a second chance to \u201clive life as one who upholds the law, and lives to help others.\u201d\n\nSteven Benjamin Floyd \u2013 President Trump granted a full pardon to Steven Benjamin Floyd. This pardon is supported by Representative Mark Green. Mr. Floyd joined the United States Marines Corps at age 17 and earned a combat action ribbon in Iraq. He pled guilty to one count of bank robbery by extortion. Since his release from prison in 2009, Mr. Floyd has exemplified the power of second chances, and is raising a family and owns a successful car repair business. Mr. Floyd\u2019s dedication to service includes helping extinguish fires set during the recent unrest and repairing widows and disabled veterans\u2019 cars free of charge. President Trump thanks Mr. Floyd for his past military service and for his commitment to his community.\n\nJoey Hancock \u2013 President Trump granted a full pardon to Joey Hancock. Senator Roger Wicker, and Mr. Hancock\u2019s employer, pastor, and other members of his community all support this pardon. Mr. Hancock was convicted for conspiracy to possess with intent to distribute a controlled substance. Following his release from prison, Mr. Hancock has been a hard-working employee and active in his church and community.\n\nDavid E. Miller \u2013 President Trump granted a full pardon to David E. Miller. Governor Bill Lee, Mr. Miller\u2019s employer, and numerous colleagues support this pardon. In 2015, Mr. Miller pled guilty to one count of making a false statement to a bank. Today, Mr. Miller is the development director for the charitable organization Men of Valor, where he helps previously incarcerated men rebuild relationships with their faith, family, and society. Governor Lee describes Mr. Miller as having \u201cembraced the ministry\u2019s work and [has] committed himself to doing right and serving others.\u201d\n\nJames Austin Hayes \u2013 President Trump granted a full pardon to James Austin Hayes. Mr. Hayes\u2019s pardon is supported by Paula White, Rick Hendrick of Hendrick Motorsports, and NASCAR legend Jeff Gordon. Nearly 10 years ago, Mr. Hayes was convicted of conspiracy to commit insider trading. Mr. Hayes cooperated immediately and extensively and disgorged all profits he earned in a related civil action. Since his conviction, Mr. Hayes has been active in his church and his community.\n\nDrew Brownstein \u2013 President Trump granted a full pardon to Drew Brownstein, who, other than this conviction, was described by his sentencing judge as someone who \u201cgoes out of his way to help people that are less fortunate.\u201d This pardon is supported by the Assistant Attorney General for the Antitrust Division, Makan Delrahim, and several of Mr. Brownstein\u2019s friends and family. Mr. Brownstein was convicted of insider trading and has since paid his fines and forfeitures in full. Both before and after his conviction, Mr. Brownstein has volunteered extensively as a youth coach with the Boys & Girls club in Denver and the Jewish Family Services of Colorado.\n\nRobert Bowker \u2013 President Trump granted a full pardon to Robert Bowker. Mr. Bowker\u2019s pardon is supported by Ann Marie Pallan, Sherriff Butch Anderson, and the late Robert Trump. Nearly 30 years ago, Mr. Bowker pled guilty to a violation the Lacey Act, which prohibits trafficking in wildlife, when he arranged for 22 snakes owned by Rudy \u201cCobra King\u201d Komarek to be transported to the Miami Serpentarium. Although he did not ask for any animals in return, he was offered 22 American alligators. After pleading guilty, Mr. Bowker was sentenced to probation. Mr. Bowker has dedicated resources to animal conservation efforts in the intervening decades, including as a member of the Humane Society of the United States, World Wildlife Fund, and Wildlife Conservation Society.\n\nAmir Khan \u2013 President Trump granted a full pardon to Amir Khan. This pardon is supported by his adult children and members of the community. Mr. Khan pled guilty to wire fraud. Notably, he immediately paid back the victim more than in full and has demonstrated remorse for his conduct. Prior to the pandemic, Mr. Khan volunteered at the organization 3 Square Meals, and has regularly donated to charities including St. Jude Children\u2019s Hospital, Boys Town, Covenant House, Tunnel to Towers Foundation, and the Salvation Army.\n\nShalom Weiss \u2013 President Trump commuted the sentence of Shalom Weiss. This commutation is supported by former U.S. Attorney General Edwin Meese, former Solicitors General Ken Starr and Seth Waxman, former United States Representative Bob Barr, numerous members of the New York legislature, notable legal figures such as Professor Alan Dershowitz and Jay Sekulow, former U.S. Attorney Brett Tolman, and various other former elected officials. Mr. Weiss was convicted of racketeering, wire fraud, money laundering, and obstruction of justice, for which he has already served over 18 years and paid substantial restitution. He is 66 years old and suffers from chronic health conditions.\n\nSalomon Melgen \u2013 President Trump commuted the sentence of Salomon Melgen. This commutation is supported by Senator Bob Menendez, Representative Mario Diaz-Balart, numerous members of Brigade 2506, Col. Mark D. Holten, as well as his friends, family, and former employees. Dr. Melgen was convicted of healthcare fraud and false statements. Numerous patients and friends testify to his generosity in treating all patients, especially those unable to pay or unable to afford healthcare insurance.\n\nPatrick Lee Swisher \u2013 President Trump granted a full pardon to Patrick Lee Swisher. This pardon is supported by Representative Dan Bishop, Rick Hendrick, and numerous business associates. Mr. Swisher was convicted of tax fraud and false statements. After his release from prison, Mr. Swisher started a successful business that employs over 1000 individuals. He also is involved in a religious non-profit organization that provides college scholarships to those in his community. In addition, he has mentored former felons and helped them re-integrate into society.\n\nRobert Sherrill \u2013 President Trump granted a full pardon to Robert Sherrill. Mr. Sherrill was convicted of conspiracy to distribute and possession with intent to distribute cocaine. Mr. Sherrill has taken full responsibility for his criminal past and received treatment for his drug addiction. He started a commercial cleaning business as well as a non-profit organization that mentors at-risk youth.\n\nDr. Robert S. Corkern \u2013 President Trump granted a full pardon to Robert S. Corkern. This pardon is supported by Senators Roger Wicker and Cindy Hyde-Smith, Governor Phil Bryant, and Dr. Michael Mansour. Dr. Corkern was convicted of Federal program bribery. This pardon will help Dr. Corkern practice medicine in his community, which is in dire need of more doctors as it has struggled to keep up with demand for emergency services. Dr. Corkern served in the Mississippi Army National Guard and has generously provided his services to low-income patients.\n\nDavid Lamar Clanton \u2013 President Trump granted a full pardon to David Lamar Clanton. This pardon is supported by Senator Roger Wicker, Alton Shaw, Mark Galtelli, and Terri Rielley. Mr. Clanton was convicted of false statements and related charges. Mr. Clanton\u2019s supporters testify to his contributions to the community, especially with respect to issues surrounding rural healthcare. Mr. Clanton has been active with 4-H Clubs and other organizations in his community.\n\nGeorge Gilmore \u2013 President Trump granted a full pardon to George Gilmore. This pardon is supported by Bill Stepien, former Governor Chris Christie, James McGreevey, James Florio, Donald DiFrancesco, John Bennett, Kimberly Guadagno, Thomas MacArthur, Gerald Cardinale, Michael Testa, Jr., David Avella, Joseph Buckelew, Lawrence Bathgate II, Larry Weitzner, and Adam Geller. Mr. Gilmore was convicted for failure to pay payroll taxes and false statements. Mr. Gilmore has made important civic contributions over his career in New Jersey.\n\nDesiree Perez \u2013 President Trump granted a full pardon to Desiree Perez. Ms. Perez was involved in a conspiracy to distribute narcotics. Since her conviction, Ms. Perez has taken full accountability for her actions and has turned her life around. She has been gainfully employed and has been an advocate for criminal justice reform in her community.\n\nRobert \u201cBob\u201d Zangrillo \u2013 President Trump granted a full pardon to Robert Zangrillo. This pardon is supported by Len Blavatnik, Geoff Palmer, Tom Barrack, Sean Parker, Walid Abu-Zalaf, Medo Alsaloussi, and Kevin Downing. Mr. Zangrillo was charged in connection with the Varsity Blues investigation. However, his daughter did not have others take standardized tests for her and she is currently earning a 3.9 GPA at the University of Southern California. Mr. Zangrillo is a well-respected business leader and philanthropist.\n\nHillel Nahmad \u2013 President Trump granted a full pardon to Hillel Nahmad. This pardon is supported by members of his community. Mr. Nahmad was convicted of a sports gambling offense. Since his conviction, he has lived an exemplary life and has been dedicated to the well-being of his community.\n\nBrian McSwain \u2013 The President granted a full pardon to Brian McSwain. This pardon is supported by Senator Lindsey Graham, two former United States Attorneys for the District of South Carolina, and other former law enforcement officers. Since serving his 18 month sentence for a drug crime committed in the early 1990s, Mr. McSwain has been gainfully employed and has been passed over for several promotion opportunities due to his felony conviction.\n\nJohn Duncan Fordham \u2013 President Trump granted a full pardon to John Duncan Fordham. Mr. Fordham was convicted on one count of health care fraud. A judge later dismissed the conspiracy charge against him.\n\nWilliam \u201cEd\u201d Henry \u2013 President Trump granted a full pardon to William \u201cEd\u201d Henry of Alabama. This pardon is supported by Senator Tommy Tuberville. Mr. Henry was sentenced to 2 years\u2019 probation for aiding and abetting the theft of government property and paid a $4,000 fine.\n\nIn addition, President Trump commuted the sentences to time served for the following individuals: Jeff Cheney, Marquis Dargon, Jennings Gilbert, Dwayne L. Harrison, Reginald Dinez Johnson, Sharon King, and Hector Madrigal, Sr."
        },
        {
            "authors": [
                "Benjamin M. Friedman"
            ],
            "title": "How Religion Shaped Modern Economics",
            "contents": "The conventional account of modern thinking about economics starts with Adam Smith\u2019s 1776 book \u201cThe Wealth of Nations.\u201d Smith\u2019s insights, in turn, are usually said to be a product of the secularism of the Enlightenment\u2014a historic turn from thinking in terms of a God-centered universe toward ideals of human rationality and self-sufficiency.\n\nBut that story is seriously incomplete. In fact, our modern Western understanding of market competition as the key to economic progress owes a great deal to religion\u2014specifically, the new ideas that emerged in the English-speaking Protestant world in the late 17th and the 18th century. Critics of capitalism sometimes complain that the belief in competitive markets, among economists and many ordinary citizens too, is a form of religion. There is something to the idea, though not in the way the critics mean.\n\nThe idea that there is a connection between religion and capitalism isn\u2019t new. The German sociologist Max Weber famously argued that religious belief had been an important spur to the growth of capitalism. Weber pointed in particular to the Calvinist notion of predestination\u2014the belief that God decided whether each individual would be saved or damned before the person was even born. Driven by anxiety over their ultimate fate, Weber theorized, believers sought to convince themselves that they were among God\u2019s \u201celect\u201d by displaying virtues like thrift, industriousness and individual initiative. In this way, a \u201cProtestant ethic\u201d emerged that enabled the rise of modern capitalism.\n\nFor purposes of economic thinking, however, Weber had things exactly backward. Belief in predestination actually hindered the emergence of modern capitalism\u2019s key idea\u2014that human beings can rationally advance their own and others\u2019 economic condition. After all, if it was impossible for individuals to make any choice or take any action to promote their ultimate spiritual prospects, how could they make choices or take action to improve their well-being in this world?\n\nBy Adam Smith\u2019s time, however, Protestant thought had turned away from the idea of predestination. The English and Scottish clergy began to preach instead that men and women are endowed with reason and that human character is inherently good. John Tillotson, the Archbishop of Canterbury in the late 17th century, argued that far from being predestined to heaven or hell, people can \u201ccooperate\u201d with God in effecting their salvation. As the philosopher John Locke put it in a famous metaphor, people have \u201cthe candle of the Lord\u201d by which to see and then act."
        },
        {
            "authors": [
                "Matthew S. Schwartz"
            ],
            "title": "Paradox-Free Time Travel Is Theoretically Possible, Researchers Say",
            "contents": "Paradox-Free Time Travel Is Theoretically Possible, Researchers Say\n\nEnlarge this image toggle caption Timothy A. Clary/AFP via Getty Images Timothy A. Clary/AFP via Getty Images\n\n\"The past is obdurate,\" Stephen King wrote in his book about a man who goes back in time to prevent the Kennedy assassination. \"It doesn't want to be changed.\"\n\nTurns out, King might have been on to something.\n\nCountless science fiction tales have explored the paradox of what would happen if you went back in time and did something in the past that endangered the future. Perhaps one of the most famous pop culture examples is in Back to the Future, when Marty McFly goes back in time and accidentally stops his parents from meeting, putting his own existence in jeopardy.\n\nBut maybe McFly wasn't in much danger after all. According a new paper from researchers at the University of Queensland, even if time travel were possible, the paradox couldn't actually exist.\n\nResearchers ran the numbers and determined that even if you made a change in the past, the timeline would essentially self-correct, ensuring that whatever happened to send you back in time would still happen.\n\n\"Say you traveled in time in an attempt to stop COVID-19's patient zero from being exposed to the virus,\" University of Queensland scientist Fabio Costa told the university's news service.\n\n\"However, if you stopped that individual from becoming infected, that would eliminate the motivation for you to go back and stop the pandemic in the first place,\" said Costa, who co-authored the paper with honors undergraduate student Germain Tobar.\n\n\"This is a paradox \u2014 an inconsistency that often leads people to think that time travel cannot occur in our universe.\"\n\nA variation is known as the \"grandfather paradox\" \u2014 in which a time traveler kills their own grandfather, in the process preventing the time traveler's birth.\n\nThe logical paradox has given researchers a headache, in part because according to Einstein's theory of general relativity, \"closed timelike curves\" are possible, theoretically allowing an observer to travel back in time and interact with their past self \u2014 potentially endangering their own existence.\n\nBut these researchers say that such a paradox wouldn't necessarily exist, because events would adjust themselves.\n\nTake the coronavirus patient zero example. \"You might try and stop patient zero from becoming infected, but in doing so, you would catch the virus and become patient zero, or someone else would,\" Tobar told the university's news service.\n\nIn other words, a time traveler could make changes, but the original outcome would still find a way to happen \u2014 maybe not the same way it happened in the first timeline but close enough so that the time traveler would still exist and would still be motivated to go back in time.\n\n\"No matter what you did, the salient events would just recalibrate around you,\" Tobar said.\n\nThe paper, \"Reversible dynamics with closed time-like curves and freedom of choice,\" was published last week in the peer-reviewed journal Classical and Quantum Gravity. The findings seem consistent with another time travel study published this summer in the peer-reviewed journal Physical Review Letters. That study found that changes made in the past won't drastically alter the future.\n\nBestselling science fiction author Blake Crouch, who has written extensively about time travel, said the new study seems to support what certain time travel tropes have posited all along.\n\n\"The universe is deterministic and attempts to alter Past Event X are destined to be the forces which bring Past Event X into being,\" Crouch told NPR via email. \"So the future can affect the past. Or maybe time is just an illusion. But I guess it's cool that the math checks out.\"",
            "published_at": "2020-09-27T00:00:00"
        },
        {
            "authors": [],
            "title": "Flow-based Programming",
            "contents": "In computer programming, Flow-Based Programming (FBP) is a programming paradigm, discovered/invented by J. Paul Rodker Morrison in the late '60s, that uses a \"data processing factory\" metaphor for designing and building applications. FBP defines applications as networks of \"black box\" processes, which communicate via data chunks (called Information Packets) travelling across predefined connections (think \"conveyor belts\"), where the connections are specified externally to the processes. These black box processes can be reconnected endlessly to form different applications without having to be changed internally. FBP is thus naturally component-oriented.\n\nFBP is a special case of dataflow programming characterized by asynchronous, concurrent processes \"under the covers\", Information Packets with defined lifetimes, named ports, \"bounded buffer\" connections, and definition of connections external to the components - it has been found to support improved development time and maintainability, reusability, rapid prototyping, simulation, improved performance, and good communication among developers, maintenance staff, users, systems people, and management - not to mention that FBP naturally takes advantage of multiple cores... without the programmer having to struggle with the intricacies of multitasking!\n\nFBP exhibits \"data coupling\", described in the article on coupling as the loosest type of coupling between components. The concept of loose coupling is in turn related to that of service-oriented architectures, and FBP fits a number of the criteria for such an architecture, albeit at a more fine-grained level than most examples of this architecture.\n\nFBP promotes high-level, functional style of specifications that simplify reasoning about system behavior. An example of this is the distributed data flow model for constructively specifying and analyzing the semantics of distributed multi-party protocols.\n\nFBP involves a significant \"paradigm shift\" from conventional programming, as it uses an \"assembly line\" image of data processing, which really has to be used to experience its advantages. Not surprisingly given the hold that the so-called \"von Neumann\" paradigm has over programming technology, over the last 40+ years a number of companies have come out with FBP-like or \"FBP-inspired\" systems (to borrow a term coined by Joe Witt of Cloudera), which do not involve this paradigm shift, including some of the biggest and most successful companies in the industry, such as IBM, Microsoft, Facebook, Google, and the New York Times (see FBP External References). Unfortunately, without the FBP paradigm shift, these systems cannot fully achieve the advantages listed above, although most of them combine a visual notation with some degree of componentry. We have therefore been forced to use the term \"classical FBP\" to refer to the FBP architecture described in this web site. It is hard for someone reading documentation to tell whether a given product calling itself \"FBP- like\" is \"classical\" or not, but, roughly, multiple input ports, \"back pressure\", and asynchronism, constitute a sort of litmus test to check whether systems should be classed as \"classical\" FBP systems. While the FBP paradigm shift may cause some trouble at first, given the prevalence of the older programming approaches, in fact it embodies a simple mental model (not \"simple\" as in easy but \"simple\" as in elegant, to quote Joe Witt), and has proven over several decades to result in applications that are more understandable and maintainable than those built using the conventional (von Neumann) paradigm.\n\nIf you have come across Flow-Based Programming by way of NoFlo, Flowhub, Node-RED, or similar packages, it is recommended that you read FBP vs. FBP-inspired Systems, which attempts to highlight the differences and similarities between FBP proper, or \"classical\" FBP, (the subject of this web site) on the one hand, and NoFlo and other similar \"FBP-inspired\" systems on the other.\n\nFBP is described in J. Paul Morrison's book, Flow-Based Programming: A New Approach to Application Development\", now in its 2nd edition, which can be obtained in paperback form from Amazon, or in 2 e-book formats: Kindle and epub.\n\nFurther down on this web page you will find a bunch of useful links, including some videos demonstrating the use of the FBP diagramming tool, DrawFBP. During FBP's early days, networks were coded by hand, and initially we used pencil and paper, followed by more sophisticated graphical tools, whether or not they played well with FBP software. However, with the advent of good graphical support, we now have a powerful graphical tool (DrawFBP), and in recent years the emphasis has been shifting to doing the diagramming on-line, and then generating the network code (we can see this progression with other flow tools on the market). Now that compile and run functions (for Java and C#) have been added to DrawFBP, the developer can do much of the development graphically, and on-line.\n\nWe are in the process of \"growing\" a tutorial which will introduce the reader to the basic concepts of FBP by means of a series of \"steps\", which use the diagramming tool DrawFBP to progressively expand an application diagram and generate and run code at many of those steps.\n\nThere is a Google group on FBP, with almost 800 members, and a Slack channel has recently been set up - here is the invitation link, if you are interested in joining: Slack invitation.\n\nFBP Tenets (open-ended)\n\nIf work is boring, let a robot do it!\n\nCorollary: using a design tool should be fun!\n\nNo one language should try to do everything !\n\n! The world is asynchronous - don't try to force the systems we build into a synchronous framework!\n\nThe von Neumann paradigm is only appropriate within a single process! Duh!\n\nprocess! Duh! A process needs to be able to have more than one input port, and needs to be able to choose which one to receive from!\n\nIf a process's upstream connection fills up, the process feeding it will be suspended (unless the connection is marked \"DropOldest\") (\"back pressure\")\n\nLinks\n\nVideo Interview with J. Paul Rodker Morrison, Summer 2013\n\nIn addition, there are six Youtube videos about DrawFBP, illustrating a number of basic (\"classical\") FBP concepts (what we are now calling \"FBP-inspired\" or \"FBP-like\" systems do not necessarily contain all of these, although DrawFBP should be able to support most of these systems):\n\nFinally, a video presentation at a Meetup in Toronto in Sept. 2013 organized by Paul Tarvydas, CEO of Tarvydas-Sanford Controls Inc., and Dann Toliver, also containing a few jokes and reminiscences...\n\nGoogle group on Flow-Based Programming\n\nC2-style Flow-Based Programming Wiki (last maintained in 2010)\n\nMore general FBP Wiki by Vladimir Sibirov, et al., intended to cover both \"classical\" FBP and FBP-inspired approaches (still under development )\n\nFBP Specification wiki - under development by Vladimir Sibirov\n\nTwitter hashtag: #FlowBased\n\nSlack Channel"
        },
        {
            "authors": [
                "Joe Stech",
                "Jan"
            ],
            "title": "Why outsource your auth system?",
            "contents": "You\u2019re a software engineering leader, and you\u2019re great at your job. You know that the optimal path for software development lies in figuring out which components of your design to implement from scratch and which have already been implemented by specialists and can be reused.\n\nYou also know that these aren\u2019t decisions that you can only make once \u2013 you have to keep reevaluating based on environment changes and the needs of new products.\n\nAuthentication is one of those components that you deal with all the time. Auth is a necessary part of any software product, but how you implement auth is not necessarily always the same. Careful consideration is needed, because your decision to outsource will not only impact speed of development, but also long-term product maintenance \u2013 you don\u2019t want to slow down time to market because you re-implemented an entire auth system unnecessarily, but you also don\u2019t want to use an auth system that is going to cause problems down the road.\n\nSo what are the primary considerations when making your decision?\n\nSpeed to market\n\nThis is the most obvious consideration. Depending on the features you need, it could literally take months to implement auth in-house, whereas it could take less than a day to incorporate an outsourced solution.\n\nYou could say \u201cbut what if we only need a bare-bones implementation? Some salted hashes in a database and we\u2019re good to go!\u201d That\u2019s a totally valid point, and if you don\u2019t anticipate needing sophisticated auth features then your best bet might be to do a quick in-house implementation and move on.\n\nHowever, time and time again I\u2019ve seen product developers underestimate the sophistication of features that will be required when their userbases grow. Most of the time development organizations then fall prey to the sunk costs fallacy and double down on augmenting their in-house solution, even when it may be more efficient to abandon the home-grown effort and replace it with an outsourced solution. This will cause huge issues for maintainability, which I\u2019ll talk about further below.\n\nConsequences of an auth breach\n\nPlanning for the worst possible case can prevent total financial ruin for your company or division. If a breach of security happens and PII (Personally Identifiable Information) is leaked from your in-house auth system, it can not only cause your company reputational damage but also significant financial penalties (not to mention potential jail time if you try to cover up the breach).\n\nIf you outsource your auth system you can limit your liability, and also protect your reputation \u2013 if there is a breach on your auth provider\u2019s side, it\u2019s likely that the breach will extend beyond your company. A breach in an outsourced auth provider that is used by many different companies will be big news, and customers will be more likely to forgive you for making a mistake in your choice of auth provider than for implementing a poor auth system yourself.\n\nA not-insignificant addendum is that I believe your in-house system is much more likely to suffer a breach than an outsourced provider who is an expert in security. I have no studies to support this claim, but I have never seen a major auth provider compromised, and I\u2019ve definitely seen companies suffer breaches due to their own in-house auth implementations \u2013 this article discusses a compilation of 21 million plaintext passwords collected from various breaches wherein passwords were not hashed and salted by auth systems.\n\nProperly storing passwords is an incredibly low bar, and yet companies that manage their own auth still do it incorrectly all the time.\n\nConsequences of an auth outage\n\nWhile less damaging than breaches, outages can still cause reputational damage and liability issues if your SLAs make uptime guarantees. Similarly to breaches, if you outsource your auth system it\u2019s likely that any auth outage will extend beyond your company.\n\nAs an example, when Microsoft\u2019s Azure Active Directory (AAD) went down for a good portion of the afternoon late last year, logins for applications across the internet stopped working.\n\nWhen your competitors\u2019 authorization systems are down at the same time yours are, nobody blames you for it, but when you\u2019re the only company having issues, you suffer reputationally. No matter what your outsourced auth system is (FusionAuth, Cognito, AAD, etc), you can be almost certain that you won\u2019t be alone in the event of an outage.\n\nMaintainability\n\nThere are trade-offs here. The benefits to an in-house system include:\n\nYour engineers can design the exact system to fit your needs, and you\u2019ll have unrestricted ability to add very specific features if requirements change over time.\n\nIf the same engineers who built the system are maintaining it, then they\u2019ll have the context required to anticipate issues as they add features.\n\nHowever, the drawbacks can be large:\n\nComplex new features can take significant time to build. Outsourced auth systems likely have these features already built (things like multi-factor authentication, user management interfaces, analytics and audit logs, and brute force hack detection, among others).\n\nWith an in-house solution, you\u2019ll have to budget time to monitor new security threats and patch your system in a rapidly evolving threat landscape.\n\nYou won\u2019t get the benefits of a dedicated team that are constantly improving your auth system. This is actually a bigger deal than it seems, because if you outsource auth then other companies will also be filing issue reports and feature requests on your behalf, so you reap the benefits of those extra eyes as well.\n\nIn the case of mergers or acquisitions, an in-house solution is likely to be terrible at combining different databases of users and managing things like duplicates or incomplete data. Such enterprise identity unification efforts can founder on internal auth systems. FusionAuth, on the other hand, supports modeling different user bases with tenants.\n\nCost of in-house vs outsourced auth\n\nWhen building an in-house auth system your costs are all ostensibly sunk (engineer salaries). However, if building your system in-house delays time to market or prevents creation of other features, the build could cost you a significant amount of real income. There will also be on-going maintenance costs with an in-house solution.\n\nWhen making cost calculations, you should compare:\n\nRevenue lost by slower time-to-market PLUS engineering cost to implement in-house solution PLUS on-going maintenance costs of in-house solution PLUS increased risk of data breach PLUS increased risk of outage PLUS increased risk during a merger or acquisition\n\nvs\n\nThe monthly cost incurred by an outsourced provider PLUS the lack of complete control\n\nWhen evaluating different auth providers, you\u2019ll also want to consider whether an outsourced provider charges on a sliding scale based on number of users or if the cost is fixed. AWS Cognito, for instance, will continue to charge more as your application gains more users. FusionAuth, in contrast, has options that charge a single rate for unlimited users. If your app is small and you don\u2019t expect it to grow much, a sliding scale may be better for you. If you don\u2019t want a large unexpected bill as you gain more users, a provider that allows for fixed costs may be more appealing.\n\nAuth may be unrelated to your core competency\n\nAs a final consideration, you may want to evaluate if your engineers even have the ability to implement a secure in-house auth system without a significant investment in education. This is something that many leaders dismiss, since they have great faith in the intelligence and skill of their people.\n\nHowever, knowledge and intelligence aren\u2019t the same, and just because your engineers are capable of becoming auth experts doesn\u2019t mean you want them to spend the time to do so.\n\nAs an engineering leader, you have a responsibility to ensure that your engineers are spending their time on efforts that will optimally contribute to the long-term success of your organization. Auth is a necessary component, but is it really a differentiator for your application?\n\nOnly you have the context to make the best decisions for your company, but I hope this article has helped you think through some of the considerations involved in outsourcing your auth.",
            "published_at": "2021-01-20T00:00:00"
        },
        {
            "authors": [
                "Maria Temming",
                "Erin Garcia De Jesus",
                "Jake Buehler",
                "Jonathan Lambert",
                "Bruce Bower",
                "Carolyn Gramling",
                "Lisa Grossman",
                "Emily Conover",
                "Hours Ago",
                "Alexandra Witze"
            ],
            "title": "Space station detectors found the source of weird \u2018blue jet\u2019 lightning",
            "contents": "Scientists have finally gotten a clear view of the spark that sets off an exotic type of lightning called a blue jet.\n\nBlue jets zip upward from thunderclouds into the stratosphere, reaching altitudes up to about 50 kilometers in less than a second. Whereas ordinary lightning excites a medley of gases in the lower atmosphere to glow white, blue jets excite mostly stratospheric nitrogen to create their signature blue hue.\n\nBlue jets have been observed from the ground and aircraft for years, but it\u2019s hard to tell how they form without getting high above the clouds. Now, instruments on the International Space Station have spotted a blue jet emerge from an extremely brief, bright burst of electricity near the top of a thundercloud, researchers report online January 20 in Nature.\n\nUnderstanding blue jets and other upper-atmosphere phenomena related to thunderstorms, such as sprites (SN: 6/14/02) and elves (SN: 12/23/95), is important because these events can affect how radio waves travel through the air \u2014 potentially impacting communication technologies, says Penn State space physicist Victor Pasko, who was not involved in the work.\n\nCameras and light-sensing instruments called photometers on the space station observed the blue jet in a storm over the Pacific Ocean, near the island of Nauru, in February 2019. \u201cThe whole thing starts with what I think of as a blue bang,\u201d says Torsten Neubert, an atmospheric physicist at the Technical University of Denmark in Kongens Lyngby. That \u201cblue bang\u201d was a 10-microsecond flash of bright blue light near the top of the cloud, about 16 kilometers high. From that flashpoint, a blue jet shot up into the stratosphere, climbing as high as about 52 kilometers over several hundred milliseconds.\n\nThe spark that generated the blue jet may have been a special kind of short-range electric discharge inside the thundercloud, Neubert says. Normal lightning bolts are formed by discharges between oppositely charged regions of a cloud \u2014 or a cloud and the ground \u2014 many kilometers apart. But turbulent mixing high in a cloud may bring oppositely charged regions within about a kilometer of each other, creating very short but powerful bursts of electric current, Neubert says. Researchers have seen evidence of such high-energy, short-range discharges in pulses of radio waves from thunderstorms detected by ground-based antennas.",
            "published_at": "2021-01-21T18:12:34-05:00"
        },
        {
            "authors": [
                "View My Complete Profile"
            ],
            "title": "Siris: Name Etymologies",
            "contents": "For a rough introduction to my philosophy of blogging, including the Code of Amiability Ito follow on this weblog, please read my fifth anniversary post . I consider blogging to be a very informal type of publishing - like putting up thoughts on your door with a note asking for comments. Nothing in this weblog is done rigorously: it's a forum to let my mind be unruly, a place for jottings and first impressions. Because I consider posts here to be 'literary seedings' rather than finished products, nothing here should be taken as if it were anything more than an attempt to rough out some basic thoughts on various issues. Learning to look at any topic philosophically requires, I think, jumping right in, even knowing that you might be making a fool of yourelf; so that's what I do. My primary interest in most topics is the flow and structure of reasoning they involve rather than their actual conclusions, so most of my posts are about that. If, however, you find me making a clear factual error, let me know; blogging is a great way to get rid of misconceptions."
        },
        {
            "authors": [
                "Jeffrey M. Perkel"
            ],
            "title": "Ten computer codes that transformed science",
            "contents": "Illustration by Pawe\u0142 Jo\u0144ca\n\nIn 2019, the Event Horizon Telescope team gave the world the first glimpse of what a black hole actually looks like. But the image of a glowing, ring-shaped object that the group unveiled wasn\u2019t a conventional photograph. It was computed \u2014 a mathematical transformation of data captured by radio telescopes in the United States, Mexico, Chile, Spain and the South Pole1. The team released the programming code it used to accomplish that feat alongside the articles that documented its findings, so the scientific community could see \u2014 and build on \u2014 what it had done.\n\nIt\u2019s an increasingly common pattern. From astronomy to zoology, behind every great scientific finding of the modern age, there is a computer. Michael Levitt, a computational biologist at Stanford University in California who won a share of the 2013 Nobel Prize in Chemistry for his work on computational strategies for modelling chemical structure, notes that today\u2019s laptops have about 10,000 times the memory and clock speed that his lab-built computer had in 1967, when he began his prizewinning work. \u201cWe really do have quite phenomenal amounts of computing at our hands today,\u201d he says. \u201cTrouble is, it still requires thinking.\u201d\n\nEnter the scientist-coder. A powerful computer is useless without software capable of tackling research questions \u2014 and researchers who know how to write it and use it. \u201cResearch is now fundamentally connected to software,\u201d says Neil Chue Hong, director of the Software Sustainability Institute, headquartered in Edinburgh, UK, an organization dedicated to improving the development and use of software in science. \u201cIt permeates every aspect of the conduct of research.\u201d\n\nScientific discoveries rightly get top billing in the media. But Nature this week looks behind the scenes, at the key pieces of code that have transformed research over the past few decades.\n\nAlthough no list like this can be definitive, we polled dozens of researchers over the past year to develop a diverse line-up of ten software tools that have had a big impact on the world of science. You can weigh in on our choices at the end of the story.\n\nLanguage pioneer: the Fortran compiler (1957)\n\nThe first modern computers weren\u2019t user-friendly. Programming was literally done by hand, by connecting banks of circuits with wires. Subsequent machine and assembly languages allowed users to program computers in code, but both still required an intimate knowledge of the computer\u2019s architecture, putting the languages out of reach of many scientists.\n\nThat changed in the 1950s with the development of symbolic languages \u2014 in particular the \u2018formula translation\u2019 language Fortran, developed by John Backus and his team at IBM in San Jose, California. Using Fortran, users could program computers using human-readable instructions, such as x = 3 + 5. A compiler then turned such directions into fast, efficient machine code.\n\nThis CDC 3600 computer, delivered in 1963 to the National Center for Atmospheric Research in Boulder, Colorado, was programmed with the help of the Fortran compiler.Credit: University Corporation for Atmospheric Research/Science Photo Library\n\nIt still wasn\u2019t easy: in the early days, programmers used punch cards to input code, and a complex simulation might require tens of thousands of them. Still, says Syukuro Manabe, a climatologist at Princeton University in New Jersey, Fortran made programming accessible to researchers who weren\u2019t computer scientists. \u201cFor the first time, we were able to program [the computer] by ourselves,\u201d Manabe says. He and his colleagues used the language to develop one of the first successful climate models.\n\nNow in its eighth decade, Fortran is still widely used in climate modelling, fluid dynamics, computational chemistry \u2014 any discipline that involves complex linear algebra and requires powerful computers to crunch numbers quickly. The resulting code is fast, and there are still plenty of programmers who know how to write it. Vintage Fortran code bases are still alive and kicking in labs and on supercomputers worldwide. \u201cOld-time programmers knew what they were doing,\u201d says Frank Giraldo, an applied mathematician and climate modeller at the Naval Postgraduate School in Monterey, California. \u201cThey were very mindful of memory, because they had so little of it.\u201d\n\nSignal processor: fast Fourier transform (1965)\n\nWhen radioastronomers scan the sky, they capture a cacophony of complex signals changing with time. To understand the nature of those radio waves, they need to see what those signals look like as a function of frequency. A mathematical process called a Fourier transform allows researchers to do that. The problem is that it\u2019s inefficient, requiring N2 calculations for a data set of size N.\n\nIn 1965, US mathematicians James Cooley and John Tukey worked out a way to accelerate the process. Using recursion, a \u2018divide and conquer\u2019 programming approach in which an algorithm repeatedly reapplies itself, the fast Fourier transform (FFT) simplifies the problem of computing a Fourier transform to just N log 2 (N) steps. The speed improves as N grows. For 1,000 points, the speed boost is about 100-fold; for 1 million points, it\u2019s 50,000-fold.\n\nThe \u2018discovery\u2019 was actually a rediscovery \u2014 the German mathematician Carl Friedrich Gauss worked it out in 1805, but he never published it, says Nick Trefethen, a mathematician at the University of Oxford, UK. But Cooley and Tukey did, opening applications in digital signal processing, image analysis, structural biology and more. \u201cIt\u2019s really one of the great events in applied mathematics and engineering,\u201d Trefethen says. FFT has been implemented many times in code. One popular option is called FFTW, the \u2018fastest Fourier transform in the west\u2019.\n\nA night view of part of the Murchison Widefield Array, a radio telescope in Western Australia that uses fast Fourier transforms for data collection.Credit: John Goldsmith/Celestial Visions\n\nPaul Adams, who directs the molecular biophysics and integrated bioimaging division at Lawrence Berkeley National Laboratory in California, recalls that when he refined the structure of the bacterial protein GroEL in 19952, the calculation took \u201cmany, many hours, if not days\u201d, even with the FFT and a supercomputer. \u201cTrying to do those without the FFT, I don\u2019t even know how we would have done that realistically,\u201d he says. \u201cIt would have just taken forever.\u201d\n\nMolecular cataloguers: biological databases (1965)\n\nDatabases are such a seamless component of scientific research today that it can be easy to overlook the fact that they are driven by software. In the past few decades, these resources have ballooned in size and shaped many fields, but perhaps nowhere has that transformation been more dramatic than in biology.\n\nToday\u2019s massive genome and protein databases have their roots in the work of Margaret Dayhoff, a bioinformatics pioneer at the National Biomedical Research Foundation in Silver Spring, Maryland. In the early 1960s, as biologists worked to tease apart proteins\u2019 amino acid sequences, Dayhoff began collating that information in search of clues into evolutionary relationships between different species. Her Atlas of Protein Sequence and Structure, first published in 1965 with three co-authors, described what was then known of the sequences, structures and similarities of 65 proteins. The collection was the first that \u201cwas not tied to a specific research question\u201d, historian Bruno Strasser wrote in 20103. And it encoded its data in punch cards, which made it possible to expand the database and search it.\n\nOther computerized biological databases followed. The Protein Data Bank, which today details more than 170,000 macromolecular structures, went live in 1971. Russell Doolittle, an evolutionary biologist at the University of California, San Diego, created another protein database called Newat in 1981. And 1982 saw the release of the database that would become GenBank, the DNA archive maintained by the US National Institutes of Health.\n\nThe Protein Data Bank has an archive of more than 170,000 molecular structures including this bacterial \u2018expressome\u2019, which combines the processes of RNA and protein synthesis.Credit: David S. Goodsell and RCSB PDB (CC BY 4.0)\n\nSuch resources proved their worth in July 1983, when separate teams led by Michael Waterfield, a protein biochemist at the Imperial Cancer Research Fund in London, and Doolittle independently reported a similarity between the sequences of a particular human growth factor and a protein in a virus that causes cancer in monkeys. The observation suggested a mechanism for oncogenesis-by-virus \u2014 that by mimicking a growth factor, the virus induces uncontrolled growth of cells4. \u201cThat set the light bulb off in some of the minds of biologists who were not into computers and statistics,\u201d says James Ostell, former director of the US National Center for Biotechnology Information (NCBI): \u201cWe can understand something about cancer from comparing sequences.\u201d\n\nBeyond that, Ostell says, the discovery marked \u201can advent of objective biology\u201d. In addition to designing experiments to test specific hypotheses, researchers could mine public data sets for connections that might never have occurred to those who actually collected the data. That power grows drastically when different data sets are linked together \u2014 something NCBI programmers achieved in 1991 with Entrez, a tool that allows researchers to freely navigate from DNA to protein to literature and back.\n\nStephen Sherry, current acting director of the NCBI in Bethesda, Maryland, used Entrez as a graduate student. \u201cI remember at the time thinking it was magic,\u201d he says.\n\nForecast leader: the general circulation model (1969)\n\nAt the close of the Second World War, computer pioneer John von Neumann began turning computers that a few years earlier had been calculating ballistics trajectories and weapon designs towards the problem of weather prediction. Up until that point, explains Manabe, \u201cweather forecasting was just empirical\u201d, using experience and hunches to predict what would happen next. Von Neumann\u2019s team, by contrast, \u201cattempted to do numerical weather prediction based upon laws of physics\u201d.\n\nThe equations had been known for decades, says Venkatramani Balaji, head of the Modeling Systems Division at the National Oceanographic and Atmospheric Administration\u2019s Geophysical Fluid Dynamics Laboratory in Princeton, New Jersey. But early meteorologists couldn\u2019t solve them practically. To do so required inputting current conditions, calculating how they would change over a short time period, and repeating \u2014 a process so time-consuming that the mathematics couldn\u2019t be completed before the weather itself caught up. In 1922, the mathematician Lewis Fry Richardson spent months crunching a six-hour forecast for Munich, Germany. The result, according to one history, was \u201cwildly inaccurate\u201d, including predictions that \u201ccould never occur under any known terrestrial conditions\u201d. Computers made the problem tractable.\n\nIn the late 1940s, von Neumann established his weather-prediction team at the Institute for Advanced Study at Princeton. In 1955, a second team \u2014 the Geophysical Fluid Dynamics Laboratory \u2014 began work on what he called \u201cthe infinite forecast\u201d \u2014 that is, climate modelling.\n\nManabe, who joined the climate modelling team in 1958, set to work on atmospheric models; his colleague Kirk Bryan addressed those for the ocean. In 1969, they successfully combined the two, creating what Nature in 2006 called a \u201cmilestone\u201d in scientific computing.\n\nToday\u2019s models can divide the planet\u2019s surface into squares measuring 25 \u00d7 25 kilometres, and the atmosphere into dozens of levels. By contrast, Manabe and Bryan\u2019s combined ocean\u2013atmosphere model5 used 500-km squares and 9 levels, and covered just one-sixth of the globe. Still, says Balaji, \u201cthat model did a great job\u201d, allowing the team to test for the first time the impact of rising carbon dioxide levels in silico.\n\nNumber cruncher: BLAS (1979)\n\nScientific computing typically involves relatively simple mathematical operations using vectors and matrices. There are just a lot of them. But in the 1970s, there was no universally agreed set of computational tools for performing such operations. As a result, programmers working in science would spend their time devising efficient code to do basic mathematics rather than focusing on scientific questions.\n\nWhat the programming world needed was a standard. In 1979, it got one: Basic Linear Algebra Subprograms, or BLAS6. The standard, which continued to evolve up to 1990, defined dozens of fundamental routines for vector and, later, matrix mathematics.\n\nIn effect, BLAS reduced matrix and vector mathematics to a basic unit of computation as fundamental as addition and subtraction, says Jack Dongarra, a computer scientist at the University of Tennessee in Knoxville who was a member of the BLAS development team.\n\nCray-1 supercomputer: before the BLAS programming tool was introduced in 1979, there was no linear algebra standard for researchers working on machines such as the Cray-1 supercomputer at Lawrence Livermore National Laboratory in California.Credit: Science History Images/Alamy\n\nBLAS was \u201cprobably the most consequential interface to be defined for scientific computing\u201d, says Robert van de Geijn, a computer scientist at the University of Texas at Austin. In addition to providing standardized names for common functions, researchers could be sure BLAS-based code would work in the same manner on any computer. The standard also enabled computer manufacturers to optimize BLAS implementations for speedy operation on their hardware.\n\nMore than 40 years on, BLAS represents the heart of the scientific computing stack, the code that makes scientific software tick. Lorena Barba, a mechanical and aerospace engineer at George Washington University in Washington DC, calls it \u201cthe machinery inside five layers of code\u201d.\n\nSays Dongarra, \u201cIt provides the fabric on which we do computing.\u201d\n\nMicroscopy must-have: NIH Image (1987)\n\nIn the early 1980s, programmer Wayne Rasband was working with a brain-imaging lab at the US National Institutes of Health in Bethesda, Maryland. The team had a scanner to digitize X-ray films, but no way to display or analyse them on their computer. So Rasband wrote a program to do just that.\n\nThe program was specifically designed for a US$150,000 PDP-11 minicomputer \u2014 a rack-mounted, decidedly non-personal computer. Then, in 1987, Apple released its Macintosh II, a friendlier and much more affordable option. \u201cIt seemed obvious to me that that would work a lot better as a kind of laboratory image analysis system,\u201d Rasband says. He ported his software to the new platform and rebranded it, seeding an image-analysis ecosystem.\n\nNIH Image and its descendants empowered researchers to view and quantify just about any image, on any computer. The software family includes ImageJ, a Java-based version that Rasband wrote for Windows and Linux users, and Fiji, a distribution of ImageJ developed by Pavel Tomancak\u2019s group at the Max Planck Institute of Molecular Cell Biology and Genetics in Dresden, Germany, that includes key plug-ins. \u201cImageJ is certainly the most foundational tool that we have,\u201d says Beth Cimini, a computational biologist who works on the Imaging Platform of the Broad Institute in Cambridge, Massachusetts. \u201cI\u2019ve literally never spoken to a biologist who has used a microscope but not ImageJ or its offshoot project, Fiji.\u201d\n\nThe ImageJ tool, with the help of a plug-in, can automatically identify cell nuclei in microscope images, as here.Credit: Ignacio Arganda-Carreras/ImageJ\n\nThat\u2019s partly because these tools are free, Rasband says. But it\u2019s also because it\u2019s easy for users to customize the tool to their needs, says Kevin Eliceiri, a biomedical engineer at the University of Wisconsin\u2013Madison, whose team has taken the lead on ImageJ development since Rasband\u2019s retirement. ImageJ features a deceptively simple, minimalist user interface that has remained largely unchanged since the 1990s. Yet the tool is infinitely extensible thanks to its built-in macro recorder (which allows a user to save workflows by recording sequences of mouse clicks and menu selections), extensive file-format compatibility and flexible plug-in architecture. \u201cHundreds of people\u201d have contributed plug-ins, says Curtis Rueden, the programming lead in Eliceiri\u2019s group. These additions have greatly expanded the toolset for researchers, with functions to track objects over time in videos or automatically identify cells, for instance.\n\n\u201cThe point of the program isn\u2019t to be the be-all and end-all,\u201d Eliceiri says, \u201cit\u2019s to serve the purpose of its users. And unlike Photoshop and other programs, ImageJ can be whatever you want it to be.\u201d\n\nSequence searcher: BLAST (1990)\n\nThere might be no better indicator of cultural relevance than for a software name to become a verb. For search, think Google. And for genetics, think BLAST.\n\nEvolutionary changes are etched into molecular sequences as substitutions, deletions, gaps and rearrangements. By searching for similarities between sequences \u2014 particularly among proteins \u2014 researchers can discover evolutionary relationships and gain insight into gene function. The trick is to do so quickly and comprehensively across rapidly ballooning databases of molecular information.\n\nDayhoff provided one crucial piece of the puzzle in 1978. She devised a \u2018point accepted mutation\u2019 matrix that allowed researchers to score the relatedness of two proteins based not only on how similar their sequences are, but also on the evolutionary distance between them.\n\nIn 1985, William Pearson at the University of Virginia in Charlottesville and David Lipman at the NCBI introduced FASTP, an algorithm that combined Dayhoff\u2019s matrix with the ability to perform rapid searches.\n\nYears later, Lipman, along with Warren Gish and Stephen Altschul at the NCBI, Webb Miller at Pennsylvania State University in University Park, and Gene Myers at the University of Arizona, Tucson, developed an even more powerful refinement: the Basic Local Alignment Search Tool (BLAST). Released in 1990, BLAST combined the search speed required to handle fast-growing databases with the ability to pick up matches that were more evolutionarily distant. At the same time, the tool could calculate how likely it is that those matches occurred by chance.\n\nThe result was incredibly fast, Altschul says. \u201cYou could put in your search, take one sip of coffee, and your search would be done.\u201d But more importantly, it was easy to use. In an era when databases were updated by post, Gish established an e-mail system and later a web-based architecture that allowed users to run searches on the NCBI computers remotely, thus ensuring their results were always up-to-date.\n\nThe system gave the then-budding field of genome biology a transformative tool, says Sean Eddy, a computational biologist at Harvard University in Cambridge, Massachusetts \u2014 a way to work out what unknown genes might do on the basis of the genes they were related to. And for sequencing labs everywhere, it provided a clever neologism: \u201cIt\u2019s just one of these things that became a verb,\u201d Eddy says. \u201cYou just talked about BLASTing your sequences.\u201d\n\nPreprint powerhouse: arXiv.org (1991)\n\nIn the late 1980s, high-energy physicists routinely sent physical copies of their submitted manuscripts to colleagues by post for comment and as a courtesy \u2014 but only to a select few. \u201cThose lower in the food chain relied on the beneficence of those on the A-list, and aspiring researchers at non-elite institutions were frequently out of the privileged loop entirely,\u201d wrote physicist Paul Ginsparg in 20117.\n\nIn 1991, Ginsparg, then at Los Alamos National Laboratory in New Mexico, wrote an e-mail autoresponder to level the playing field. Subscribers received daily lists of preprints, each associated with an article identifier. With a single e-mail, users across the world could submit or retrieve an article from the lab\u2019s computer system, get lists of new articles or search by author or title.\n\nGinsparg\u2019s plan was to retain articles for three months, and to limit content to the high-energy physics community. But a colleague convinced him to retain the articles indefinitely. \u201cThat was the moment it transitioned from bulletin board to archive,\u201d he says. And papers flooded in from much farther afield than Ginsparg\u2019s own discipline. In 1993, Ginsparg migrated the system to the Internet, and in 1998 he gave it the name it goes by today: arXiv.org.\n\nNow in its thirtieth year, arXiv houses some 1.8 million preprints \u2014 all available for free \u2014 and attracts more than 15,000 submissions and some 30 million downloads per month. \u201cIt\u2019s not hard to see why the arXiv is such a popular service,\u201d the editors of Nature Photonics wrote8 a decade ago on the occasion of the site\u2019s twentieth anniversary: \u201cThe system provides researchers with a fast and convenient way to plant a flag that shows what they did and when, avoiding the hassle and time required for peer review at a conventional journal.\u201d\n\nSource: arXiv.org\n\nThe site\u2019s success catalysed a boom in sister archives in biology, medicine, sociology and other disciplines. The impact can be seen today in tens of thousands of preprints that have been published on the virus SARS-CoV-2.\n\n\u201cIt\u2019s gratifying to see a methodology, considered heterodox outside of the particle-physics community 30 years ago, now more generally viewed as obvious and natural,\u201d Ginsparg says. \u201cIn that sense, it\u2019s like a successful research project.\u201d\n\nData explorer: IPython Notebook (2011)\n\nFernando P\u00e9rez was a graduate student \u201cin search of procrastination\u201d in 2001 when he decided to take on a core component of Python.\n\nPython is an interpreted language, which means programs are executed line by line. Programmers can use a kind of computational call-and-response tool called a read\u2013evaluate\u2013print loop (REPL), in which they type code and a program called an interpreter executes it. A REPL allows for quick exploration and iteration, but P\u00e9rez noted that Python\u2019s wasn\u2019t built for science. It didn\u2019t allow users to easily preload modules of code, for instance, or keep data visualizations open. So P\u00e9rez wrote his own version.\n\nThe result was IPython, an \u2018interactive\u2019 Python interpreter that P\u00e9rez unveiled in December 2001 \u2014 all 259 lines of it. A decade later, P\u00e9rez, working with physicist Brian Granger and mathematician Evan Patterson, migrated that tool to the web browser, launching the IPython Notebook and kick-starting a data-science revolution.\n\nWhy Jupyter is data scientists\u2019 computational notebook of choice\n\nLike other computational notebooks, IPython Notebook combined code, results, graphics and text in a single document. But unlike other such projects, IPython Notebook was open-source, inviting contributions from a vast developer community. And it supported Python, a popular language for scientists. In 2014, IPython evolved into Project Jupyter, supporting some 100 languages and allowing users to explore data on remote supercomputers as easily as on their own laptops.\n\n\u201cFor data scientists, Jupyter has emerged as a de facto standard,\u201d Nature wrote in 20189. At the time, there were 2.5 million Jupyter notebooks on the GitHub code-sharing platform; today, there are nearly 10 million, including the ones that document the 2016 discovery of gravitational waves and the 2019 imaging of a black hole. \u201cThat we made a small contribution to those projects is extremely rewarding,\u201d P\u00e9rez says.\n\nFast learner: AlexNet (2012)\n\nArtificial intelligence (AI) comes in two flavours. One uses codified rules, the other enables a computer to \u2018learn\u2019 by emulating the neural structure of the brain. For decades, says Geoffrey Hinton, a computer scientist at the University of Toronto, Canada, AI researchers dismissed the latter approach as \u201cnonsense\u201d. In 2012, Hinton\u2019s graduate students Alex Krizhevsky and Ilya Sutskever proved otherwise.\n\nThe venue was ImageNet, an annual competition that challenges researchers to train an AI on a database of one million images of everyday objects, then test the resulting algorithm on a separate image set. At the time, the best algorithms miscategorized about one-quarter of them, Hinton says. Krizhevsky and Sutskever\u2019s AlexNet, a \u2018deep-learning\u2019 algorithm based on neural networks, reduced that error rate to 16%10. \u201cWe basically halved the error rate, or almost halved it,\u201d notes Hinton.\n\nHinton says the team\u2019s success in 2012 reflected the combination of a big-enough training data set, great programming and the newly emergent power of graphical processing units \u2014 the processors that were originally designed to accelerate computer video performance. \u201cSuddenly we could run [the algorithm] 30 times faster,\u201d he says, \u201cor learn on 30 times as much data.\u201d\n\nThe real algorithmic breakthrough, Hinton says, actually occurred three years earlier, when his lab created a neural network that could recognize speech more accurately than could conventional AIs that had been refined over decades. \u201cIt was only slightly better,\u201d Hinton says. \u201cBut already that was the writing on the wall.\u201d\n\nThose victories heralded the rise of deep learning in the lab, the clinic and more. They\u2019re why mobile phones are able to understand spoken queries and why image-analysis tools can readily pick out cells in photomicrographs. And they are why AlexNet takes its place among the many tools that have fundamentally transformed science, and with them, the world."
        },
        {
            "authors": [],
            "title": "Sundayy",
            "contents": "No feeds, just reflections.\n\nSee a different side of your friends, family, and yourself. Start reflecting intentionally on what matters most, together."
        },
        {
            "authors": [
                "Jeff Geerling",
                "Michael Horne",
                "Paul Beech",
                "James Adams",
                "Stewart Watkiss",
                "Phil Atkin",
                "Suman Harapanahalli",
                "Manish Buttan",
                "Colin Deady",
                "Simon Lukas"
            ],
            "title": "Meet Raspberry Silicon: Raspberry Pi Pico now on sale at $4",
            "contents": "Today, we\u2019re launching our first microcontroller-class product: Raspberry Pi Pico. Priced at just $4, it is built on RP2040, a brand-new chip developed right here at Raspberry Pi. Whether you\u2019re looking for a standalone board for deep-embedded development or a companion to your Raspberry Pi computer, or you\u2019re taking your first steps with a microcontroller, this is the board for you.\n\nYou can buy your Raspberry Pi Pico today online from one of our Approved Resellers. Or head to your local newsagent, where every copy of this month\u2019s HackSpace magazine comes with a free Pico, as well as plenty of guides and tutorials to help you get started with it. If coronavirus restrictions mean that you can\u2019t get to your newsagent right now, you can grab a subscription and get Pico delivered to your door.\n\nOops!\u2026 We Did It Again\n\nMicrocomputers and microcontrollers\n\nMany of our favourite projects, from cucumber sorters to high altitude balloons, connect Raspberry Pi to the physical world: software running on the Raspberry Pi reads sensors, performs computations, talks to the network, and drives actuators. This ability to bridge the worlds of software and hardware has contributed to the enduring popularity of Raspberry Pi computers, with over 37 million units sold to date.\n\nBut there are limits: even in its lowest power mode a Raspberry Pi Zero will consume on the order of 100 milliwatts; Raspberry Pi on its own does not support analogue input; and while it is possible to run \u201cbare metal\u201d software on a Raspberry Pi, software running under a general-purpose operating system like Linux is not well suited to low-latency control of individual I/O pins.\n\nMany hobbyist and industrial applications pair a Raspberry Pi with a microcontroller. The Raspberry Pi takes care of heavyweight computation, network access, and storage, while the microcontroller handles analogue input and low-latency I/O and, sometimes, provides a very low-power standby mode.\n\nUntil now, we\u2019ve not been able to figure out a way to make a compelling microcontroller-class product of our own. To make the product we really wanted to make, first we had to learn to make our own chips.\n\nRaspberry Si\n\nIt seems like every fruit company is making its own silicon these days, and we\u2019re no exception. RP2040 builds on the lessons we\u2019ve learned from using other microcontrollers in our products, from the Sense HAT to Raspberry Pi 400. It\u2019s the result of many years of hard work by our in-house chip team.\n\nWe had three principal design goals for RP2040: high performance, particularly for integer workloads; flexible I/O, to allow us to talk to almost any external device; and of course, low cost, to eliminate barriers to entry. We ended up with an incredibly powerful little chip, cramming all this into a 7 \u00d7 7 mm QFN-56 package containing just two square millimetres of 40 nm silicon. RP2040 has:\n\nDual-core Arm Cortex-M0+ @ 133MHz\n\n264KB (remember kilobytes?) of on-chip RAM\n\nSupport for up to 16MB of off-chip Flash memory via dedicated QSPI bus\n\nDMA controller\n\nInterpolator and integer divider peripherals\n\n30 GPIO pins, 4 of which can be used as analogue inputs\n\n2 \u00d7 UARTs, 2 \u00d7 SPI controllers, and 2 \u00d7 I2C controllers\n\n16 \u00d7 PWM channels\n\n1 \u00d7 USB 1.1 controller and PHY, with host and device support\n\n8 \u00d7 Raspberry Pi Programmable I/O (PIO) state machines\n\nUSB mass-storage boot mode with UF2 support, for drag-and-drop programming\n\nAnd this isn\u2019t just a powerful chip: it\u2019s designed to help you bring every last drop of that power to bear. With six independent banks of RAM, and a fully connected switch at the heart of its bus fabric, you can easily arrange for the cores and DMA engines to run in parallel without contention.\n\nFor power users, we provide a complete C SDK, a GCC-based toolchain, and Visual Studio Code integration.\n\nAs Cortex-M0+ lacks a floating-point unit, we have commissioned optimised floating-point functions from Mark Owen, author of the popular Qfplib libraries; these are substantially faster than their GCC library equivalents, and are licensed for use on any RP2040-based product.\n\nWith two fast cores and and a large amount of on-chip RAM, RP2040 is a great platform for machine learning applications. You can find Pete Warden\u2019s port of Google\u2019s TensorFlow Lite framework here. Look out for more machine learning content over the coming months.\n\nFor beginners, and other users who prefer high-level languages, we\u2019ve worked with Damien George, creator of MicroPython, to build a polished port for RP2040; it exposes all of the chip\u2019s hardware features, including our innovative PIO subsystem. And our friend Aivar Annamaa has added RP2040 MicroPython support to the popular Thonny IDE.\n\nRaspberry Pi Pico\n\nRaspberry Pi Pico is designed as our low-cost breakout board for RP2040. It pairs RP2040 with 2MB of Flash memory, and a power supply chip supporting input voltages from 1.8-5.5V. This allows you to power your Pico from a wide variety of sources, including two or three AA cells in series, or a single lithium-ion cell.\n\nPico provides a single push button, which can be used to enter USB mass-storage mode at boot time and also as a general input, and a single LED. It exposes 26 of the 30 GPIO pins on RP2040, including three of the four analogue inputs, to 0.1\u201d-pitch pads; you can solder headers to these pads or take advantage of their castellated edges to solder Pico directly to a carrier board. Volume customers will be able to buy pre-reeled Pico units: in fact we already supply Pico to our Approved Resellers in this format.\n\nThe Pico PCB layout was co-designed with the RP2040 silicon and package, and we\u2019re really pleased with how it turned out: a two-layer PCB with a solid ground plane and a GPIO breakout that \u201cjust works\u201d.\n\nReely good\n\nWhether Raspberry Pi Pico is your first microcontroller or your fifty-first, we can\u2019t wait to see what you do with it.\n\nRaspberry Pi Pico documentation\n\nOur ambition with RP2040 wasn\u2019t just to produce the best chip, but to support that chip with the best documentation. Alasdair Allan, who joined us a year ago, has overseen a colossal effort on the part of the whole engineering team to document every aspect of the design, with simple, easy-to-understand examples to help you get the most out of your Raspberry Pi Pico.\n\nYou can find complete documentation for Raspberry Pi Pico, and for RP2040, its SDK and toolchain, here.\n\nTo help you get the most of your Pico, why not grab a copy of Get Started with MicroPython on Raspberry Pi Pico by Gareth Halfacree and our very own Ben Everard. It\u2019s ideal for beginners who are new (or new-ish) to making with microcontrollers.\n\nOur colleagues at the Raspberry Pi Foundation have also produced an educational project to help you get started with Raspberry Pi Pico. You can find it here.\n\nPartners\n\nOver the last couple of months, we\u2019ve been working with our friends at Adafruit, Arduino, Pimoroni, and Sparkfun to create accessories for Raspberry Pi Pico, and a variety of other boards built on the RP2040 silicon platform. Here are just a few of the products that are available to buy or pre-order today.\n\nAdafruit Feather RP 2040\n\nRP2040 joins the hundreds of boards in the Feather ecosystem with the fully featured Feather RP 2040 board. The 2\u2033 \u00d7 0.9\u2033 dev board has USB C, Lipoly battery charging, 4MB of QSPI flash memory, a STEMMA QT I2C connector, and an optional SWD debug port. With plenty of GPIO for use with any FeatherWing, and hundreds of Qwiic/QT/Grove sensors that can plug and play, it\u2019s the fast way to get started.\n\nFeathery goodness\n\nAdafruit ItsyBitsy RP 2040\n\nNeed a petite dev board for RP2040? The Itsy Bitsy RP 2040 is positively tiny, but it still has lots of GPIO, 4MB of QSPI flash, boot and reset buttons, a built-in RGB NeoPixel, and even a 5V output logic pin, so it\u2019s perfect for NeoPixel projects!\n\nSmall is beautiful\n\nArduino Nano RP2040 Connect\n\nArduino joins the RP2040 family with one of its most popular formats: the Arduino Nano. The Arduino Nano RP2040 Connect combines the power of RP2040 with high-quality MEMS sensors (a 9-axis IMU and microphone), a highly efficient power section, a powerful WiFi/Bluetooth module, and the ECC608 crypto chip, enabling anybody to create secure IoT applications with this new microcontroller. The Arduino Nano RP2040 Connect will be available for pre-order in the next few weeks.\n\nGet connected!\n\nPimoroni PicoSystem\n\nPicoSystem is a tiny and delightful handheld game-making experience based on RP2040. It comes with a simple and fast software library, plus examples to make your mini-gaming dreams happen. Or just plug it into USB and drop the best creations from the Raspberry Pi-verse straight onto the flash drive.\n\nPixel-pushing pocket-sized playtime\n\nPimoroni Pico Explorer Base\n\nPico Explorer offers an embedded electronics environment for educators, engineers, and software people who want to learn hardware with less of the \u201chard\u201d bit. It offers easy expansion and breakout along with a whole bunch of useful bits.\n\nGo explore!\n\nSparkFun Thing Plus \u2013 RP2040\n\nThe Thing Plus \u2013 RP2040 is a low-cost, high-performance board with flexible digital interfaces featuring Raspberry Pi\u2019s RP2040 microcontroller. Within the Feather-compatible Thing Plus form factor with 18 GPIO pins, the board offers an SD card slot, 16MB (128Mbit) flash memory, a JST single-cell battery connector (with a charging circuit and fuel gauge sensor), an addressable WS2812 RGB LED, JTAG PTH pins, mounting holes, and a Qwiic connector to add devices from SparkFun\u2019s quick-connect I2C ecosystem.\n\nThing One, or Thing Two?\n\nSparkFun MicroMod RP2040 Processor\n\nThe MicroMod RP2040 Processor Board is part of SparkFun\u2019s MicroMod modular interface system. The MicroMod M.2 connector makes it easy to connect your RP2040 Processor Board with the MicroMod carrier board that gives you the inputs and outputs you need for your project.\n\nThe Mighty Micro\n\nSparkFun Pro Micro \u2013 RP2040\n\nThe Pro Micro RP2040 harnesses the capability of RP2040 on a compact development board with the USB functionality that is the hallmark of all SparkFun\u2019s Pro Micro boards. It has a WS2812B addressable LED, boot button, reset button, Qwiic connector, USB-C, and castellated pads.\n\nGo Pro\n\nCredits\n\nIt\u2019s fair to say we\u2019ve taken the long road to creating Raspberry Pi Pico. Chip development is a complicated business, drawing on the talents of many different people. Here\u2019s an incomplete list of those who have contributed to the RP2040 and Raspberry Pi Pico projects:\n\nDave Akerman, Sam Alder, Alasdair Allan, Aivar Annamaa, Jonathan Bell, Mike Buffham, Dom Cobley, Steve Cook, Phil Daniell, Russell Davis, Phil Elwell, Ben Everard, Andras Ferencz, Nick Francis, Liam Fraser, Damien George, Richard Gordon, F Trevor Gowen, Gareth Halfacree, David Henly, Kevin Hill, Nick Hollinghurst, Gordon Hollingworth, James Hughes, Tammy Julyan, Jason Julyan, Phil King, Stijn Kuipers, Lestin Liu, Simon Long, Roy Longbottom, Ian Macaulay, Terry Mackown, Simon Martin, Jon Matthews, Nellie McKesson, Rod Oldfield, Mark Owen, Mike Parker, David Plowman, Dominic Plunkett, Graham Sanderson, Andrew Scheller, Serge Schneider, Nathan Seidle, Vinaya Puthur Sekar, Mark Sherlock, Martin Sperl, Mike Stimson, Ha Thach, Roger Thornton, Jonathan Welch, Simon West, Jack Willis, Luke Wren, David Wright.\n\nWe\u2019d also like to thank our friends at Sony Pencoed and Sony Inazawa, Microtest, and IMEC for their help in bringing these projects to fruition.\n\nBuy your Raspberry Pi Pico from one of our Approved Resellers today, and let us know what you think!\n\nFAQs\n\nAre you planning to make RP2040 available to customers?\n\nWe hope to make RP2040 broadly available in the second quarter of 2021.",
            "published_at": "2021-01-21T06:59:06+00:00"
        },
        {
            "authors": [],
            "title": "Linux 5.11 Is Now Looking Great For AMD Zen 2 / Zen 3 Performance",
            "contents": "Not only is the AMD \"CPU frequency invariance regression\" from that new support with the in-development Linux 5.11 kernel on course to address the performance shortcomings I outlined last month, but with the patched kernel for a number of workloads the performance is now ahead of where it was at with Linux 5.10.\n\nAt the end of last year I pointed out that the early Linux 5.11 kernel code was regressing hard for AMD performance when using the \"Schedutil\" scheduler utilization governor as is now often the default behavior of the kernel. As bisected back then and outlined in full with that aforelinked article, the performance regressions seen on AMD Zen 2 and Zen 3 systems stem from the introduction of CPU frequency invariance support added this cycle.\n\nNow the shortcoming with that initial CPU frequency invariance support is in the process of being worked out. As outlined last night, a fix has been proposed for addressing that major performance regression for AMD systems with Linux 5.11.\n\nThat fix hasn't been merged yet to mainline but I have been eagerly testing the patch now. While less than 24 hours into the testing, I can already say: Linux 5.11 is now in much better shape for AMD. Not only is the performance issue resolved, but for multiple workloads the Linux 5.11 performance is better off than 5.10!\n\nIn this article are some preliminary figures using an AMD EPYC 7702 server and AMD Ryzen 9 5950X desktop. As outlined in prior articles, this CPU frequency invariance support in combination with the CPUFreq governor only impact Zen 2 hardware and newer due to its reliance on ACPI CPPC. Let's take a look at some of these initial numbers now off a patched kernel while in the coming days will be more workloads tested and more AMD CPUs being benchmarked. Follow-up articles will also offer a look at the CPU power efficiency/consumption as well as seeing how this Schedutil performance compares now to the performance governor - today's article is just the culmination of a day's worth of tests."
        },
        {
            "authors": [
                "Brenda Goh"
            ],
            "title": "Alibaba's Jack Ma makes first public appearance in three months",
            "contents": "SHANGHAI (Reuters) - Alibaba Group founder Jack Ma made his first public appearance since October on Wednesday when he spoke to a group of teachers by video, easing concern about his unusual absence from the limelight and sending shares in the e-commerce giant surging.\n\nSpeculation over Ma\u2019s whereabouts has swirled in the wake of news this month that he was replaced in the final episode of a reality TV show he had been a judge on, and amid a regulatory clampdown by Beijing on his sprawling business empire.\n\nThe billionaire, who commands a cult-like reverence in China, had not appeared in public since Oct. 24, when he blasted China\u2019s regulatory system in a speech at a Shanghai forum. That set him on a collision course with officials and led to the suspension of a blockbuster $37 billion IPO for Alibaba\u2019s financial affiliate Ant Group.\n\nUntil then Ma often appeared in public, speaking at conferences and other events, though less frequently than in 2019 due to the coronavirus pandemic.\n\nAlibaba and his charitable foundation both confirmed Ma, a former English teacher, participated in an online ceremony for rural teachers organised by the foundation on Wednesday. They declined to provide further comment.\n\nIn the 50-second video, Ma, wearing a navy pullover, spoke from a room with grey walls, a large painting and floral arrangements. It was not clear where the room was.\n\n\u201cIt was good to see that Jack Ma has resurfaced \u2013 my assumption was that he decided (with some encouragement) to take a temporarily lower profile after making comments that annoyed the government,\u201d said Dan Kern, chief investment officer of Alibaba investor TFC Financial Management in Boston whose funds hold positions in the stock.\n\nSlideshow ( 2 images )\n\nAlibaba\u2019s Hong Kong-listed shares jumped to finish 8.5% higher on the news, which was first reported by Tianmu News, a media outlet backed by the government of Zhejiang, the province where Alibaba\u2019s headquarters are based. The company\u2019s American Depositary Receipts (ADRs) rose nearly 5%.\n\nThe video also contained footage, dated Jan. 10, of Ma visiting with colleagues a school in Tonglu county, part of Hangzhou city, the capital of Zhejiang.\n\n\u201cJack Ma\u2019s reappearance has given investors peace of mind after a lot of rumours, allowing them to pile into the stock which had been a laggard in the market,\u201d said Steven Leung, sales director at brokerage UOB Kay Hian in Hong Kong.\n\nThe stock has erased losses suffered after Alibaba became the target of an antitrust investigation launched last month by Chinese authorities, but remains some 11% below levels prior to the cancellation of the Ant IPO.\n\nThe topic \u201cJack Ma makes his first public appearance\u201d and his video address to the teachers soon began trending on China\u2019s Twitter-like Weibo, triggering heavy discussion.\n\nAlthough Ma has stepped down from corporate positions and earnings calls, he retains significant influence over Alibaba and Ant and promotes them globally at business and political events. He also continues to mentor management talent in the \u201cAlibaba Partnership\u201d, a 35-member group of company managers.\n\nThe company plans to raise at least $5 billion through the sale of a U.S. dollar-denominated bond this month.",
            "published_at": "2021-01-20T04:33:02+00:00"
        },
        {
            "authors": [
                "Saikrishna Bangalore Prakash",
                "Judge Thomas B. Griffith",
                "Anita S. Krishnakumar",
                "Kevin P. Tobia"
            ],
            "title": "MetaRules for Ordinary Meaning",
            "contents": "\u201cOrdinary meaning\u201d is a notoriously undefined concept in statutory interpretation theory. Courts and scholars sometimes describe ordinary meaning as the meaning that a \u201creasonable reader\u201d would ascribe to the statutory language at issue, but it remains unclear how judges and lawyers should go about identifying such meaning. Over the past few decades, as textualism has come to dominate statutory interpretation, courts increasingly have employed dictionary definitions as (purportedly) neutral, and sometimes dispositive, evidence of ordinary meaning. And in the past few years especially, some judges and scholars have advocated using corpus linguistics \u2014 patterns of usage across various English-language sources \u2014 as an objective guide to the ordinary meaning of statutory words and phrases. Professor Kevin Tobia\u2019s illuminating article Testing Ordinary Meaning seeks to test empirically how accurately these two interpretive aids \u2014 dictionary definitions and corpus linguistics \u2014 reflect ordinary meaning.\n\nTo do so, Tobia uses a series of experimental studies based on surveys of laypeople gathered through Amazon\u2019s Mechanical Turk, as well as surveys of federal and state judges and law students at Harvard, Yale, and Columbia. The study uses as a baseline for \u201cordinary meaning\u201d the unaided collective intuitions of laypeople, federal judges, and law students \u2014 and compares those unaided intuitions to the meaning these three groups of interpreters selected when asked to apply dictionary definitions or corpus linguistics to the same set of terms. Tobia\u2019s experiment constitutes an admirable effort to dissect the concept of ordinary meaning, and one that yields important information and results. In my view, there are three key takeaways from his study: (1) judges and nonexperts assess meaning similarly; (2) ordinary meaning is often unclear; and (3) dictionaries and corpus linguistics provide meanings that diverge from each other and from ordinary meaning, with dictionaries tending to reflect expansive, or \u201clegalist,\u201d word meaning and corpus linguistics tending to reflect \u201cprototypical\u201d meaning.\n\nI agree with many of the conclusions drawn by Tobia\u2019s thoughtful article. This Response will focus primarily on a few points of disagreement as well as on some methodological lessons that might be gleaned from his findings. First, I discuss an important question that Tobia\u2019s study glosses over \u2014 the question of who the appropriate audience (or \u201cordinary reader\u201d) is for a particular statute \u2014 and I suggest that Tobia\u2019s data do not support the strong version of his claim that different audiences judge statutory meaning similarly. Second, I consider the methodological implications of Tobia\u2019s findings that ordinary meaning often is unclear and that dictionary definitions, corpus linguistics, and collective intuition about ordinary meaning often diverge from each other for a Supreme Court and bench that have moved increasingly toward a textualist approach to statutory interpretation. Specifically, I suggest two metarules that courts might adopt to help curb judicial discretion and uncertainty over ordinary meaning: (1) a rule instructing that certain categories of statutes should be construed in light of their prototypical (or, conversely, legalist) meaning; and (2) a rule directing that differences in the ordinary meaning identified by dictionaries, corpus linguistics, different judges, and/or surveys of laypeople should be considered prima facie evidence that a statute is ambiguous and lacks a \u201cplain\u201d meaning.\n\nI. The \u201cAudience\u201d Question: Who Is the Relevant \u201cOrdinary Reader\u201d?\n\nPerhaps the most stunning finding Tobia reports is that judges and nonexperts have similar intuitions about the ordinary meaning of ordinary terms. Tobia bases this claim on survey data indicating that judges, law students, and laypeople were remarkably similar in the rates at which they categorized certain specific items (for example, \u201ccar,\u201d \u201cbus,\u201d \u201cairplane,\u201d \u201ccanoe,\u201d \u201croller skates\u201d) as \u201cvehicles\u201d or not \u201cvehicles.\u201d But on closer inspection, there are a few problems with basing such a broad claim on this limited data. First, Tobia does not provide data regarding the relative rates at which judges and nonexperts attributed similar (or dissimilar) meanings to any of the other terms used in his study \u2014 that is, \u201ccarry,\u201d \u201cinterpreter,\u201d \u201clabor,\u201d \u201ctangible object,\u201d \u201cweapon,\u201d \u201canimal,\u201d \u201cfurniture,\u201d \u201cfood,\u201d and \u201cclothing.\u201d (To be fair, the study did not gather data from judges and law students for these other terms. ) So it is unclear whether the symmetry he observes between judges, law students, and laypeople would be replicated in other contexts, with respect to other words; there could simply be something special about the word \u201cvehicles\u201d that produces greater consistency across interpreters than other terms would.\n\nSecond, and perhaps more importantly, Tobia\u2019s data with respect to \u201cvehicles\u201d show noteworthy variation among judges and nonexperts in close or borderline cases, as opposed to easy cases. That is, while there appears to be little disagreement between judges and nonexperts that \u201ccars,\u201d \u201ctrucks,\u201d and \u201cbuses\u201d are vehicles \u2014 or that \u201cdrones,\u201d \u201croller skates,\u201d and \u201cbaby carriers\u201d are not vehicles \u2014 there is considerable variation between these groups regarding whether borderline items such as an \u201celectric wheelchair,\u201d a \u201cbaby stroller,\u201d or a \u201cWorld War II Truck\u201d that has been decorated as a World War II monument are vehicles. Similar variations occur with respect to how judges and nonexperts apply dictionary definitions in borderline cases. Why does this matter? Because most legal disputes involve close cases: litigants do not tend to go to court to determine whether a \u201ccar\u201d is a vehicle; they tend to go to court to resolve disagreements over whether borderline items like \u201cbaby strollers\u201d or \u201celectric wheelchairs\u201d qualify as vehicles.\n\nThis variance also matters because if judges and laypeople disagree about the ordinary meaning of statutory terms in borderline cases, then the key question in such cases becomes: Who is the relevant audience or \u201cordinary reader\u201d of the statute \u2014 judges, the average person on the street, or some other group of people? This is a crucial question in statutory interpretation but one that often is ignored. It may be the case, for example, that for criminal statutes or statutes that deal with education, housing, or voting rights, the relevant audience or \u201cordinary reader\u201d is the average person on the street. Conversely, for statutes that govern cost-shifting among litigants, jurisdiction or other matters of court procedure, or remedies, the relevant audience or \u201cordinary reader\u201d may instead be judges. There are also potential equality and gender dimensions embedded in this audience question: it may be the case that certain terms have different meanings to men versus women or to people of different racial or ethnic backgrounds.\n\nTobia\u2019s data, in my view, suggest that it might be worthwhile, in anticipation of the difficult cases that tend to make it to adjudication, for the legislature \u2014 or the judiciary \u2014 to establish default rules about who the relevant \u201cordinary reader\u201d is for certain kinds of statutes or even certain kinds of statutory provisions. For example, the audience or relevant \u201cordinary reader\u201d for those provisions of a statute that govern procedural or legal matters might be judges, whereas the audience for provisions that govern citizen behavior directly may be laypeople. Whether the relevant statutory audience should be determined for entire statutes or for particular kinds of statutory provisions is an important subquestion that courts, legislators, and scholars can and should think deeply about. My aim in this Response is merely to raise the issue.\n\nOperationally, default rules about the appropriate statutory audience could be established as follows: Congress, ideally, could specify who the relevant audience, or reader, is for individual statutes (or for particular kinds of statutory provisions) when it enacts them. In cases where a particular statutory provision is designed to have multiple audiences, Congress could specify a \u201ctarget audience\u201d for purposes of judicial interpretation or, conversely, indicate clearly that the statute or particular provisions of the statute are directed at multiple audiences \u2014 and perhaps list those multiple audiences.\n\nAlternately, courts could establish their own default rules \u2014 similar to existing canons of construction that call for liberal or narrow construction of certain categories of statutes or provisions, or that favor certain parties when a statute is ambiguous \u2014 specifying the audience that courts should bear in mind when seeking to identify a statute\u2019s \u201cordinary meaning.\u201d In individual cases, a court might choose to ignore the default rule, perhaps because the provision at issue is one that speaks to a different audience than the overall statute, but in so doing the court would at least have to confront the audience question \u2014 something that current statutory interpretation canons and rules do not require.\n\nTo see how these recommendations might work in practice, consider two leading statutory interpretation cases that highlight the importance of the audience question and that demonstrate how default rules could help constrain the judicial search for ordinary meaning.\n\nIn Taniguchi v. Kan Pacific Saipan, Ltd., the Court considered the meaning of the term \u201cinterpreters,\u201d which appeared in a statute that listed \u201ccompensation of interpreters\u201d as one of several kinds of litigation costs that courts have the power to award prevailing parties. A majority of the Court held that the term \u201cinterpreters\u201d did not encompass written document translation services paid for by the plaintiff, citing fourteen dictionary definitions that referred to \u201cinterpreters\u201d as persons who provide oral translation services. Justice Ginsburg dissented, citing three legal (and other) dictionaries, as well as several federal court of appeals and district court decisions treating translators of written documents as \u201cinterpreters.\u201d Justice Ginsburg\u2019s dissent did not directly address the audience question, but it did express her view that the \u201ckey\u201d context for determining the meaning of the term \u201cinterpreters\u201d was \u201cthe practice of federal courts both before and after [the statute\u2019s] enactment.\u201d In other words, Justice Ginsburg seemed to believe that the relevant audience, or readers, of the cost-shifting statute were federal judges, although she framed her analysis in terms of federal courts\u2019 past \u201cpractice\u201d rather than their understanding of the ordinary meaning of the term \u201cinterpreters.\u201d The majority opinion did not grapple with the audience question at all. One wonders whether the case would have come out differently if the Court\u2019s inquiry into the ordinary meaning of \u201cinterpreters\u201d had been framed in terms of who \u2014 judges or the average person on the street \u2014 constituted the relevant audience for the statute.\n\nIn another memorable case, McNally v. United States, Justice Stevens argued, in dissent, that there were no due process notice problems with applying a federal mail fraud statute to a Kentucky public official and private individuals engaged in a kickback scheme through which an insurance company hired by the State of Kentucky funneled commissions to the personal accounts of the public official and other politically active party members. The federal statute prohibited the use of the mails to execute \u201cany scheme or artifice to defraud, or for obtaining money or property by means of false or fraudulent pretenses, representations, or promises.\u201d The statutory confusion arose because although the Kentucky official and others had been enriched by the kickback scheme, they had not actually defrauded the citizens of Kentucky out of any money or property. Justice Stevens explained that in determining the meaning and reach of the statutory terms at issue, \u201cit is appropriate to identify the class of litigants\u201d to whom the statute is being applied. In this case, that class of litigants consisted of \u201cthe most sophisticated practitioners of the art of government among us,\u201d and Justice Stevens noted that the \u201cgovernment executives, judges, and legislators who have been accused, and convicted, of mail fraud under the well-settled construction of the statute . . . are people who unquestionably knew that their conduct was unlawful.\u201d In Justice Stevens\u2019s view, then, the relevant audience, or ordinary reader, of the mail fraud statute was sophisticated government officials \u2014 not the average layperson.\n\nAs Taniguchi and McNally illustrate, courts have been dancing around questions about the relevant audience or \u201cordinary reader\u201d of a particular statute for years, although they rarely confront the question squarely. And while Tobia\u2019s data suggest that the audience question may be inconsequential for easy cases, in which different audiences are likely to identify the same ordinary meaning, his data also highlight the necessity and potential benefits of clarifying the relevant audience for those hard cases that inevitably will arise.\n\nSpecifying the relevant audience or \u201cordinary reader\u201d might also help dictate which external sources interpreters should consult to help identify a statute\u2019s ordinary meaning: certain kinds of dictionaries may be appropriate for certain kinds of statutes or terms \u2014 for example, legal dictionaries for statutes dealing with court procedure, popular dictionaries or perhaps corpus linguistics for criminal statutes, and possibly even medical or scientific dictionaries for certain statutes. Similarly, certain corpora may be more appropriate for certain kinds of statutes than for others \u2014 for example, the TV Corpus or Movie Corpus may be better for criminal statutes (as guides to popular meaning), whereas the Corpus of U.S. Supreme Court Opinions may be more appropriate for procedural statutes.\n\nFinally, Tobia\u2019s study assumes that there is such a thing as a \u201ccorrect\u201d baseline ordinary meaning for statutory terms \u2014 one that can be identified by measuring the collective intuition of laypeople \u2014 and against which the accuracy of dictionary definitions and corpus linguistics can be gauged. But this assumption itself elides the audience question \u2014 in that it assumes that the collective intuition of ordinary people represents the correct baseline ordinary meaning. (Note, again, that Tobia\u2019s study does not measure the intuitions of judges or law students for terms other than \u201cvehicles\u201d \u2014 so his assessments of the accuracy of dictionary definitions and corpus linguistics depend on ordinary people\u2019s intuitions, as surveyed on Amazon\u2019s Mechanical Turk, as the baseline for \u201cordinary meaning.\u201d) As I have suggested above, that may be appropriate for some statutes; but for others, the relevant audience or reader should be judges and/or lawyers \u2014 and the relevant baseline measure for ordinary meaning should be judicial or other legal professionals\u2019 intuitions.\n\nII. Ordinary Meaning Subcanons\n\nTobia also concludes that the ordinary meaning of specific words often is unclear, based on survey data indicating that laypeople regularly disagreed with each other about the ordinary meaning of several different statutory terms and that dictionary definitions, corpus linguistics, and collective intuition about ordinary meaning often diverged from each other. He further concludes that dictionary definitions tend to encompass a broad, or expansive, reading of the term at issue, whereas corpus linguistics analysis tends to reflect a word\u2019s prototypical meaning; this was evidenced by the fact that study subjects who applied dictionary definitions concluded that nearly every entity tested (from cars to zip lines) qualified as a \u201cvehicle,\u201d whereas subjects asked to apply corpus linguistics took a narrower view of the meaning of \u201cvehicle\u201d and were less likely to conclude that any of the tested entities qualified.\n\nAs noted above, I am unsurprised by these findings \u2014 in part because my own empirical research is consistent with them. In a recent study on the extent of judicial dueling over interpretive resources in the U.S. Supreme Court, for example, I measured how often majority and dissenting opinions in the same case used the same interpretive tool to reach opposing outcomes. For plain or ordinary meaning analysis, I found a 42.7% rate of judicial dueling \u2014 meaning that in over 40% of the Court\u2019s divided vote cases in which at least one opinion argued that the statute had an ordinary or plain meaning, an opposing opinion countered that the statute had a different ordinary meaning. Perhaps even more interestingly, I found that in 41.2% of the cases in which majority and dissenting opinions disagreed about a statute\u2019s plain meaning, one opinion advocated adopting the \u201ccore\u201d or \u201cprototypical\u201d meaning of the word at issue while the other focused on the broad or legalist meaning of the word.\n\nOne solution, or response, to Tobia\u2019s experimental findings (or mine, for that matter) is to acknowledge that dictionaries and corpus linguistics cannot serve as dispositive determinants of ordinary meaning and to urge that interpreters use both of these sources in tandem or that they consult other contextual clues, such as statutory purpose, alongside such sources to determine whether a statutory term should be given its prototypical or legalist meaning (dictionary or corpus meaning). These are the solutions Tobia suggests toward the end of his article. But there are other possible responses to these experimental findings about the indeterminacy of ordinary meaning. One is to seek to reduce, ex ante, the universe of possible ordinary meanings among which judges can choose \u2014 on the theory, subscribed to by many textualists in the context of interpretive tools such as legislative history and statutory purpose, that if given multiple options, judges will find it far too easy to choose an ordinary meaning that fits their ideological policy preferences. Another possible response is to treat disagreement about ordinary meaning in dictionary definitions versus corpora, or in laypersons\u2019 survey responses, or in judicial decisions, as establishing a prima facie case of ambiguity that would in turn trigger a move to second-order interpretive canons or tools \u2014 and would prompt courts to abandon the search for ordinary meaning altogether. This section explores these latter two possibilities.\n\nA. Prototypical vs. Expansive Meaning\n\nOne possible lesson from Tobia\u2019s study is that it might be helpful for interpreters to know in advance \u2014 ex ante, before a particular dispute is before them \u2014 whether a given statute, or particular kinds of statutory provisions, should be interpreted in light of their prototypical or their expansive meaning. That is, it might be helpful if the ordinary meaning canon or rule contained metarules, or subcanons, dictating that certain statutes, or certain categories of statutes or provisions, should be given their prototypical or expansive meaning. Such metarules might be established in a number of ways:\n\n1. Congress, when enacting individual statutes, could specify that a statute should be interpreted in light of its prototypical meaning or, conversely, in an expansive or legalist manner. Alternately, Congress could enact a more general statute, along the lines of the Dictionary Act, dictating that certain categories of statutes should be given either their prototypical or expansive meaning \u2014 for example, terms in criminal statutes should be interpreted in light of their prototypical meaning, while terms in antitrust statutes should be interpreted in light of their expansive, or legalist meaning.\n\n2. Another option would be for courts, and the Supreme Court in particular, to establish ex ante rules dictating that certain kinds of statutes, or provisions within statutes, should be interpreted based on their prototypical or legalist meaning. If the Court does not wish to make such broad pronouncements, it could make a determination about whether a statute should be interpreted in light of its prototypical or legalist meaning on a case-by-case basis, for each individual statute that comes before it \u2014 and that determination could then bind lower courts in the future. Lower courts also could analogize from those statutes with respect to which the Court has articulated such a metarule to other similar statutes the Court has not yet evaluated.\n\n3. A third, path-of-least-resistance alternative is that courts could turn existing liberal or narrow construction rules into metarules about expansive versus prototypical meaning.\n\nCourts already recognize a number of interpretive rules, often called \u201ccanons of construction,\u201d that essentially tell interpreters to read statutory terms broadly or narrowly. Examples include the canon calling for narrow interpretation of exemptions from federal taxation, the canon directing that veterans\u2019 benefits statutes should be liberally construed, and the whole act rule directive that provisos (statutory provisions that create exceptions) should be narrowly construed to cover only those items that clearly fall within the exception. Such canons rather easily could be transformed into metarules, or ordinary meaning subcanons, dictating that tax exemptions should be given their prototypical rather than expansive meaning, that veterans\u2019 benefits statutes should be construed in terms of their expansive meaning, and that provisos should be given their prototypical meaning. A canonical direction to construe a statute narrowly is, in essence, a command to give the statute\u2019s terms only those meanings that clearly fall within its core (or prototypical) coverage; likewise, a directive to construe a statute liberally is, in essence, a command to construe the statute\u2019s terms expansively to encompass all that they reasonably can cover. Accordingly, courts should be able to convert many existing canons into metarules about prototypical versus expansive meaning in a relatively straightforward manner.\n\nOnce such metarules are established \u2014 whether by Congress or the courts \u2014 those rules in turn could be used to guide interpreters\u2019 reliance on dictionaries or corpus linguistics as external aids to ordinary meaning. That is, where metarules dictate that a statute should be interpreted in light of its prototypical meaning, interpreters might use corpus linguistics as an interpretive aid; whereas where metarules indicate that a statute should be interpreted in light of its expansive meaning, interpreters might instead consult dictionary definitions as an external aid.\n\nOrdinary meaning metarules, or subcanons, of the kind recommended above would have a number of interpretive advantages. First, they would cabin the universe of potential ordinary meanings among which judges can choose \u2014 and thus reduce judicial discretion and opportunities for decisionmaking based on ideology or personal policy preferences. Second, and relatedly, metarules would help ensure consistency in the interpretation of the same statute. Under our current system of statutory interpretation, there is no methodological stare decisis (at least in federal courts) dictating that once one provision of Statute A has been construed using X, Y, and Z canons or tools, other provisions of that same statute must also, in the future, be construed using the same canons or interpretive tools. Thus, a court may interpret one provision of Title VII using legislative history and statutory purpose but later interpret another provision (or even another term in the same provision!) using the whole act rule and dictionary definitions. And there is no guidance dictating that if a court gives a term in a particular statutory provision its prototypical meaning in Case 1, it must give another provision in that same statute or even another term in that same statutory provision its prototypical meaning in Case 2. Thus, courts \u2014 including the U.S. Supreme Court \u2014 are currently free to construe the term \u201cnational origin\u201d in Title VII\u2019s list of prohibited bases for employment discrimination in light of its prototypical meaning but to construe the term \u201creligion\u201d in that same list of prohibited bases for discrimination expansively, in light of its legalist meaning. That is, courts may limit the term \u201cnational origin\u201d to its core applications, leaving out borderline cases, while interpreting the term \u201creligion\u201d \u2014 which appears just three words away from \u201cnational origin\u201d in the same statutory sentence \u2014 broadly to encompass borderline applications.\n\nIf, however, Congress or the Court were to establish a metarule dictating that Title VII (or civil rights or antidiscrimination statutes generally) should be interpreted in light of its legalist (or prototypical) meaning, then all of the terms appearing in Title VII\u2019s employment discrimination provision would have to be construed using the same kind of ordinary meaning \u2014 that is, prototypical or legalist. We do not have to look far to realize the practical implications of this kind of metarule: one way of understanding Justices Gorsuch\u2019s and Kavanaugh\u2019s opposing opinions in the recently decided Bostock v. Clayton County case is as a clash over precisely this kind of legalist versus prototypical meaning analysis of the statutory phrase \u201cbecause of sex.\u201d In that case, Justice Gorsuch (and a majority of the Court) concluded that the phrase should be given an expansive, or legalist meaning, to include discrimination based on sexual orientation or gender identity; whereas Justice Kavanaugh (and two other dissenters) concluded that the phrase should be limited to its prototypical meaning, which would not include such discrimination.\n\nB. Prima Facie Ambiguity\n\nTobia\u2019s experimental findings also suggest two other possible metarules that could help clarify, or standardize, the role played by ordinary meaning analysis in statutory interpretation. In particular, his findings that laypeople (and judges) often disagree about the ordinary meaning of specific words and that dictionaries and corpora often produce conflicting ordinary meanings suggest that courts perhaps should be more willing, more often, to conclude that a given statute is ambiguous. There are at least two ways in which such a suggestion could be operationalized. First, interpreters could check dictionary definitions and corpus linguistics research results against each other: if and when they find conflicting meanings, that divergence itself could be taken as prima facie evidence that the statute has no plain or ordinary meaning \u2014 and that interpreters should move on to other interpretive tools to determine the meaning of the term at issue. This approach would have the effect of shifting the court\u2019s focus, in cases where dictionary definitions and corpus linguistics produce different meanings, from identifying a statute\u2019s ordinary meaning to identifying the meaning that makes the most sense in light of the statute\u2019s other provisions or structure (the whole act rule), logical deductions encompassed in language canons, its purpose or legislative history, policy norms embodied in substantive canons of construction, and so on.\n\nA second, alternate approach would be for courts to use the lack of a clear or internally consistent collective intuition among laypeople or judges as prima facie evidence that a statute has no clear meaning. That is, if laypeople or judges demonstrate substantial disagreement about a term\u2019s ordinary meaning, that fact itself could be taken as prima facie evidence that the statutory term at issue is ambiguous \u2014 and that courts must determine statutory meaning based on other canons or interpretive tools. Judicial disagreement is relatively easy to measure; divergence among the judges on an appeals court or lower court splits over a statute\u2019s ordinary meaning could be used as gauges. Indeed, a few state court judges have suggested, in opinions issued in specific cases, that this kind of judicial disagreement should be taken as conclusive evidence of ambiguity. Conversely, lower court consensus could be taken as evidence that there is substantial agreement among judges about the statute\u2019s ordinary meaning.\n\nWith respect to laypeople, Tobia\u2019s experimental study and others like it suggest the possibility that courts might take robust survey data into account as evidence of the lack of a clear ordinary meaning. That is, if a survey of laypeople indicates that there is substantial disagreement over the ordinary meaning of a statutory term \u2014 perhaps defined by data indicating that less than sixty-five percent of those surveyed agreed on the same meaning \u2014 courts should consider that prima facie evidence that the statute lacks a clear meaning. There is an emerging literature suggesting that survey data should be used to determine the actual ordinary meaning of terms in contract and statutory interpretation; my suggestion is to instead use such data to determine whether a statutory term or phrase has a readily identifiable ordinary meaning at all.\n\nThere is, of course, a risk of opportunistic survey design or data presentation once the stakes of a particular survey question are known in an actual, live case. This suggests that litigants and their attorneys should not be the source of such survey data. But there are other, more reliable ways to obtain such data. For example, academics could conduct surveys similar to Tobia\u2019s and submit them to the court in amicus briefs. Academics could even seek to head off potential allegations of bias by testing several terms in well-known statutes ex ante, ahead of litigation \u2014 and then publishing or making those survey results available independent of pending lawsuits. Alternately, law clerks or long-term court employees or special masters could be trained to perform such surveys and could conduct them when cases arise.\n\nA second, less radical metarule suggested by Tobia\u2019s data is that judicial, lay, or dictionary-versus-corpora disagreement over a statute\u2019s ordinary meaning could be used to make threshold determinations about statutory clarity with respect to canons that are triggered only when a statute is deemed to be ambiguous. Sometimes referred to as \u201cclarity doctrines,\u201d such canons include interpretive rules like the rule of lenity, the canon of constitutional avoidance, and the first step of the Chevron deference test. Clarity doctrines have received a fair amount of academic attention recently. Justice Kavanaugh and Professor Lawrence Solan, for example, have argued forcefully for eliminating (or at least curbing) reliance on clarity doctrines, on the grounds that the question of whether a statute is clear or ambiguous is often itself ambiguous. Others have suggested that courts establish clarity thresholds akin to confidence levels \u2014 for example, ninety percent confidence that they have identified the correct reading of the statute \u2014 before a statute may be declared \u201cclear.\u201d\n\nTobia\u2019s experimental study, by contrast, suggests a possible measure of clarity that is far more concrete than any previously recommended measure and one that could mitigate concerns about \u201cthe ambiguity of ambiguity\u201d that have motivated some to advocate abandoning clarity doctrines altogether. That is, courts could use one or more of the indicators of disagreement over ordinary meaning discussed above as prima facie evidence of ambiguity sufficient to trigger the relevant clarity doctrine. In other words, where judges, laypeople, or dictionaries-versus-corpora are split over a statutory term\u2019s ordinary meaning, that disagreement itself could be taken as presumptive evidence that the statute is ambiguous \u2014 and that the rule of lenity, avoidance canon, Chevron deference, or other relevant clarity doctrine should be applied to decide the statutory question at issue. Of course, courts would have to establish ex ante what level of disagreement among judges, laypersons, or dictionaries-versus-corpora would suffice to trigger the prima facie presumption. But once that determination is made, Tobia\u2019s experimental study suggests several ways to measure the level of agreement. As a starting point, I would propose something like a two-thirds majority rule \u2014 requiring that at least sixty-five percent of judges or laypersons who have considered the statutory question agree on its ordinary meaning in order for the statute to be considered \u201cclear.\u201d This would mean that where judges or laypersons split anywhere in the range from 50%\u201350% to 64%\u201336% about the statute\u2019s meaning, interpreters should turn to default rules designed to apply in the absence of textual clarity.\n\nFor dictionaries-versus-corpora, the measure could be simpler \u2014 for example, determining (1) whether definitions from different dictionaries produce consistent outcomes; (2) whether the meanings identified across different relevant corpora are consistent; and (3) if different dictionaries and corpora both produce internally consistent meanings, comparing the meaning produced by dictionaries to the meaning produced by corpora. In cases where different dictionaries or different corpora suggest inconsistent meanings, that internal inconsistency itself could be taken as evidence of ambiguity. Where different dictionaries and different corpora overwhelmingly (again, sixty-five percent could be used as a measure) produce a consistent meaning, the meaning generated by each source could then be compared for consistency; where consistency is found, the statute would be declared \u201cclear,\u201d and where inconsistency is found, it would be declared ambiguous.\n\nConclusion\n\nThe experimental study described in Tobia\u2019s Testing Ordinary Meaning is rich and full of valuable data about how laypersons judge ordinary meaning \u2014 and about the specific kinds of ordinary meaning that dictionaries and corpus linguistics tend to measure. In my view, it oversells a bit regarding the breadth of the information it provides about how judges judge ordinary meaning, as judges were asked to evaluate the meaning of only one of ten terms about which laypeople were surveyed. As a result, it is difficult to tell whether judges and nonexperts really do judge ordinary meaning similarly, or if the way they evaluated the one statutory term studied across all three groups (\u201cvehicle\u201d) was anomalous. This observation is important because it implicates significant questions about who the relevant audience, or ordinary reader, is for specific statutes: for some statutes, the answer may be ordinary citizens, or laypeople, but for others it may be judges. The lack of deeper data about judges should not, however, detract from what is otherwise a splendid article. Tobia\u2019s data regarding the intersection between how laypersons, dictionaries, and corpus linguistics define ordinary meaning is incredibly useful and important. Indeed, both the data and methodological approach of his study suggest numerous possible ways that courts or legislatures might narrow the universe of potential ordinary meanings, ex ante, for specific statutes \u2014 in order to cabin judicial discretion and promote greater predictability in statutory interpretation. I have suggested a few such possibilities in this Response; it is my hope that Tobia\u2019s article will prompt further discussion about how best to use dictionaries and corpus linguistics \u2014 and how best to define what is meant by \u201cordinary meaning\u201d \u2014 in the future.\n\n* Mary C. Daly Professor of Law, St. John\u2019s University School of Law, Visiting Professor of Law, Georgetown University Law Center. I owe deep thanks to Deborah A. Widiss and Tara Leigh Grove for valuable comments on earlier versions of this Response. Special thanks also to Dean Michael A. Simons and St. John\u2019s University School of Law for generous research support. All errors are my own."
        },
        {
            "authors": [],
            "title": "Dynamic type systems aren't even simpler \u00ab hisham.hm",
            "contents": "\ud83d\udd17 Dynamic type systems aren\u2019t even simpler\n\nAlexis King just published a great blog post titled \u201cNo, dynamic type systems are not inherently more open\u201d.\n\nThat reminded me of the talk I gave last year at FOSDEM, titled \u201cMinimalism versus types\u201d, where I advocated for static types from a slightly different angle. I tried to convince people that types in dynamically-typed programs are often more complicated than people realize. And often more complicated than in typical statically-typed languages.\n\nPeople often say the opposite, that static type systems are more complicated, and dynamically-typed languages are simpler. At the surface level, this seems true: in a dynamic world you just go merrily declaring variables, assigning values and doing things with them, without ever having to write down any types, no matter how trivial or complex they are. Things can\u2019t get any simpler in the typing department than \u201cdoing nothing\u201d, right?\n\nWell, types are nothing more than the shapes and allowed behaviors of your data. So it\u2019s not like you don\u2019t have shapes and behaviors in any program, regardless of the language\u2026 so, you have types, whether you write them or not. They are there, even in assembly language, even if at a conceptual level, as the sets of \u201cvalid values\u201d your program can manipulate. And you have to think about them, and they have to make sense, and they have to do the right thing. So, in short, in a correct dynamically-typed program the types need to be just as correct as they are in a statically-typed one (or else you\u2019ll get runtime errors).\n\nIn other words, the types are there, but you have to run the type checker in your head. And you know what\u2019s funny? When people don\u2019t write down the types, they often end up with types that are often more complicated than the types from people who do write them. The complexity just goes under the radar, so it piles up.\n\nOne day you open that module which you haven\u2019t touched in six months, and you see a function call where the third argument is null . You need to remember what kinds of variables you can pass to that third argument, or read the whole source code to figure it out. You follow through the code to see all places that third argument is used and realize the accepted type of the third argument depends on what you give to the second argument. Congratulations, you\u2019re dealing with a dependent type, which means you\u2019ve just surpassed Haskell in terms of type system complexity. Compilers that deal with this kind of type system are so complex they are effectively proof assistants (and are at the forefront of programming language research), and here you are dealing with those data types with your brain (and your faith in your ability to come up with sufficient tests) alone.\n\nGiven that there is no mechanical type checker to prescribe what is expressible, and that the dynamic runtime will accept anything as long as the program doesn\u2019t crash, when doing typechecking in your head you essentially have the world\u2019s most powerful and complicated type checker at your disposal. And once you start making use of this power, you end up dealing with the world\u2019s most complicated type system.\n\nAnd when you give people expressive power, they use it. In my experience, people go wild constructing complicated structures in dynamic languages that they normally wouldn\u2019t in static languages. It\u2019s not that static languages are less powerful (Turing equivalence, blah blah), but they make the things you\u2019re doing more obvious to you (Alexis\u2019s post has some great examples). In a dynamically-typed program people are all to keen to make variables and data structures perform double or triple duty (\u201cthis is a X but also a Y under circumstances Z\u201d), but when they have to write down what they\u2019re doing as types, it\u2019s like a little conscience check, and they think twice before creating a more complex type for something that could be described in a simpler way (simple example: they\u2019ll probably make two plain functions instead of making one function that takes a string argument that changes the behavior of other arguments). Static types nudge you towards simpler, less \u201cclever\u201d solutions (and we all know what kind of solution is more maintainable in the long run).\n\nBut okay, let\u2019s assume we avoid \u201cclever\u201d and pick the same solutions in either. Writing the same program in a static or a dynamic language to process the same data in the same way, you will end up with values of roughly the same types in both. The fact that the variables have static types or not does not change that.\n\n\u201cBut in a dynamic language I don\u2019t have to write the types! It\u2019s less work!\u201d\n\n\u201cNot having to\u201d write types but having to think about them anyway is like doing math \u201cnot having to\u201d write anything down and doing all calculations in your head. How is it an advantage to not use pen and paper to track down your train of thought as you do a complex calculation, and instead be restricted to do it only in your head? And how is an advantage to not have a mechanical tool \u2014 like a calculator, which can actually compute the things you wrote down \u2014 to check whether what you wrote with pen and paper makes sense?\n\nI\u2019m lazy, so I hate doing any math in my head. I\u2019ll take writing things down and have a machine check it for me any day of the week. Why wouldn\u2019t I want the same when programming? That\u2019s what computers are for, right? To save us from computing things in our head. So I\u2019ll write my types, and have the compiler check whether they make sense, thank you very much. It\u2019s less work.\n\nFollow\n\n\ud83d\udc26 Twitter \u25aa \ud83d\udc18 Mastodon \u25aa RSS - posts in English, posts em Portugu\u00eas, todos / all\n\nLatest posts\n\nSearch\n\nAdmin area",
            "published_at": "2020-01-20T00:00:00"
        },
        {
            "authors": [
                "Vertical Field",
                "While Paul Is Loath To Reveal His Age",
                "He Will Admit To Cutting His It Teeth On A",
                "Although He Won'T Say Which Version . An Obsessive Fascination With Computer Technology Blossomed Hobinto Career Before Hopping Over To France For Years",
                "Where He Started Work For New Atlas In Now Back In His Native Blighty",
                "He Serves As Managing Editor In Europe."
            ],
            "title": "Vertical farms grow veggies on site at restaurants and grocery stores",
            "contents": "Last month we reported that a huge vertical farming operation near Copenhagen in Denmark recently completed its first harvest. That setup uses hydroponics, but the veggies grown in Vertical Field urban farming pods take root in real soil.\n\nTraditional agricultural farming involves the use of a lot of land and resources to grow crops, and then even more resources to harvest and transport the goods \u2013 sometimes thousands of miles \u2013 to where consumers can get to them.\n\nAs well as requiring a fraction of the growing space, controlled-environment agriculture systems such as hydroponics operations can be much more efficient, are no longer bound by season or location, the growing to harvest cycle is reduced and crops could be produced all year, and as with the Copenhagen operation, they can be set up close to where the food is purchased or consumed.\n\nThe Vertical Field setup retains many of the advantages of hydroponic vertical farms, but instead of the plants growing in a nutrient-packed liquid medium, the container-based pods treat their crops to real soil, supplemented by a proprietary mix of minerals and nutrients. The company says that it opted for geoponic production \"because we found that it has far richer flavor, color, and quality.\"\n\nVertical Field's urban farms grow walls of veggies inside recycled shipping containers Vertical Field\n\n\"Vertical Field offers a revolutionary way to eat the freshest greens and herbs, by producing soil-based indoor vertical farms grown at the very location where food is consumed,\" said the company's CEO, Guy Elitzur. \"Not only do our products facilitate and promote sustainable life and make a positive impact on the environment, we offer an easy-to-use real alternative to traditional agriculture. Our urban farms give new meaning to the term \u2018farm to table,\u2019 because one can virtually pick their own greens and herbs at supermarkets, restaurants or other retail sites.\"\n\nThe recycled and repurposed 20- or 40-ft (6/12-ft) shipping containers used to host the farms can be installed within reach of consumers, such as in the parking lot of a restaurant or out back at the grocery store. Growers can also scale up operations to more than one pod per site if needed, and the external surfaces could be covered in a living wall of decorative plants to make them more appealing.\n\nThe vertical urban farms are claimed capable of supporting the production of a wide range of fruits and veggies \u2013 from leafy greens and herbs to strawberries and mushrooms, and more. And it's reported to use up to 90 percent less water than a traditional farming setup.\n\n\"Through internal experiments with our irrigation method using data from sensors and models we have understood that this is the level of water efficiency,\" Vertical Field's Noa Winston told New Atlas. \"Thus we arrived at an optimal irrigation protocol tailored to the needs of the plant.\"\n\nAccording to the company's website, though pesticide-free, the system is not yet considered organic (though Vertical Field is currently in the process of attaining organic certification for the urban farm unit from the USDA). The crops also grow in a bug-free environment.\n\n\"The container is kept bug-free because it is sealed off, automated, and we limit human entry to only essential people and essential work,\" Winston explained. \"The container farm itself is not a street vendor or a point of sale, therefore unnecessary or frequent entry does not occur.\"\n\nInstalling a Vertical Field urban farm in a grocery store parking lot means that consumers can benefit from fresh veggies all year long Vertical Field\n\nUnlike some high-tech farming solutions, staff won't need special training to work with the vertical farm as the automated growing process monitors, irrigates, and fertilizes the crops as they grow thanks to arrays of sensors that continually feed data on climate, soil condition, LED lighting and so on to management software. Each vertical farm unit has its own Wi-Fi comms technology installed to enable operators to tap into the system via a mobile app.\n\nThe company told us that, by way of example, one container pilot farm offered a growing space of 400 sq ft (37 sq m) and yielded around 200 lb (90 kg) of produce per month, harvested daily. Lighting remained on for 16 hours per day. We assume that the pods are completely powered from the grid at their respective locations, though the company says that it is looking at ways to make use of solar panels as well as making more efficient use of water.\n\nVertical Field has been around since 2006, and has built a number of living green walls around the world since then. The soil-based vertical farm initiative was started in 2019.\n\nRecent installations include the first Vertical Field container farm in the US at a restaurant named Farmers & Chefs in Poughkeepsie, New York, which started producing its own crops of fresh greens in mid-April 2020. Last month, following a successful pilot, Israel's largest supermarket chain, Rami Levy, signed an agreement with the company to roll vertical farms into dozens of store locations over the course of the next five years.\n\n\"The Rami Levy chain understands the social responsibility that it has for customers as related to food security and supplying the highest quality products while maintaining low prices,\" said the chain's Yafit Attias Levy. \"Our customers bought Vertical Field's produce during the pilot, and returned to purchase more. Therefore, we have decided to expand the partnership with Vertical Field to additional branches of the supermarket, and to offer fresh, high-quality, and pesticide-free produce in a way that increases shelf-life for our customers.\"\n\nThe Vertical Field urban farm can produce crops year round, without the use of pesticides Vertical Field\n\nAnd earlier this month, Moderntrendo SRO \u2013 one of the largest agricultural distributors in the Ukraine \u2013 signed up for a pilot project that will start with supermarket chain Varus, and potentially expand to other chains.\n\n\"We are extremely excited about our partnership with Moderntrendo SRO which has led to the project with Varus and will lead to more projects in the near future with more chains in Ukraine,\" Vertical Field's Guy Elitzur said. \"One of the realizations that have surfaced during the COVID-19 crisis is the need to develop solutions that allow urban residents access to healthy food, with minimal human handling and without depending on transportation and shipping from remote locations. We are delighted to be able to provide - and expand access to - healthy, and high-quality vegetables grown right outside the consumer's door.\"\n\nAs well as grocery outlets and restaurants, the company sees its container-based vertical farms also being installed in hotels, universities, hospitals, and so on, in the future. The video below has more.\n\nGrow Vegetables On-Site with Vertical Field\n\nSource: Vertical Field",
            "published_at": "2021-01-20T15:47:18.450000"
        },
        {
            "authors": [],
            "title": "Installing Debian on modern hardware [LWN.net]",
            "contents": "Installing Debian on modern hardware [LWN subscriber-only content]\n\nWelcome to LWN.net The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider accepting the trial offer on the right. Thank you for visiting LWN.net! Free trial subscription Try LWN for free for 1 month: no payment or credit card required. Activate your trial subscription now and see why thousands of readers subscribe to LWN.net.\n\nIt is an unfortunate fact of life that non-free firmware blobs are required to use some hardware, such as network devices (WiFi in particular), audio peripherals, and video cards. Beyond that, those blobs may even be required in order to install a Linux distribution, so an installation over the network may need to get non-free firmware directly from the installation media. That, as might be guessed, is a bit of a problem for distributions that are not willing to officially ship said firmware because of its non-free status, as a recent discussion in the Debian community shows.\n\nSurely Dan Pal did not expect the torrent of responses he received to his short note to the debian-devel mailing list about problems he encountered trying to install Debian. He wanted to install the distribution on a laptop that was running Windows 10, but could not use the normal network installation mechanism because the WiFi device required non-free firmware. He tracked down the DVD version of the distribution and installed that, but worried that Debian is shooting itself in the foot by not prominently offering more installation options: \"The current policy of hiding other versions of Debian is limiting the adoption of your OS by people like me who are interested in moving from Windows 10.\"\n\nThe front page at debian.org currently has a prominent \"Download\" button that starts to retrieve a network install (\"netinst\") CD image when clicked. But that image will not be terribly useful for systems that need non-free firmware to make the network adapter work. Worse yet, it is \"impossible to find\" a working netinst image with non-free firmware, Sven Joachim said, though he was overstating things a bit. Alexis Murzeau suggested adding a link under the big download button that would lead users to alternate images containing non-free firmware. He also pointed out that there are two open bugs (one from 2010 and another from 2016) that are related, so the problem is hardly a new one.\n\nWhile they are hard to find, there are unofficial images with non-free firmware for Debian, as Holger Levsen noted; he also pointed to his 2017 blog post that he uses to rediscover those images when he needs them. It is a rather strange situation; Emanuele Rocca put it this way:\n\nSo the current situation is that we make an active effort to produce two different types of installation media: one that works for all users, and one broken for most laptops. Some sort of FOSS version of an anti-feature. Then we publish the broken version on the front page, and hide very carefully the version that works. This absurdly damages our users without improving the state of Free Software in any way, while Ubuntu puts the firmware back into the images and can rightly claim to be easier to install.\n\nBut Jeremy Stanley took exception to that characterization:\n\nThe one you say \"works for all users\" doesn't \"work\" for me because it contains proprietary closed-source software I don't want. This boils down to a debate over whether the Debian community values convenience over ideals. It can be argued that for users who value convenience more, Ubuntu already exists. Why compete with that and compromise Debian's ideals at the same time?\n\nThat is, of course, the crux of the matter. Debian has a set of ideals about the kinds of software it distributes, enshrined in the Debian Free Software Guidelines (DFSG); non-free licenses do not fit within those ideals. In addition, the Debian Social Contract (which contains the DFSG) specifically notes that \"non-free works are not a part of Debian\". But the problem at hand is that potential users may not even be able to install Debian (or use it once installed) if they cannot access the network; it is hard for some to see how that advances the cause of free software, which is also a part of the contract.\n\nIn response to Stanley, Russ Allbery pointed out that there is a middle ground. No one had suggested removing the official images that do not have the non-free firmware, but there are some interested in making it easier to find the images needed for much of today's hardware.\n\nThe point is to make things easier for our users. Right now, we're doing that for you but not for the users who don't care whether firmware is non-free. I think the idea is that we should consider making things easier for both groups of users. There's no reason to make things worse for you and others who want the fully free installer in the process.\n\nThe official installer does offer the option of installing non-free firmware from a USB drive, \"but very few people use it\", Andrew M.A. Cater said. Allbery described the process he goes through to try to use that mechanism; it is far from straightforward even for someone quite familiar with Debian:\n\nI have always managed to get it to work, but usually it's an hour of cursing and Googling things and making different USB sticks in different formats with different file systems and retrying parts of the installation until I hit on the magic combination of factors to make it work.\n\nOne can only imagine that new users who encounter this wall are unlikely to continue to down the Debian path. Allbery said that an installer with non-free firmware would work much better for him, but he wasn't able to find the specific one he needed (for the \"testing\" version of the distribution). Andrey Rahmatullin said that the inability to find these images is caused by a \"failing of the Debian websites\"; there should be an easier path to find the alternate installation images. Russell Stuart said that he always runs into the same problem that Allbery reported and that, even though Stuart is a strong proponent of the separation of free and non-free in Debian, firmware is a different beast:\n\n[...] these firmware blobs are peculiar. They don't run on the same CPU, we talk to them with strictly open API's and protocols. In that way, they aren't anything new. We already talk to and depend on megabytes of nonfree software to get our laptop's booted, but we tolerate it because it lives in ROM. We don't consider firmware in ROM to be part of Debian even though it must be running for our Debian machines to run.\n\nAfter Paul Wise pointed out that there actually are unofficial images with non-free firmware for the testing distribution, Holger Wansing suggested some changes (and as a patch) for the web site to make it easier for users to find these images when needed. As Marc Haber said, though, the installation experience is likely the first impression a potential new user will get; \"we should not be trying THIS hard to be a failure in this very important part of the relationship our product is building with the user\". But pointing users at the unofficial images is different from Debian officially distributing this non-free firmware, as Steve McIntyre pointed out:\n\nThere's a major difference here - do we want Debian's *official* media to include non-free stuff? We've had this discussion a few times, including in person back at DC15 [DebConf 15] at least. Back then, the overwhelming response was *no*. We can change that, but it's not something to do lightly.\n\nHaber feels strongly that being purists about firmware is only leading to fewer new users. Wise agreed in part: \"the current situation wrt hardware and software freedom is pretty catastrophic\". He suggested making things clearer for users and potential users, perhaps by way of an \"installer launcher app\". That app would analyze the needs of the existing hardware to help guide (and presumably educate) users in their installer choice.\n\nWhile there were lots of ideas of how to make things better, this problem has existed for a long time in Debian. Marco d'Itri said that he had raised the issue back in 2004, but it likely goes much further back than that. Ansgar Burchardt said that in 2016 he had proposed creating a new section in the repository to hold the non-free firmware (separate from the rest of the non-free software), which might be a preliminary step. But consensus was not reached and that effort died on the vine. As with the open bugs, these accounts show that the distribution has been struggling with this issue for quite some time.\n\nAt this point, it is not at all clear what will happen. The discussion may just fade away, only to be picked up again down the road. The problem is real and making the situation better, at least, does not seem all that difficult, nor particularly harmful to Debian's overall goals. But that has been true all along and here we are. It would seem that there has simply not been enough \"push\" to make progress, but with any luck, this time around things will be different."
        },
        {
            "authors": [],
            "title": "FedEx Shipping Damage Creates Fractured Artworks",
            "contents": "Since 2007, artist Walead Beshty has been cleverly using FedEx\u2019s shipping infrastructure to create a series of artworks. He constructs glass objects that fit exactly into FedEx\u2019s shipping boxes and then ships them to galleries and museums without any protection against damage. Any cracks or breaks in the glass became part of the work upon display at its destination. According this interview, part of what interested Beshty about doing this project related to the proprietary sizes of FedEx\u2019s boxes:\n\nAs for the corporate dimension, I was aware that standard FedEx boxes are SSCC coded (serial shipping container code), a code that is held by FedEx and excludes other shippers from registering a box with the same dimensions. In other words, the size of an official FedEx box, not just its design, is proprietary; it is a volume of space which is a property exclusive to FedEx. When thinking about the work, its scale and so on, it made sense to adhere to that proprietary volume, because, as a modular, it had a real and preexisting significance in daily life, it was common, specific, and immediately familiar. That is, it had an iconic resonance that a more arbitrary form or shape wouldn\u2019t have.\n\nAnd each time the work is shipped \u2014 say from one gallery to another \u2014 it\u2019s unwittingly altered further by a system created by a massive multinational corporation:"
        },
        {
            "authors": [],
            "title": "Project Zero: The State of State Machines",
            "contents": "News and updates from the Project Zero team at Google"
        },
        {
            "authors": [],
            "title": "Caitlin Green: Lissingleys, the meeting-place of Anglo-Saxon & Anglo-Scandinavian Lindsey, and the antiquity of Ogilby's 1675 road from Lincoln to Grimsby",
            "contents": "This is the blog of Dr Caitlin Green FSA. It features posts on my main academic research foci alongside other topics that I'm currently working on, including drafts of papers, ideas and similar\u2014these are usually identifiable by the presence of footnotes. You're free to cite these drafts if they are of interest, and are reminded that academic blogs are indeed citable under most citation systems. In addition, the current site also houses posts relating to my personal interests, including long-distance trade, migration and contacts; landscape and coastal history; early literature and legends; and the history, archaeology, place-names and legends of Lincolnshire and Cornwall. For further details of this website & how to contact me, please see the ' About ' page or @caitlinrgreen on Twitter."
        },
        {
            "authors": [
                "Author S"
            ],
            "title": "Fediverse in 2020",
            "contents": "Happy 2021, fedizens!\n\nOne more year in Fediverse passed. Let\u2019s have a look at what we have achieved in 2020.\n\nFediverse in numbers\n\nThe number of servers grew from 5.027 last year to ~5.900 by the end of 2020. That\u2019s about 900 new instances.\n\nThe number of registered users, on the other hand, has decreased and by the end of the year remains slightly above 4.000.000. This may be due to the fact that several large servers went down during the year, and because some administrators periodically remove long inactive accounts to free up space for newcomers.\n\nWhatever the reason, we may rejoice that more small instances keep appearing. That\u2019s what Fediverse is supposed to be doing \u2013 decentralizing!\n\nSeven networks growing most rapidly in online server numbers in 2020 were:\n\n\u2022 PeerTube \ud83c\udf89 +320 servers\n\n\u2022 Mastodon \ud83c\udf89 +237 servers\n\n\u2022 Pleroma \ud83c\udf89 +224 servers\n\n\u2022 WriteFreely \ud83c\udf89 +74 servers\n\n\u2022 Misskey \ud83c\udf89 +58 servers\n\n\u2022 Mobilizon \ud83c\udf89 +45 servers\n\n\u2022 Pixelfed \ud83c\udf89 +40 servers\n\nNote: these numbers are an approximation based on statistics voluntarily provided by server administrators. Servers go offline / online every hour, every day, so the numbers reflect only part of Fediverse at the time this article was published.\n\nCredits: picture is based on an image from Pixabay.com\n\nFediverse in projects\n\nAt least 19 new projects joined Fediverse in 2020!\n\n\ud83c\udf1f Hash2Pub \u2013 a fully-decentralised DHT-based relay for global hashtag federation\n\n\ud83c\udf1f Bonfire \u2013 a plug & play federated social network based on CommonsPub\n\n\ud83c\udf1f Bookwyrm \u2013 social reading and reviewing, a federated alternative to GoodReads\n\n\ud83c\udf1f The Occasion Octopus \u2013 a federated network of Open Data for discovering interesting events\n\n\ud83c\udf1f OLKi \u2013 linguistic corpora exchange platform, a simple gateway to Fediverse for scientific interaction\n\n\ud83c\udf1f Immers Space \u2013 a virtual reality metaverse platform powered by Mozilla Hubs and ActivityPub-Express\n\n\ud83c\udf1f Lotide \u2013 a federated forum / link aggregator\n\n\ud83c\udf1f Tavern \u2013 a minimalistic ActivityPub server\n\n\ud83c\udf1f Learn Awesome \u2013 a review aggregation site generalized to all learning resources organized by topics, formats and difficulty\n\n\ud83c\udf1f OpenEngiadina \u2013 a knowledge base and a social network using ActivityPub and the Semantic Web\n\n\ud83c\udf1f Gathio \u2013 self-destructing, shareable, no-registration event pages\n\n\ud83c\udf1f SemApps \u2013 a collaborative, generic knowledge management system for easing data storage and filtering\n\n\ud83c\udf1f FlockingBird \u2013 network for professionals, a LinkedIn alternative\n\n\ud83c\udf1f SkoHub \u2013 a publication / subscription infrastructure for Open Educational Resources; allows to follow specific subjects and be notified when new content is published\n\n\ud83c\udf1f Lumen AP server \u2013 ActivityPub server using Lumen framework\n\n\ud83c\udf1f Reedlebee \u2013 a viable Goodreads alternative; book reading progress, lists, reviews, comments, and more\n\n\ud83c\udf1f RavenVale \u2013 federating guild website using GuildWars2 API\n\n\ud83c\udf1f XWiki Extension \u2013 an implementation of the ActivityPub for XWiki\n\n\u2192 For a full list of Fediverse projects in development see Miscellaneous page.\n\nProject forks:\n\n\u2022 Mistpark 2020 \u2013 also known as \u201cmisty\u201d, a webserver app that supports ActivityPub and Zot protocols, fork of Zap\n\nRelated new projects:\n\n\u2022 SepiaSearch \u2013 search engine for PeerTube\n\nFediverse 2020 timeline\n\n\u2714 January 6: Smithereen open sources its codebase\n\n\u2714 January 23: ForgeFed gets funded by NGI Zero Discovery Fund established by NLnet with financial support from the European Comission\u2019s Next Generation Internet program\n\n\u2714 January 30: Funkwhale receives funding from NGI Zero (NLnet)\n\n\u2714 February: Simon Urli announces plans to add ActivityPub to XWiki, an open-source Wiki platform; receives funding from NGI Zero (NLNet)\n\n\u2714 April: The French ministry deploys about 35 PeerTube instances, creating a vast platform of educational videos without tracking and advertisements\n\n\u2714 April 7: New project OpenEngiadina joins the scene. The project was supported by NGI Zero (NLnet)\n\n\u2714 May: Mastodon\u2019s creator announces that some progress has been made on adding end-to-end encryption for direct messages in Mastodon. By the end of 2020, this is a work in progress\n\n\u2714 May 16: PixelDroid, an Android client for Pixelfed, prepares for an alpha release\n\n\u2714 May 18: Fediverse \ud83d\udc23 celebrates its 12 birthday\n\n\u2714 June 23: Lemmy project is funded by NGI Zero (NLnet)\n\n\u2714 July 23: Masto.host, the biggest provider for Mastodon, experiences a major DDoS attack\n\n\u2714 August: Google threatens to remove several popular Fediverse apps from Google Play on the pretext that servers in Fediverse engage in hate speech and users can be exposed to it via these apps. Fedilab, a popular multi-purpose Fediverse app, receives Google warning too. This brings a wave of rage from users who blame Google for double standards and demand to also ban Twitter apps and Chrome, as all of them expose users to hate speech. Heated discussions on HackerNews, Reddit and other tech outlets follow\n\n\u2714 August 8: @cj creates an \ud83d\udca1 ActivityPub library for developers where one can find articles about ActivityPub protocol and other technical topics\n\n\u2714 September 09: Funkwhale releases first stable version\n\n\u2714 September 15: Mario Vavti, Hubzilla core developer, achieves a grant from NGIZero (NLnet)\n\n\u2714 September 22: Framasoft introduces SepiaSearch - a search engine to help discover videos and channels on PeerTube\n\n\u2714 September 30: Pixelcat, Android client for Pixelfed, Mastodon and Pleroma, gets traction\n\n\u2714 October: \ud83d\udc4d Debian donates 10.000 Euros to fund free and decentralized live-streaming in PeerTube\n\n\u2714 October 1: Bonfire releases its official website explaining what the project is about\n\n\u2714 October 2-5th: A conference about the present and future of ActivityPub takes place in Barcelona. \ud83d\udca1 Watch the talks\n\n\u2714 October 3: SepiaSearch is now integrated in Searx\n\n\u2714 October 20: Lemmy starts federating via ActivityPub\n\n\u2714 October 27: Mobilizon stable version 1.0 and a guided tour of the project are released\n\n\u2714 November: Streaming is coming to PeerTube, a feature awaited by many\n\n\u2714 November 18: Funkwhale is looking for new maintainers\n\n\u2714 December 25: Socialhome project adds alpha support for Matrix\n\n\u2714 December: An ActivityPub conference happens during rC3, \ud83d\udca1 watch the talk about Funkwhale.\n\nIs any important 2020 event missing? Feel free to suggest it in issues or send a suggestion in a direct message.\n\nFediverse is mostly run by volunteers who spend their own money to keep the network going. If you enjoy being on Fediverse, please, consider sponsoring your own local server or donating to Fedi project you most often use."
        },
        {
            "authors": [],
            "title": "Tejotron",
            "contents": ""
        },
        {
            "authors": [
                "University Of Tokyo"
            ],
            "title": "First-ever atomic resolution video of salt crystals forming in real time",
            "contents": "A sodium chloride crystal growing in a vibrating carbon nanohorn. Credit: \u00a9 2021 American Chemical Society\n\nTwo novel techniques, atomic-resolution real-time video and conical carbon nanotube confinement, allow researchers to view never-before-seen details about crystal formation. The observations confirm theoretical predictions about how salt crystals form and could inform general theories about the way in which crystal formation produces different ordered structures from an otherwise disordered chemical mixture.\n\nCrystals include many familiar things, such as snowflakes, salt grains and even diamonds. They are regular and repeating arrangements of constituent molecules that grow from a chaotic sea of those molecules. The process of growth from this disordered state to an ordered one is known as nucleation, and although it has been studied for centuries, the exact goings-on at the atomic level have never been experimentally confirmed, until now.\n\nIt's not just enough to be able to see molecules at the atomic level\u2014that ability has been with us for a few decades now. The thing about a crystal's growth is, it's a dynamic process and observations of its development are as important as observations of its structure. Luckily, researchers at the Department of Chemistry at the University of Tokyo solved this with their single-molecule atomic-resolution real-time electron microscopy technique, or SMART-EM. This captures details of chemical processes at 25 images per second.\n\n\"One of our master's students, Masaya Sakakibara, used SMART-EM to study the behavior of sodium chloride (NaCl)\u2014salt,\" said Project Assistant Professor Takayuki Nakamuro. \"To hold samples in place, we use atom-thick carbon nanohorns, one of our previous inventions. With the stunning videos Sakakibara captured, we immediately noticed the opportunity to study the structural and statistical aspects of crystal nucleation in unprecedented detail.\"\n\nSodium chloride growth in action. Credit: \u00a9 2021 American Chemical Society\n\nNakamuro and his team looked at the videos Sakakibara had captured and were the first people ever to see tiny cuboid crystals made of tens of molecules of NaCl emerging from the chaotic mixture of separate sodium and chloride ions. Straight away, they noticed a statistical pattern in the frequency at which the crystals emerged; it followed what's known as a normal distribution, which has long been theorized but only now experimentally verified.\n\n\"Salt is just our first model substance to probe the fundamentals of nucleation events,\" said University Professor Eiichi Nakamura. \"Salt only crystallizes one way. But other molecules, such as carbon, can crystallize in multiple ways, leading to graphite or diamond. This is called polymorphism and no one has seen the early stages of the nucleation that leads to it. I hope our study provides the first step in understanding the mechanism of polymorphism.\"\n\nSodium chloride growth in action. Credit: \u00a9 2021 American Chemical Society\n\nThe team doesn't just have diamonds in mind though; polymorphism in crystal growth is an essential process in the production of some pharmaceutical and electronic components too.\n\nExplore further Development of a new technique for growing high-quality gallium nitride crystals\n\nMore information: Takayuki Nakamuro, Masaya Sakakibara, Hiroki Nada, Koji Harano, Eiichi Nakamura. Capturing the Moment of Emergence of Crystal Nucleus from Disorder. Journal of the American Chemical Society. DOI: 10.1021/jacs.0c12100 Journal information: Journal of the American Chemical Society Takayuki Nakamuro, Masaya Sakakibara, Hiroki Nada, Koji Harano, Eiichi Nakamura. Capturing the Moment of Emergence of Crystal Nucleus from Disorder."
        },
        {
            "authors": [
                "Thomas Burghardt"
            ],
            "title": "SpaceX acquires former oil rigs to serve as floating Starship spaceports",
            "contents": "SpaceX has acquired two former oil drilling rigs to serve as these floating spaceports. Named Phobos and Deimos, after the two moons of Mars, they are currently undergoing modifications to support Starship launch operations.\n\nSpaceX has long been hinting at future floating launch and landing sites for their Starship launch system. The super heavy lift launch vehicle will have a large blast danger area and pose noise concerns if launched frequently near populated areas. Therefore, sea launch platforms will play a key role in the launch cadence SpaceX plans to reach with Starship, including on-orbit refueling flights for deep space missions and transportation from one place to another on Earth.\n\nJob postings by SpaceX have indicated that work on offshore launch platforms has begun in Brownsville, Texas, near their Starship manufacturing and launch facilities in Boca Chica.\n\nPositions included crane operators, electricians, and offshore operations engineers, and several of the job listings specified that the position was part of the company\u2019s Starship program. Job descriptions for these positions included responsibilities like \u201cdesigning and building an operational offshore rocket launch facility\u201d and required the \u201cability to work on an offshore platform in Brownsville, Texas.\u201d\n\nI've been exploring around the Port of Brownsville while waiting for Starship testing and found an oil rig that appears to be named Deimos, after one of the moons of Mars! Based on job postings and @elonmusk's tweets, I'm willing to bet that SpaceX is involved. @NASASpaceflight pic.twitter.com/zhTOGNnZKd \u2014 Jack Beyer (@thejackbeyer) January 19, 2021\n\nThe Port of Brownsville is home to a handful of oil drilling rigs, as drilling operations are regularly conducted in the Gulf of Mexico. One rig in port was photographed by NASASpaceflight before and after a sign with the name Deimos was added in recent months.\n\nPhotographs of the same rig showed that it was previously named ENSCO 8500, owned and operated by offshore drilling company Ensco Rowan PLC.\n\nEnsco Rowan merged and became Valaris PLC in 2019. The rig, then named Valaris 8500, was sold with another rig named Valaris 8501 to an undisclosed buyer in August 2020 when Valaris filed for bankruptcy. The two rigs sold for $3.5 million each, and as it turns out, the undisclosed buyer was SpaceX.\n\nBoth rigs have been officially renamed Deimos (formerly ENSCO/Valaris 8500) and Phobos (formerly ENSCO/Valaris 8501), and are now owned by Lone Star Mineral Development LLC. Lone Star was incorporated in June 2020, just before the two rigs were purchased, and a principal of the company is Bret Johnsen, who is also the CFO and President of the Strategic Acquisitions Group at SpaceX.\n\nSpaceX CEO Elon Musk had tweeted that \u201cSpaceX is building floating, superheavy-class spaceports for Mars, moon, & hypersonic travel around Earth\u201d the same month that Lone Star was incorporated. It appears that Lone Star Mineral Development LLC is a subsidiary of SpaceX.\n\nFollowing up on @thejackbeyer's find, I can confirm that Deimos and Phobos are the names of two oil rigs purchased by SpaceX \u2013 likely for conversion to support Starship operations. ENSCO 8500 and ENSCO 8501 were the previous names of the rigs. They are nearly identical twins. \u2014 Michael Baylor (@nextspaceflight) January 19, 2021\n\nSpaceX\u2019s job posting for crane operating positions in Brownsville mentioned Seatrax S90 cranes by name as one of the types an operator would be using. This same type of crane is the primary model used on the ENSCO 8500 and 8501 series rigs.\n\nThe Phobos rig was seen via satellite imagery in the Port of Galveston on January 13, 2020. Galveston is near Houston, Texas, to the northeast of Brownsville and Boca Chica.\n\nThe idea for offshore launch and landing facilities was first presented officially by SpaceX in 2017, when Elon Musk revealed Starship\u2019s \u201cEarth to Earth\u201d transportation plan. Launching cargo and passengers on suborbital flights around Earth requires operating loud launch vehicles from locations accessible from major cities. Thus, the spaceports would be placed far enough offshore to mitigate noise from Starship\u2019s Raptor engines as well as the sonic booms created by landing stages, both the Starship spacecraft and the Super Heavy booster.\n\nOffshore launch facilities can also help alleviate congestion and noise levels from SpaceX\u2019s land based Starship facilities at Boca Chica, Texas and the Kennedy Space Center in Florida.\n\nThe primary driver of Starship\u2019s Development is enabling missions to the moon and Mars, and those flights will require on-orbit refueling using a tanker variant of Starship in Low Earth Orbit. Delivering crew and/or cargo to the moon will require at least one refueling, and missions to Mars may require multiple refueling flights.\n\nSpaceX also plans to eventually send multiple Starships to Mars during a single interplanetary transfer window. These flights will be in addition to perhaps hundreds of Starship missions to Earth orbit before carrying any people.\n\nDeimos, one of the oil rigs SpaceX is planning on converting into a floating Spaceport, as seen from the air today, 01-19-21. Its name has been painted on the helipad and lifeboats, so cool. @elonmusk @NASASpaceflight pic.twitter.com/y9VtwJqnFy \u2014 Jack Beyer (@thejackbeyer) January 19, 2021\n\nAll of these factors result in a rapid Starship launch cadence which would be difficult to serve using only the two land based launch and landing sites. Phobos and Deimos will provide two more facilities for this purpose.\n\nThe two rigs will require extensive modification to support fueling, payload integration, launch, and landing operations. As this work continues, SpaceX teams at Boca Chica are continuing to test prototype Starship vehicles and construct the first Super Heavy boosters.\n\nBased on the extensive work still needed to prepare the rigs, Phobos and Deimos will likely enter service after the initial orbital flights of the Starship launch system.\n\nThe first orbital Starship launch from Boca Chica could occur in late 2021, pending successful Starship and Super Heavy testing throughout the year.\n\nLead photo via Jack Beyer for NSF\n\n\n\n",
            "published_at": "2021-01-19T22:00:46+00:00"
        },
        {
            "authors": [],
            "title": "Librem 5 Update: Shipping Estimates and CPU Supply Chain \u2013 Purism",
            "contents": "It\u2019s been a busy holiday and New Year\u2019s season at Purism as we continue to ship out Librem 5s to backers each week. We know for those who haven\u2019t received their Librem 5 yet, what they most want to know is when their Librem 5 will arrive. In summary, we will be providing shipping estimates within the next week to the backers within the original crowdfunding campaign (orders through October 2017), but not all backers yet, based on our confidence in the estimates. The rest of this post will explain what is going into our shipping estimates, and why we can\u2019t yet provide shipping estimates to every backer.\n\nWhen we published the shipping FAQ we explained some of the factors in the shipping calculation:\n\nThat calculation depends not only on their place in line, but also on our knowing our average and maximum weekly phone throughput in advance, which we don\u2019t expect to know until we are at least a few weeks into the process. We expect to have a good idea on these projections by the end of the year, however.\n\nNow we are happy to say that we not only have a good idea on our shipping throughput, we actually exceeded our expectations for how many we could ship! So hopefully by the end of this week, or possibly the beginning of next week, we will be contacting a large group of backers who we feel we can provide a reliable shipping estimate. Note that this will be a separate email from the emails we already send out each week to confirm shipping information to the next group of backers who are ready to receive their Librem 5.\n\nThe Road to Shipping Parity\n\nBack when we published the shipping FAQ, we expected that by this point we would be able to provide every backer with an accurate shipping estimate and be able to predict when we would hit shipping parity\u2013the moment when all of the backlog has cleared and a new order would be fulfilled in our standard 10-business-day window. Once you know how many Librem 5s you can ship in a week, it seems like it would be a relatively straightforward calculation to apply that to a person\u2019s place in line and estimate a shipping date.\n\nMaking Librem 5 Just In Time\n\nIn our case the calculation is a little more complicated due to the fact that we employ a \u201cJust In Time\u201d manufacturing process for the Librem 5s, which is pretty common in the industry. We estimate our shipping throughput and make slightly more Librem 5s than we think we can ship in a period of time. The next manufacturing run of Librem 5s then arrives around the time we complete shipping out the previous run. This has a few benefits, but the main benefit is if we were to identify a hardware problem in the existing Librem 5 manufacturing process (whether a systemic flaw, or a flaw in a particular manufacturing run) it impacts a smaller number of Librem 5s and can be fixed for future batches.\n\nSo when making these shipping estimates, we not only factor in our shipping throughput, but also the size of future manufacturing runs, which we now are increasing based on the fact we\u2019ve exceeded our initial estimates. We can then calculate which run a particular order would be in, when we will make that next set of Librem 5s, and be able to estimate when a particular Librem 5 will ship. We also factor in and plan for events like Chinese New Year, which cause essentially everything in China to shut down for a few weeks.\n\nCPU Supply Chain\n\nOne downside to using Just In Time manufacturing is that you must factor in all of the different lead times for all the different individual components that go into the Librem 5. While some components have relatively short lead times, others sometimes have lead times extending out multiple months. You have to factor all of this in to ensure that everything is ordered in advance so that it arrives just when you need it.\n\nIf you talk to anyone in manufacturing they will tell you that this has been a particularly challenging year for the supply chain. Whether you are talking about toilet paper, N95 masks, rubber gloves, or semiconductors, the global pandemic has made supply chains less reliable, and lead times and shipping times incredibly unpredictable. It\u2019s left everyone in the industry scrambling from source A to B to C down to Z sometimes to find inventory. It even added a delay a few months back to our Librem 14 timeline due to Intel having trouble fulfilling all of their CPU orders.\n\nOur customers have told us they want ever more information on what happens behind the scenes of making a phone like the Librem 5, so in the interest of transparency we are sharing what we\u2019ve been hearing from our own suppliers. The iMX-8 processor we use in our Librem 5 is also popular in the automotive industry, and currently NXP has been hit with a global semiconductor shortage due to a dramatic increase in demand from auto makers.\n\nThis shortage has increased the lead times for CPU orders, which is of course a critical component in the Librem 5. As we started getting word about this shortage we were proactive in sourcing and purchasing all the CPUs we can, and continue to do so, while also factoring these increased lead times into future orders.\n\nWhat Does This Mean For Me?\n\nWhat does this mean for you? Based on our efforts thus far there\u2019s a good chance it will not affect your shipping time as we continue to track down new CPU supplies and plan for future manufacturing runs. So far it hasn\u2019t caused a delay.\n\nHowever we wanted to let everyone know about this potential issue far in advance, because it will impact how many people get shipping estimates. We only want to send shipping estimates when we know for sure we have the CPUs to fulfill them, so this week instead of sending estimates to everyone like we had planned, we are only sending estimates out up to the point we have CPUs that will arrive just in time. This happens to coincide with all the orders placed through October 2017\u2013the end of our original crowdfunding campaign.\n\nAs we secure more CPU supply, and feel confident about the supply chain for future manufacturing runs we will send out additional shipping estimates. Hopefully soon we will be able to account for the whole backlog and can calculate when we hit shipping parity.\n\nCertification Update\n\nWe\u2019ve also gotten some questions about the various hardware certifications for the Librem 5 including Respect Your Freedom (RYF), FCC and CE. While we designed the Librem 5 to qualify for each of these certifications, we had to wait to start the certification processes until we had the final mass-produced \u201cEvergreen\u201d Librem 5 since changes in the hardware would require re-certification.\n\nEach of these certification processes are under way. While the transmitters in the Librem 5 (the removable cellular modem and WiFi card) already have FCC and CE certification, we are seeking certification for device as a whole. We are still in the middle of these time-consuming certification processes and will post an update to our site when there is any news on any of these fronts.\n\nThank You\n\nWe want you to have your Librem 5 as soon as possible and appreciate everyone\u2019s patience as we continue to process orders and get through our backlog. It\u2019s everyone\u2019s support through this monumental process that has made the Librem 5 a reality.",
            "published_at": "2021-01-12T22:29:22+00:00"
        },
        {
            "authors": [],
            "title": "How To Use White-Labeling To Create Profitable Apps",
            "contents": "As branding strategies evolve, tech companies continue to find ways to use white-labeling to create profitable apps. White-labeling refers to creating a foundational product or service that can be rebranded and further customized by other companies. This model provides a way to service many brands with one product that still looks unique to end users. In a previous blog post on fintech and mobile app development, we discussed the capacity of white-labeling to improve app integration and deliver data-driven tools. While there are many advantages associated with the decision to white-label a product, it is not without its challenges. We share our insights on the topic below. Learn how to use white-labeling to create profitable apps given the inherent strengths and limitations of the model.\n\nPros and Cons of White-labeling\n\nAlthough the practice of white-labeling apps exists across various industries, the business model for all brands is the same. Suppose you want to create a white-labeled \u201call resorts\u201d app to service property management companies. This app would need to include features that appeal to virtually all resort managers. For example, the app could include the ability to view specific amenities of a resort upon selection. Property managers could further customize the app each time a resort is added to the product.\n\nAlthough creating one product that services multiples resorts is more efficient, this technique has its limitations. A significant con to white-labeling is the appearance of a lack of uniqueness. Using the all resorts app as an example, this product may be less appealing to individual resorts since they don\u2019t want to be listed side-by-side in the same app with their competitors. Resorts would prefer to own their customer relationship and create the impression that they\u2019ve done something unique to serve them.\n\nOn the other hand, the model benefits property managers because it is more efficient to maintain one app. It also simplifies things for users since they only need one app to access information about multiple resorts. White-labeled products also cut out the time and expense required to build users\u2019 trust. Instead, companies deliver customized products to their existing loyal customer base. A customer who is familiar with a particular property management company is more likely to book a resort from its app. For this reason, white-labeling is especially useful to industries in which trust is paramount. This is why financial and medical institutions rely heavily on white-labeling.\n\nConsiderations for Developing a Profitable White-labeled Product\n\nLimitations on brand-specific features\n\nLimitations on brand-specific features substantially impact how producers use white-labeling to create profitable apps. Brands invariably request new features that may only be relevant to them. Offering no brand-specific customization is easiest for white-label providers. The more brand-specific features added to an app, the more difficult it is to maintain the app over time. It is tempting to individualize features in order to appease customers. The temptation is especially strong when a business is starting and revenue is especially critical. However, restricting the number of brand-specific features is key to profitability.\n\nThe cost to develop a white-labeled product is relatively low at the start. Expenses grow over time as more brands (with more needs) are added to the platform. Ideally developers should strive to flatten out the expense curve so that the cost to add a new brand to the app is small. Setting limits on brand-specific customization is key to reaching this goal.\n\nPublishing Requirements\n\nThe adoption of white-labeled products by multiple companies results in the availability of similar looking products that market themselves as being unique to a specific brand. This outcome inevitably presents a challenge to using white-labeling to create profitable digital products. It has also contributed to some pushback from Apple. Initially Apple chose to reject submissions from white-label providers. Apple later relaxed its policy to allow white-labeled products to be created so long as each brand creates its own developer account and publishes its rebranded app through that account. This stipulation aims to strike a balance between Apple\u2019s interest in an \u201capp store cleanup\u201d and the growing popularity of white-labeled products. Developers interested in creating profitable white-labeled products need to stay informed of these evolving restrictions.\n\nAdequate Specificity\n\nThe design of white-labeled products needs to be general enough to be adopted by multiple brands yet specific enough to capitalize on niche markets and their goals. For example, Intel used white-labeling to create a gaming laptop that\u2019s similar to a lot of other laptops but also has qualities that appeal to a specific market. The new product for gaming enthusiasts is conducive to white-labeling due to features including \u201can advanced cooling system, excellent performance, great build quality, a light chassis, and shockingly good battery life.\u201d The same connection between profitability and embodiment of a middle ground between generic and targeted features exists for white-labeled apps.\n\nAlignment with Emerging Trends\n\nThe most profitable white-labeled products provide services that are becoming increasingly popular. If relabeled products don\u2019t reflect relevant trends, then they are unlikely to provide a launching point from which multiple brands can service a growing need. White-labeled products that are at the forefront of new trends are also able to acquire data that will become increasingly valuable over time. An example of a timely white-labeled product is Greenlight, a debit card for kids that was recently white-labeled by Chase. Fintech is now essential to personal banking. Therefore, apps that capitalize on this growing trend will be highly valuable to top brands.\n\nInspiringApps has extensive experience developing white-labeled products for organizations that are influential in numerous sectors. Contact us today to find out how our team can help you use white-labeling to reach a diverse set of organizations.",
            "published_at": "2021-01-06T17:42:07+00:00"
        },
        {
            "authors": [],
            "title": "The harmful assumptions we make about tasks",
            "contents": "The harmful assumptions we make about tasks What they cause and how to correct them\n\nMost task managers make two assumptions. Do you make them too?\n\nA task manager needs to remember all of your tasks\n\n(Would you buy a task manager that lost data?) All tasks can use a similar creation workflow\n\n(You hit the \u2018create issue\u2019 button, and fill in some fields. Simple, obvious, how could it be better?)\n\nIt\u2019s not obvious that they\u2019re wrong\u2014they actually seem useful. But both at work, and at home, making these assumptions will reduce your effectiveness.\n\nAnd if you have trouble context-switching, procrastinate tasks, or have an ever-growing task backlog that looms over your head, these assumptions have likely caused you unnecessary pain.\n\nHere\u2019s how they hurt us, and how to stop it from happening.\n\nPermanent tasks cause fatigue\n\nBefore starting Tandem, my manager Tim previously co-founded Astrid, a personal task manager, which he later sold to Yahoo.\n\nAt Astrid, Tim tried an interesting re-engagement email. Users who had been inactive for 6 weeks were asked: \u201cwant to delete all your tasks and start fresh?\u201d\n\nThe hypothesis was that these inactive users had tasks that overwhelmed them. Either the amount, the complexity, or the low importance of backlogged tasks, caused users to put tasks off. So every time these users visited Astrid, they saw tasks they couldn\u2019t do, felt bad, and eventually stopped visiting\u2014procrastination in action. Astrid\u2019s re-engagement email offered the users a chance to start fresh.\n\nIt was impressively compelling. Most re-engagement emails get a 1% response rate. This email had about 5%.\n\nI imagine those users had the same feeling you get when you move from one browser to another, or from one task manager to another, and leave your bookmarks, open tabs, backlogged tasks, to start fresh in the name of increased productivity.\n\nBut simply deleting the backlog doesn\u2019t end the cycle.\n\nNowadays, Tim uses a simpler task manager\u2014a notebook. He uses a page a day, adds some tasks from JIRA, and when tasks come in during the day, he writes them down and works on them when he\u2019s available. These are mostly small tasks, things that are easy to forget. If he gets a bug (important to track) or a task he can delegate, he puts it in JIRA.\n\nThe notebook isn\u2019t meant to store issues forever. The pages get turned, and previous information becomes less accessible. So the next day, if he needs to carry over something that hasn\u2019t been completed, he needs to make the conscious decision to do so.\n\nThis solution solves task-management fatigue. And incidentally, it solves another problem:\n\nNot all tasks are created the same\n\nThere is a gap in task management. When we look at the tools used for tracking tasks and goals, there\u2019s something for every level of the company:\n\nLevel Goal Tool Company quarter-level Quarterly Goals Notion/Gdocs Org quarter-level KPIs Notion/Gdocs, JIRA Epics Team sprint-level Tickets JIRA Tasks/Bugs Personal day-level ??? ???\n\nExcept at the personal day-level, where you\u2019re working on tickets, getting requests for feature updates from your PM, bug discoveries from QA, and breaking down your actual tickets into different tasks, which may require subtasks that weren\u2019t anticipated.\n\nOne proposal is that you should track all of these tasks, no matter how small, in JIRA. But the teams I see successful at implementing this are those that can afford to work at a slower, more \u201centerprise\u201d pace. Which isn\u2019t a problem\u2014at least for engineering\u2014but it\u2019s often not an option at smaller startups, when learning cycles must be fast.\n\nEven if you work at a larger company, at a slower pace, you may relate when it comes to personal life: tasks come from a variety of directions, at all times. You can\u2019t toss every minor task in a task manager.\n\nYou can\u2019t fix every task before the next one comes in. If you only use JIRA, you choose between possibly forgetting these small tasks, or breaking your train of thought to search, categorize, and fill out a form every time you get a small task.\n\nOn the flip side, quickly writing these down in a temporary place means that you don\u2019t need to:\n\nSpend mental effort remembering minor tasks,\n\nor take significant time to file minor issues.\n\nHave your task manager filled with minor issues you\u2019ve forgotten to close,\n\nor leave hastily-written issues for later, when you forget what the vaguely titled issue is for.\n\nCorrecting for the mistakes\n\nThe solution is not complicated: use a temporary, personal task manager.\n\nDon\u2019t go out and buy something new. You already have something you can use at home. Tim\u2019s notebook is a good example. I personally message myself on Slack (and iMessage, for personal use).\n\nAn example of how I Slack myself Using Slack for daily personal tasks When I\u2019m working, and get a non-urgent task that would otherwise interrupt me, it goes in my Slack DMs. This is what it looks like: In the morning, to re-build yesterday\u2019s context, I just look at yesterday\u2019s list. If there are unfinished items, I can carry them over. But it\u2019s a conscious decision. I don\u2019t need to carry over optional, low-priority tasks. And if you\u2019ve got a bad memory, there\u2019s a huge benefit for team stand-ups. I used to write JQL filters to search JIRA (doesn\u2019t work\u2014and misses tasks), now I just take a look at Slack.\n\nThe important requirements are just that your task manager:\n\nRequires effort to carry over old tasks. Makes writing down tasks quick. Isn\u2019t used for notes (anecdotally, this adds noise, and forces you to parse notes from tasks).\n\nIf a task is critical, long term, or requires cooperation, file it in your normal issue tracker. Otherwise, it\u2019s probably a daily, personal task, and you can file it in here.\n\nSo take out a notebook, open Slack, pick what works for you. And see if you don\u2019t feel calmer and more in control of your tasks.\n\nI think these tools have interesting ideas to help individuals manage tasks.\n\nAmna\n\nRavi, my old college roommate and friend since high school, left Microsoft to build Amna, a personal task manager with tight browser integration.\n\nI resonate with the thesis: recently I switched away from Firefox Tree Style Tabs, because I noticed that by organizing my tabs (which were part of tasks), it inadvertently became a de-facto task manager. But it wasn\u2019t built for that!\n\nAmna solves that exact problem: each task you do spawns a browser that keeps all of your relevant tabs.\n\nIt helps prevent:\n\nMuscle-memory opening HN, clicking an interesting blog post, and leaving it open \u201cto read later\u201d for several weeks\n\nThe pain of trawling through bookmarks and browser history, when you realize you forgot something\n\nThe clutter of leaving tabs open to prevent forgetting something\n\nIf you\u2019ve forgotten a time when you had less than five tabs open, try using Amna.\n\nJam\n\nTwo of my ex-Cloudflare friends, Dani and Irtefa, started Jam. It\u2019s probably the fastest way for PMs/designers to directly make comments on a website. So you get less minor interruptions, and more clarity on the requested change.\n\nBut I don\u2019t include Jam here just for that. Jam also allows you to offload the actual copy changes to your PM, which flat-out decreases your minor tasks. If this sounds exciting, get on the waitlist here."
        },
        {
            "authors": [
                "More Jon Marcus"
            ],
            "title": "The pandemic is speeding up the mass disappearance of men from college",
            "contents": "Listen: Disappearance Of Men From College Campuses by Kirk Carapezza\n\nDebrin Adon, a senior at the University Park Campus School in Worcester, Massachusetts. His male classmates \u201cdon\u2019t think they\u2019re smart enough\u201d for college, Adon says. \u201cThey doubt themselves a little bit because of their life and what they\u2019ve been through and what they\u2019ve been seen as.\u201d Credit: Kate Flock for The Hechinger Report\n\nWORCESTER, Mass. \u2014 When he and his male classmates talk about going to college, said Debrin Adon, it always comes down to one thing.\n\n\u201cWe\u2019re more focused on money,\u201d said Adon, 17, a senior at a public high school here. \u201cLike, getting that paycheck, you know?\u201d Whereas, \u201cif I go to college, I\u2019ve got to pay this much and take on all this debt.\u201d\n\nThat\u2019s among the many reasons the number of men who go to college has for years been badly trailing the number of women who go. And the Covid-19 pandemic has abruptly thrown the ratio even more off balance.\n\nWhile enrollment in higher education overall fell 2.5 percent in the fall, or by more than 461,000 students compared to the fall of 2019, the decline among men was more than seven times as steep as the decline among women, according to an analysis of figures from the National Student Clearinghouse Research Center.\n\n\u201cIn a sense, we have lost a generation of men to Covid-19.\u201d Adrian Huerta, assistant professor of education, University of Southern California\n\n\u201cIn a sense, we have lost a generation of men to Covid-19,\u201d said Adrian Huerta, an assistant professor of education at the University of Southern California who studies college-going among boys and men.\n\n\u201cIt\u2019s a national crisis,\u201d said Luis Ponjuan, an associate professor of higher education administration at Texas A&M University.\n\nAdon, who attends the University Park Campus School, plans to buck the odds and go to college. He said he decided this after he realized that his parents, who immigrated to the United States from the Dominican Republic, want a better life for him. His mother is unemployed now, and his father runs a barbershop.\n\nRelated: Number of rural students planning on going to college plummets\n\nLynnel Reed, head guidance counselor at the University Park Campus School in Worcester, Massachusetts, says many of her male students are getting jobs that may divert them from college. \u201cHow do you go away to college and leave your family struggling?\u201d Credit: Kate Flock for The Hechinger Report\n\n\u201cIt wasn\u2019t dramatic,\u201d he said of the moment he made up his mind to pursue a degree in computer science; he described it while standing outside on the asphalt that surrounds the 135-year-old redbrick school, which switched to entirely virtual instruction because of the pandemic. \u201cYou know when you\u2019re in the shower and you just think about life?\u201d\n\nThat kind of epiphany has eluded many other young men.\n\nWomen now comprise nearly 60 percent of enrollment in universities and colleges and men just over 40 percent, the research center reports. Fifty years ago, the gender proportions were reversed.\n\n\u201cWe were already not doing so hot,\u201d Ponjuan said. \u201cThis pandemic exacerbates what\u2019s happening.\u201d\n\n\u201cHow do you go away to college and leave your family struggling when you know that if you just worked right now, you could help them right now with those everyday needs?\u201d Lynnel Reed, head guidance counselor, University Park Campus School\n\nIt\u2019s also opened jobs for young men from Worcester high schools at grocery stores and at Amazon, FedEx and other delivery companies, said Lynnel Reed, head guidance counselor at University Park, nearly two-thirds of whose students are considered economically disadvantaged. The school is in a neighborhood of fast-food restaurants, liquor stores, used-car lots, dollar stores and triple-deckers \u2014 homes usually shared by three families, one on each level, that are a staple of urban New England.\n\n\u201cHow do you go away to college and leave your family struggling when you know that if you just worked right now, you could help them right now with those everyday needs?\u201d Reed said.\n\nThat\u2019s a bigger pull for young men than for young women, said Derrick Brooms, a sociologist at the University of Cincinnati.\n\nThe University Park Campus School in Worcester, Massachusetts. The 135-year-old school serves about 240 students, two-thirds of them low-income. Credit: Kate Flock for The Hechinger Report\n\n\u201cIt aligns with this perception that to be a man is to be self-sufficient,\u201d Brooms said. \u201cIt\u2019s a little bit different for girls. We\u2019re teaching them about investing for even greater payoffs down the line.\u201d\n\nThis has only been exacerbated by Covid-19.\n\n\u201cIt makes more sense right now just to say, \u2018I\u2019m going to take a break because my family needs this money,\u2019 \u201d said Huerta. And even if young men resolve to go to college later, he said, history shows that \u201ctheir chances of actually coming back to higher ed are probably slim to none.\u201d\n\nWhile the number of students overall fell by more than 461,000 compared to the fall of 2019, the decline among men was more than seven times as steep as the decline among women.\n\nDespite the allure of a paycheck versus going into debt and spending years pursuing a degree, the reality is that \u201ca lot of these young men at 17 or 18 years old end up working 12-hour shifts, getting married, buy a truck, get a mortgage, and by the time they\u2019re 30, their bodies are broken,\u201d Ponjuan said. \u201cAnd now they have a mortgage, three kids to feed and that truck, and no idea what to do next.\u201d\n\nStopping education after high school not only limits men\u2019s options; it threatens to further widen socioeconomic and political divides, Brooms said.\n\nRelated: In one country, women now outnumber men in college by two to one\n\nWorcester State University in Worcester, Massachusetts. Men have trailed women in college-going for so long that the public university\u2019s enrollment is now over 60 percent women. Credit: Kate Flock for The Hechinger Report\n\nNot everyone has to go to college. Faster and less costly career and technical education can lead to in-demand, well-paying jobs in skilled trades, automation and other fields.\n\nGraduates with bachelor\u2019s degrees still generally make more than people with lesser credentials, however. And the pandemic has shown that people without degrees are more vulnerable to economic downturns. Unemployment for them, nationwide, rose more than twice as fast in the spring as unemployment for people with bachelor\u2019s degrees, the Federal Reserve Bank of San Francisco found.\n\n\u201cWe have a lot of young men who are completely disengaged from our society because quite frankly they don\u2019t feel they\u2019re being valued as men.\u201d Luis Ponjuan, associate professor of higher education administration, Texas A&M University\n\nMeanwhile, the shootings of Black civilians by police and the resulting outrage has left some young Black and Hispanic men who are still in high school \u201cdisenfranchised almost to the point where they\u2019re feeling like they\u2019re invisible, that the community doesn\u2019t value who they are, at the very time that they\u2019re developing their own identities,\u201d Ponjuan said.\n\nThat too, has an immediate impact on their motivation to get further educations, he and others said.\n\nPedro Hidalgo, a senior at the University Park Campus School in Worcester, Massachusetts. He says teachers in middle school helped him \u201cbecome more confident with my abilities, not even as just a student, but as a person.\u201d Credit: Kate Flock for The Hechinger Report\n\n\u201cWe have a lot of young men who are completely disengaged from our society because quite frankly they don\u2019t feel they\u2019re being valued as men. So they think, why even try when everybody sees me as a thug, as a delinquent, when everyone assumes the worst of me instead of assuming the best of me?\u201d\n\nPedro Hidalgo, another senior at University Park, said he \u201cnever had that belief within myself\u201d that he could go to college. Then \u201cteachers in middle school actually helped me realize that I\u2019m more than what I seem to think that I am at times. They just helped me progressively become more confident with my abilities, not even as just a student, but as a person.\u201d\n\nNow Hidalgo, 18, whose older brother started but never finished college, plans to pursue a degree in psychology and become a clinical therapist.\n\nRelated: Progress in getting underrepresented people into college and skilled jobs may be stalling because of the pandemic\n\nKellie Becker, head guidance counselor at North High School in Worcester, Massachusetts, says taking college courses while still in high school emboldens male students. \u201cThey\u2019re like, \u2018All right, you know what? Wait, I can do this.\u2019 \u201c Credit: Kate Flock for The Hechinger Report\n\nHe said he made that decision after taking dual-enrollment courses offered by his high school in collaboration with neighboring Clark University.\n\nWhen they take those college-level classes, \u201cThey\u2019re like, \u2018All right, you know what? Wait, I can do this,\u2019 \u201d said Kellie Becker, head guidance counselor at nearby North High School. \u201cIt eases their transition to college and builds their confidence.\u201d\n\nIt was the dual-enrollment program that clicked with Abdulkadir Abdullahi, 18, a student at North.\n\n\u201cI didn\u2019t think I was going to college. I didn\u2019t think it could be useful to me in the real world,\u201d said Abdullahi, son of a single father who\u2019s a postal carrier. \u201cI would rather hang out with my friends and, like, slack.\u201d\n\nThen an older sister went to college and Abdullahi took a dual-enrollment course.\n\n\u201cI was, like, \u2018Oh, I could really do this,\u2019 \u201d he said. Until then, \u201cI always thought college was going to be, like, writing 20-page essays every other week, staying up overnight.\u201d\n\nNow he plans to get a degree in sociology.\n\nSome of their male classmates still have qualms, said Adon, Abdullahi and Hidalgo.\n\nAbdulkadir Abdullahi, a senior at North High School in Worcester, Massachusetts. \u201cI didn\u2019t think I was going to college. I didn\u2019t think it could be useful to me in the real world,\u201d says Abdullahi, who changed his mind after taking college courses while still in high school. Credit: Kate Flock for The Hechinger Report\n\n\u201cThey don\u2019t think they\u2019re smart enough,\u201d Adon said. \u201cThey don\u2019t think they can do it. They doubt themselves a little bit because of their life and what they\u2019ve been through and what they\u2019ve been seen as.\u201d\n\nIn those dual-enrollment classes, Hidalgo, said, there are more girls than boys. \u201cIt\u2019s intimidating because, you know, you don\u2019t want to say the wrong thing.\u201d\n\nYoung men seem to have shorter attention spans, Abdullahi said. \u201cThere\u2019s more distractions for guys. The guy is always the class clown. I think they just lose their motivation.\u201d\n\nThere\u2019s research to back that up. Boys are more likely than girls as early as elementary school to be held back, a Brown University researcher found. They are almost 9 percentage points less likely to graduate from high school, according to the U.S. Department of Education.\n\nRelated: More people with bachelor\u2019s degrees go back to school to learn skilled trades\n\nRyan Forsyth, vice president for enrollment management at Worcester State University in Worcester, Massachusetts, where male enrollment has dropped below 40 percent. Credit: Kate Flock for The Hechinger Report\n\n\u201cBoys realize that teachers and counselors aren\u2019t invested in them in the same way that they\u2019re invested in girls,\u201d said Huerta. \u201cTeachers and counselors are more concerned with ensuring the boys are doing the basics \u2014 behaving in class \u2014 versus ensuring that they\u2019re college-ready.\u201d\n\nSo long has this been going on that enrollment at Worcester State University, across town from the University Park Campus School, is now more than 60 percent women.\n\nWomen now comprise nearly 60 percent of enrollment at universities and colleges, and men around 40 percent. Fifty years ago, the proportions were reversed.\n\nIn addition to all the other issues caused by the mass disappearance of men from college, it\u2019s a big problem for universities and colleges struggling to fill seats, said Ryan Forsyth, Worcester State\u2019s vice president for enrollment management.\n\nHe, too, sees the pandemic as encouraging more young male high school seniors to jump straight into the workforce, \u201crather than seeing the value of going into a college for a two- or four-year degree to really invest in themselves,\u201d Forsyth said on the cold and nearly empty campus.\n\nThat seems unlikely to change soon, Ponjuan said.\n\n\u201cThis pandemic only highlights the unspoken truth that it is creating short-term solutions for these young men but not long-term opportunities,\u201d he said. \u201cLong term, they\u2019re going to top out. They\u2019re not going to be able to advance. It has created a false sense of security that they\u2019ll get by just delivering packages.\u201d\n\nThis story about men in college was produced by The Hechinger Report, a nonprofit, independent news organization, in collaboration with GBH Boston. Additional reporting by Kirk Carapezza. Sign up for our higher education newsletter.",
            "published_at": "2021-01-19T12:00:00+00:00"
        },
        {
            "authors": [],
            "title": "India asks WhatsApp to withdraw new privacy policy over \u2018grave concerns\u2019 \u2013 TechCrunch",
            "contents": "India has asked WhatsApp to withdraw the planned change to its privacy policy, posing a new headache to the Facebook-owned service that identifies the South Asian nation as its biggest market by users.\n\nIn an email to WhatsApp head Will Cathcart, the nation\u2019s IT ministry said the upcoming update to the app\u2019s data-sharing policy has raised \u201cgrave concerns regarding the implications for the choice and autonomy of Indian citizens\u2026 Therefore, you are called upon to withdraw the proposed changes.\u201d\n\nThe ministry is additionally seeking clarification from WhatsApp on its data-sharing agreement with Facebook and other commercial firms and has asked why users in the EU are exempt from the new privacy policy but their counterpoint in India have no choice but to comply.\n\n\u201cSuch a differential treatment is prejudicial to the interests of Indian users and is viewed with serious concern by the government,\u201d the ministry wrote in the email, a copy of which was obtained by TechCrunch. \u201cThe government of India owes a sovereign responsibility to its citizens to ensure that their interests are not compromised and therefore it calls upon WhatsApp to respond to concerns raised in this letter.\u201d\n\nThrough an in-app alert earlier this month, WhatsApp had asked users to agree to new terms of conditions that grants the app the consent to share with Facebook some personal data about them, such as their phone number and location. Users were initially provided until February 8 to comply with the new policy if they wished to continue using the service.\n\n\u201cThis \u2018all-or-nothing\u2019 approach takes away any meaningful choice from Indian users. This approach leverages the social significance of WhatsApp to force users into a bargain, which may infringe on their interests in relation to informational privacy and information security,\u201d the ministry said in the email.\n\nThe notification from WhatsApp prompted a lot of confusion \u2014 and in some cases, anger and frustration \u2014 among its users, many of which have explored alternative messaging apps such as Telegram and Signal in recent weeks.\n\nIn a statement on Tuesday, a WhatsApp spokesperson said, \u201cWe wish to reinforce that this update does not expand our ability to share data with Facebook. Our aim is to provide transparency and new options available to engage with businesses so they can serve their customers and grow. WhatsApp will always protect personal messages with end-to-end encryption so that neither WhatsApp nor Facebook can see them. We are working to address misinformation and remain available to answer any questions.\u201d\n\nWhatsApp, which Facebook bought for $19 billion in 2014, has been sharing some limited information about its users with the social giant since 2016 \u2014 and for a period allowed users to opt-out of this. Responding to the backlash last week, the Facebook-owned app, which serves more than 2 billion users worldwide, said it was deferring the enforcement of the planned policy to May 15.\n\nWhatsApp also ran front-page ads on several newspapers in India last week, where it has amassed over 450 million users, to explain the changes and debunk some rumors.\n\nNew Delhi also shared disappointment with the timing of this update, which, to be fair, WhatsApp unveiled last year. The ministry said that it was reviewing the Personal Data Protection Bill, a monumental privacy bill that is meant to oversee how data of users are shared with the world.\n\n\u201cSince the Parliament is seized of the issue, making such a momentous change for Indian users at this time puts the cart before the horse. Since the Personal Data Protection Bill strongly follows the principle of \u2018purpose limitation,\u2019 these changes may lead to significant implementational challenges for WhatsApp should the Bill become an Act,\u201d the letter said.\n\nOn Tuesday, India\u2019s IT and Law Minister Ravi Shankar Prasad also offered loud advice to Facebook. \u201cBe it WhatsApp, be it Facebook, be it any digital platform. You are free to do business in India but do it in a manner without impinging upon the rights of Indians who operate there.\u201d",
            "published_at": "2021-01-19T00:00:00"
        },
        {
            "authors": [
                "Mike O'Neill",
                "Penn State University"
            ],
            "title": "Breakthrough Allows Inexpensive Electric Vehicle Battery to Charge in Just 10 Minutes",
            "contents": "Range anxiety, the fear of running out of power before being able to recharge an electric vehicle, may be a thing of the past, according to a team of Penn State engineers who are looking at lithium iron phosphate batteries that have a range of 250 miles with the ability to charge in 10 minutes.\n\n\u201cWe developed a pretty clever battery for mass-market electric vehicles with cost parity with combustion engine vehicles,\u201d said Chao-Yang Wang, William E. Diefenderfer Chair of mechanical engineering, professor of chemical engineering and professor of materials science and engineering, and director of the Electrochemical Engine Center at Penn State. \u201cThere is no more range anxiety and this battery is affordable.\u201d\n\nThe researchers also say that the battery should be good for 2 million miles in its lifetime.\n\nThey report today (January 18, 2021) in Nature Energy that the key to long-life and rapid recharging is the battery\u2019s ability to quickly heat up to 140 degrees Fahrenheit, for charge and discharge, and then cool down when the battery is not working.\n\n\u201cThe very fast charge allows us to downsize the battery without incurring range anxiety,\u201d said Wang.\n\nThe battery uses a self-heating approach previously developed in Wang\u2019s center. The self-heating battery uses a thin nickel foil with one end attached to the negative terminal and the other extending outside the cell to create a third terminal. Once electrons flow it rapidly heats up the nickel foil through resistance heating and warm the inside of the battery. Once the battery\u2019s internal temperature is 140 degrees F, the switch opens and the battery is ready for rapid charge or discharge.\n\nWang\u2019s team modeled this battery using existing technologies and innovative approaches. They suggest that using this self-heating method, they can use low-cost materials for the battery\u2019s cathode and anode and a safe, low-voltage electrolyte. The cathode is thermally stable, lithium iron phosphate, which does not contain any of the expensive and critical materials like cobalt. The anode is made of very large particle graphite, a safe, light and inexpensive material.\n\nBecause of the self-heating, the researchers said they do not have to worry about uneven deposition of lithium on the anode, which can cause lithium spikes that are dangerous.\n\n\u201cThis battery has reduced weight, volume and cost,\u201d said Wang. \u201cI am very happy that we finally found a battery that will benefit the mainstream consumer mass market.\u201d\n\nAccording to Wang, these smaller batteries can produce a large amount of power upon heating \u2014 40 kilowatt hours and 300 kilowatts of power. An electric vehicle with this battery could go from zero to 60 miles per hour in 3 seconds and would drive like a Porsche, he said.\n\n\u201cThis is how we are going to change the environment and not contribute to just the luxury cars,\u201d said Wang. \u201cLet everyone afford electric vehicles.\u201d\n\nReference: \u201cThermally modulated lithium iron phosphate batteries for mass-market electric vehicles\u201d by Xiao-Guang Yang, Teng Liu and Chao-Yang Wang, 18 January 2021, Nature Energy.\n\nDOI: 10.1038/s41560-020-00757-7\n\nOther Penn State researchers working on this project were Xiao-Guang Yang, assistant research professor of mechanical engineering, and Teng Liu, doctoral student in mechanical engineering.\n\nThe U.S. Department of Energy\u2019s Office of Energy Efficiency and Renewable Energy and the William E Diefenderfer Endowment supported this research.",
            "published_at": "2021-01-18T08:00:36-08:00"
        },
        {
            "authors": [
                "Nicole Lindsey",
                "Nicole Lindsey Is A Journalist",
                "Writer For More Than Years",
                "Focusing On The Intersection Of Technology",
                "Innovation",
                "Privacy. She Has A Background In Information Technology",
                "Has Worked With Various Software Companies",
                "Tech Startups On Their Public Relations",
                "Communications Initiatives.",
                "Min Read"
            ],
            "title": "Google Blocking Web Privacy Proposals at W3C",
            "contents": "Throughout much of 2019, Internet tech giant Google has attempted to portray itself as a public champion of web privacy. Yet, behind the scenes, a very different view of Google is emerging. In August 2019, at approximately the same time that Google was rolling out its much-hyped \u201cPrivacy Sandbox\u201d privacy framework, it was also working to block efforts of the World Wide Web Consortium (W3C) standards body to bolster the web privacy features of new technical specifications.\n\nGoogle\u2019s efforts to limit the web privacy powers of PING\n\nMost notably, Google was the only member of the W3C to vote \u201cNo\u201d to a proposed charter change for the Privacy Interest Group (PING), a working group of the W3C dedicated to web privacy matters. Concerned that web privacy issues were being routinely ignored by many working groups of the W3C, the Privacy Interest Group sought to expand its charter, such that it would have the ability to block any new technical specification that it felt would have negative implications for web privacy. PING, for example, had earlier voiced its concerns about the Internet of Things (IoT), and how new IoT web standards failed to address important web privacy issues.\n\nIn response to the vote request on changing the PING charter, 24 W3C members voted \u201cYes\u201d and Google (via its parent company, Alphabet, Inc.) was the only member to vote \u201cNo.\u201d Since the W3C is based on a system of consensus, even a single \u201cno\u201d vote was enough to veto the proposal. Some have compared the voting system of the W3C to that of the UN Security Council, where even a single \u201cno\u201d vote from a member like the United States, Russia or China is enough to veto a proposal. Thus, Google basically torpedoed a well-intentioned effort to inject a discussion of web privacy into more of the technical working groups of the W3C with a single vote.\n\nThe 24-1 vote would seem to clearly indicate that Google is not quite as serious about web privacy matters as it would like the public to believe, and that Google is becoming increasingly isolated in its policy stances. That\u2019s especially the case, given that Google has worked time and time again to preserve the ad tracking powers of web cookies, warning that an eliminated of ubiquitous ad-tracking cookies would mean the end of the web as we know it.\n\nAnd, as might be expected, as soon as word of the 24-1 vote on the status of PING began to leak to the media, Google went into full damage control mode. According to Google, it is simply not the case that they are against PING or against new web privacy measures. Rather, it is the case that Google is against the \u201csweeping powers\u201d of an \u201cauthoritarian review group\u201d that might \u201ccause significant unnecessary chaos in the development of the web platform.\u201d In short, Google says that it is the voice of reason here, fighting against the creeping powers of an \u201cauthoritarian\u201d tech body.\n\nGoogle vs. the W3C\n\nIt now appears that Google is attempting to impose its own web privacy ideas on the W3C, and will simply use its equivalent of a \u201cveto\u201d to block any efforts to introduce web privacy measures that might be detrimental to its overall business model. This has very profound consequences for the future development of the web, since Google now appears to be fighting against mainstream sentiment in the business and consumer worlds.\n\nIn contrast to Google, some big tech companies \u2013 including Apple \u2013 are now racing to champion new web privacy measures. And many of Google\u2019s Chrome browser rivals \u2013 including both Mozilla\u2019s Firefox and Brave \u2013 are now pushing ahead with privacy-first initiatives, including the automatic blocking of ad-tracking web cookies.\n\nThere have even been rumors and suggestions that the Privacy Interest Group (PING) has become a place for these rivals to limit or handicap the ability of Google to compete against them. Most notably, Brave has attempted to take on a co-chair role of PING, presumably in a move designed to weaken its top competitor, Google.\n\nThe important point to keep in mind, of course, is just how important third-party tracking for ad targeting purposes is to Google\u2019s business model. Being able to sidestep important privacy concerns is the key to Google being able to churn out billions of dollars in revenue each quarter. And being able to keep the web privacy status quo alive and well is key for Google\u2019s future profitability. Thus, from a purely business perspective, it is easy to see why Google might be looking to limit any initiative from the W3C regarding web privacy standards. It\u2019s hard to see Google giving up its de facto veto privilege any time soon.\n\nThe future of web privacy\n\nThe current system of the W3C is based on a \u201chorizontal review\u201d process, which is one that Google says it still supports. In a horizontal review, the Privacy Interest Group (PING) is able to make suggestions about technical specifications, but is not actually able to block these technical specifications from moving ahead. According to Google, this system is preferable to the one being proposed by an overwhelming majority of W3C members (basically, everyone except Google) because it does not have the potential to stop a web specification from being dismissed at the last minute.\n\nOne potential compromise \u2013 and one that Google has, in fact, suggested \u2013 is the creation of a series of \u201cself-serve guiding principles.\u201d In many ways, this is what the Google Privacy Sandbox was supposed to be \u2013 a framework for privacy that would not be officially binding for W3C members, but that would help to spell out certain rules of the road for web privacy.\n\n24-1 vote against the expanding web #privacy powers of PING could be #Google\u2019s way of keeping profitability of its online business model. #respectdata Click to Tweet\n\nImportantly, the tide finally appears to be turning in favor of web privacy. Not too long ago, the 24-1 vote against the expanding web privacy powers of PING might have been much closer. Now, it is only Google that appears to be fighting what appears to be an inevitable and irreversible trend toward greater web privacy and less third-party tracking across the web.",
            "published_at": "2019-10-04T22:00:00+00:00"
        },
        {
            "authors": [],
            "title": "YoYo Games is now part of Opera",
            "contents": "News\n\nYoYo Games is now part of Opera\n\nWe\u2019ve got some big news! YoYo Games is now part of Opera - the global web innovator and creator of Opera GX - Opera GX is a browser built for gamers. With features that include countless customization options, sound effects, background music, a gaming-inspired design, as well as CPU, RAM and Network Bandwidth limiters, GX is less resource-hungry and leaves more of the computer\u2019s resources for gaming. Beyond gaming, Opera\u2019s browsers, news products and fintech solutions are the trusted choice of more than 380 million people worldwide, so we\u2019re in great hands!\n\nToday\u2019s news signals the start of an exciting new journey. One that we believe will unlock many exciting possibilities for developers and educators. YoYo Games\u2019 will continue to be based in Dundee, Scotland and General Manager Stuart Poole and Technical Lead Russell Kay will both remain with the business.\n\nLooking forward, Opera has big plans for YoYo Games and GameMaker. Together with Opera GX, GameMaker will form the cornerstone of Opera Gaming - a new division focused on expanding Opera\u2019s reach and capabilities within the gaming space.\n\n\u201cWe are very excited to start working with the team at YoYo Games,\u201d said Krystian Kolondra, EVP Browsers at Opera. \u201cWe see the platform as being an ideal acquisition to complement our global ambitions in gaming, along with our Opera GX gaming browser. We look forward to further growing Opera GX and to driving the growth of GameMaker, making it more accessible to novice users and developing it into the world\u2019s leading 2D game engine used by commercial studios. We are also thrilled to find future synergies between YoYo Games\u2019 products and Opera GX.\u201d\n\nStuart Poole, General Manager of YoYo Games said, \u201cIt\u2019s been clear to us from the first time we spoke to them that the whole of the team at Opera is incredibly passionate about games. Since joining them last week, the positivity and creative energy we are seeing from them has been overwhelming. We have always had big plans for improving GameMaker across all platforms, both from the perspective of improving accessibility and further developing the features available to commercial studios; and now we can\u2019t wait to see them arrive much sooner.\u201d\n\nWe are all massively enthusiastic about the future of GameMaker!"
        },
        {
            "authors": [],
            "title": "I tried creating a web browser, and Google blocked me",
            "contents": "I tried creating a web browser, and Google blocked me",
            "published_at": "2019-04-02T09:00:00-04:00"
        },
        {
            "authors": [],
            "title": "Ask HN: People who moved from HCOL cities to the country. What was that like?",
            "contents": "- What was you last living situation in the city like?\n\n- What is your new place like?\n\n- How long ago did you move?\n\n- Why did you opt to move?\n\n- What do you like most about life in the country compared to life in the city?\n\n- What was hard about about adjusting to rural life?\n\n- How did you pick your location/property?\n\n- What would you tell others looking to relocate from HCOL cities to the country?\n\nif you'd prefer not post publicly, I've got a google form here: https://forms.gle/1oivSS3jBUTXguMq9"
        },
        {
            "authors": [
                "Ozgun Ataman"
            ],
            "title": "Choosing Haskell isn\u2019t a stand-in for good software design",
            "contents": "There\u2019s a fallacy commonplace enough on the internet and causing sufficient confusion that it could use some clarifying: Teams decide to be \u201cHaskell shops\u201d, hire for maximal type-tetris capability and therefore expect to, by direct implication and default, produce outstanding software. Stated differently for additional clarity: Haskell, or any other particular language for that matter, does not automatically solve all problems related to architecture and macro-level decision-making in software production. Believing otherwise may actually produce worse outcomes than picking a mainstream language (e.g. JavaScript) and dealing with its warts.\n\nHere are some examples where simply \u201cchoosing Haskell\u201d will not automatically solve the concern for you at all:\n\nLaying down, communicating and embodying the right hierarchy of values in software development, especially in a group setting\n\nChoosing the right internal/intermediate data representations\n\nImagining the right MVP functionality your software should have in a way that aligns with continued enhancement from there\n\nKnowing where making a huge deal out of every little issue prevents progress and where deep care needs to be taken even for the very MVP\n\nChoosing the right database \u2014 knowing when to use Postgres, Redis, DynamoDb, ElasticSearch, etc in concert as appropriate\n\nKnowing when a 300ms database call is just fine and when even 50ms isn\u2019t acceptable.\n\nKnowing when no-redundancy is completely fine and when over-the-top redundancy is prudent\n\nKnowing when to use external async queues, their limitations, downsides, etc, and when to just get something done inline\n\nFiguring out the right database schema (remembering there\u2019s always a schema even when using NoSQL)\n\nEnforcing the right amount of failure-tolerance on your functions, particularly those that do external calls, IO, etc.\n\nStriking the right balance in picking service boundaries\n\nHaving good structured logging practices, and contra \u2014 knowing when too much logging itself hurts the system\n\nHaving good instrumentation and performance tracking in your codebase\n\nEnforcing good clarity in the codebase \u2014 not too redundant but not prematurely abstract either\n\nFiguring out a way to achieve iterative growth on the solution in a team setting (usually easy for 1 person, hard even for 5)\n\nStriking the right balance in testing\n\nKnowing when to optimize coding style for long term correctness (i.e. type-tetris and all the noise from it) and when to optimize for ease of logic expression\n\nThe Haskell ecosystem has, perhaps, a larger than typical portion of its practitioners from the academic domain of interest. This is a particular strength of our community, but it often also means that more of our conversations are focused on topics like advanced type systems instead of, say, what a decent baseline application architecture should be for a variety of use cases. We have to keep reminding ourselves that the latter actually matters more for practical software \u2014 baseline Haskell2010 already gives you an outstanding toolkit but getting the above choices wrong can bust your project.\n\nTo be clear, commercial teams that choose Haskell usually do it because they believe Haskell will provide for a good syntax/language/coding environment that will bottoms-up bias the team in a positive direction. Yet we have all heard of battle stories where a given commercial effort\u2019s failure was later blamed in part on the choice of underlying tech/language (e.g. Haskell). In my assessment, the root cause is often not the language, but bad choices made in systems architecture, development guidelines, team-wide values (I may say more on this at a later time) and other similar \u201cmacro\u201d topics instead.\n\nHaskell is a fine choice and many in our community really like it (myself included), but you still need to get everything else right. Abdicating your responsibility on good software governance because you \u201cpicked Haskell and so it\u2019s all solved\u201d will likely not work.\n\nMany thanks to Ryan Trinkle, Doug Beardsley, David Drake and Michael Xavier for providing feedback on earlier drafts of this article.",
            "published_at": "2021-01-19T18:44:50.288000+00:00"
        },
        {
            "authors": [],
            "title": "Make Your Own Internet Archive With Archive Box \u2013 NixIntel",
            "contents": ""
        },
        {
            "authors": [
                "Hasnat A Amin",
                "Department Of Life Sciences",
                "Brunel University London",
                "Http",
                "Fotios Drenos",
                "Institute Of Cardiovascular Sciences"
            ],
            "title": "No evidence that vitamin D is able to prevent or affect the severity of COVID-19 in individuals with European ancestry: a Mendelian randomisation study of open data",
            "contents": "What this paper adds Uncertainty remains over the use of Vitamin D for the prevention of COVID-19 and the moderation of its symptoms.\n\nGenetic predisposition for higher levels of vitamin D and for lower chance of vitamin D insufficiency do not have evidence of association with infection from SARS-CoV-2 or severity of COVID-19 following infection.\n\nOur work supports the current NICE statement that, based on the available evidence, vitamin D should not be considered as protective of infection from SARS-CoV-2 or a way to mitigate its severity.\n\nIntroduction Vitamin D has lately been the focus of very intense scientific interest, with more than 4500 manuscripts published per year since 2015. Although vitamin D is commonly discussed in terms of bone health and calcium and phosphate homeostasis, evidence has started to emerge that it may also be involved in cancer, the cardiovascular system and inflammation.1 With the onset of the COVID-19 pandemic, these findings, in combination with previous reports of vitamin D playing a role in upper respiratory tract (URT) infections and their severity, have resulted in further interest in vitamin D and the potential use of vitamin D supplements to mitigate the spread and severity of COVID-19. We obtain vitamin D either through our diet, with certain foods such as oily fish and egg yolks being good sources, or through our exposure to ultraviolet B radiation from the sun.2 Vitamin D, whether generated or consumed, is biologically inactive and undergoes a complex metabolic process: it is first converted to 25-hydroxyvitamin D (25(OH)D) in the liver and then this is converted in the kidney to 1,25-hydroxyvitamin D, which is the active metabolite.2 The commonly assessed vitamin D levels refer to the 25(OH)D metabolite, which is the main circulating form in the body.2 Vitamin D deficiency, commonly defined as levels lower than 25 nmol/L, is a common problem both in developed3 and in developing countries.4 Although it is usually thought to be a problem in countries located at higher latitudes and with darker days,5 a high prevalence of vitamin D deficiency is also observed in countries close to the equator.6 According to guidance from the National Institute for Health and Care Excellence (NICE), those with low vitamin D levels should be treated with high-dose supplementation for a short period, followed by a lower maintenance dose,7 and all adults are advised to take a daily supplement containing 10 mg per day throughout the year to prevent vitamin D deficiency. URT infections are reportedly more frequent and more severe in individuals with lower vitamin D levels. Although these infections are more common during seasons with darker days, when vitamin D levels are lower, vitamin D has also been correlated with better pulmonary function in young adults8 and with lower reporting of coughs and colds;9 however, these studies cannot provide evidence of causation. A meta-analysis of randomised controlled trials of vitamin D supplementation for acute respiratory tract infections found a protective effect of vitamin D,10 but the study has been criticised for the way the approach was used.11 Current evidence for the use of vitamin D supplementation against COVID-19 mainly relies on assumptions based on reports of lower vitamin D being associated with a higher risk of URT infections (please see the review by Lanham-New et al).12 A handful of studies have used a data-based approach,13\u201317 but these efforts rely on correlations between vitamin D and COVID-19, which are liable to be affected by unobserved or inadequately controlled confounding factors. Although we will need well-powered and carefully executed randomised trials and a subsequent meta-analysis of the different studies to provide an accurate estimate of the effect of vitamin D on COVID-19 prevention and severity, we can anticipate the results of such studies by comparing individuals who are genetically predisposed to lower vitamin D levels with those who are not, based on the Mendelian randomisation (MR) paradigm. In a randomised controlled trial, we would minimise the effect of confounding factors by randomly assigning participants to a treatment group receiving vitamin D supplements or to a control group receiving a placebo and thus estimate the true effect of the intervention. In the natural experiment of MR, genetic variants predisposing the individual to higher levels of vitamin D are assigned randomly at conception, based on the genetic polymorphisms of their parents, in relation to other possible confounding traits. As genetic polymorphisms remain constant throughout life and the individual does not change their vitamin D intake according to their genotype, the use of this information can provide indirect evidence of causality.18 Here, using data from genome-wide association (GWA) studies for vitamin D levels, vitamin D deficiency and COVID-19 incidence and severity, we test whether genetically increased vitamin D levels are associated with SARS-CoV-2 infection risk and COVID-19 severity.\n\nMethods Population and study design We predominately used previously published and freely available data for this study. The study by Jiang et al 19 is a meta-analysis of GWAs of vitamin D levels carried out using participants of European descent. The COVID-19 Host Genetics Initiative20 uses data from multiple cohort studies,21\u201326 including the UK Biobank (UKB). The UKB individual-level data were also used following permission to use data already available for COVID-19-related research. UKB is a large prospective cohort study that recruited >500 000 UK residents between 2006 and 2010. The 22 UKB assessment centres, located throughout England, Wales and Scotland, collected baseline data from the participants in the form of questionnaires, physical and cognitive tests, and blood and urine samples.27 The age range of the participants at the time of enrolment in the study was between 40 and 69 years of age, with a mean age of 56.5 years. Men represent 45.6% of the sample. The use of the data for this project was approved by the UKB (application 44566). Genotyping In the UKB, 488 377 individuals had been genotyped for up to 812 428 variants using DNA extracted from blood samples on either the UKB Axiom array (438 427 participants) or the UK BiLEVE Axiom array (49 950 participants). Variants that did not pass standard quality control checks were excluded.28 These included tests for the presence of batch effects, plate effects, sex effects and array effects, as well as any departures from Hardy-Weinberg Equilibrium using a p value threshold of 10\u221212. Variants with a minor allele frequency of <0.01 were also excluded. Sample genotyping quality control metrics were provided by UKB.28 Samples were excluded from the analysis if they were outliers for missingness and/or PC-corrected heterozygosity and/or if they had any sex chromosome aneuploidies, as well as if the genetically inferred sex differed from the reported sex. Samples which did not have a genetically determined white British ancestry were also excluded. A list of related individuals was also provided by UKB and one individual from each related pair was excluded at random. Genetic data from studies used by the COVID-19 Host Genetics Initiative underwent quality control and imputation using the protocol described by Lam et al. 29 Phenotypes We used summary statistics from phenotypes B1 and C1 by the COVID-19 Host Genetics Initiative (September 2020 release). Phenotype B1 is a measure of COVID-19 severity and only included individuals who were positive for SARS-CoV-2 infection based on an RNA-based and/or a serology-based test: individuals who were hospitalised due to coronavirus-related symptoms were coded as cases; and individuals who were not hospitalised for 21 days or more after their positive SARS-CoV-2 test were coded as controls. Phenotype C1 is a measure of susceptibility to SARS-CoV-2 infection: individuals were coded as cases if they were diagnosed with COVID-19 by a doctor or if they were positive for SARS-CoV-2 infection based on an RNA-based and/or a serology-based test or if they self-reported being positive for COVID-19; and individuals were coded as controls if they tested negative (for all tests if multiple tests were performed) for SARS-CoV-2 infection based on an RNA-based and/or a serology-based test or if they self-reported being negative for COVID-19. Please see https://www.covid19hg.org/about/ for more details. Vitamin D deficiency: individuals whose vitamin D levels (UKB field 30890) were <25 nmol/L were coded as cases; and individuals whose vitamin D levels were \u226550 nmol/L were coded as controls. COVID-19 test results30 from the UKB were made available through linkage to national health records. Obesity: individuals whose body mass index (BMI) was \u226518.5 kg/m2 and <25 kg/m2 (UKB field 21001) were considered as having a normal weight; and individuals whose BMI was \u226530 kg/m2 were considered as being obese. Season: individuals who attended the assessment centres during December, January or February (UKB field 55) were considered as winter samples; and individuals who attended during June, July or August were considered as summer samples. Statistical analyses We used R V.4.0.231 to carry out analyses and generate plots, unless stated otherwise. We used PLINK V.1.932 to carry out genetic association analyses using UKB data and to generate the genetic risk score for vitamin D deficiency. Welch\u2019s two-sample t-test was used to assess the differences in the distribution of vitamin D levels in the following categories: obese versus normal weight; summer samples versus winter samples; and the bottom and top quartiles of the vitamin D levels genetic risk score generated using variants from Jiang et al 19 (online supplemental table 1B and figure 1). Fisher\u2019s exact test was used to assess the differences in the prevalence of vitamin D deficiency in the three aforementioned categories, except that the vitamin D deficiency genetic risk score used for this analysis was generated using the 17 variants associated with vitamin D deficiency in the UKB (see online supplemental table 1A and figure 2). Supplemental material [bmjnph-2020-000151supp001.pdf] To assess the causal effect of vitamin D levels on SARS-CoV-2 infection risk and COVID-19 severity, we used outcome summary statistics from the COVID-19 Host Genetics Initiative20 (see the Phenotypes section) and exposure summary statistics from Jiang et al 19 (see the Population and study design section) to carry out two-sample MR using the TwoSampleMR R package.33 Note: rs3755967, rs12785878 and rs8018720 were not available in the summary statistics for phenotypes B1 and C1, so we used rs17467825, rs3794060 and rs8022510, respectively, as proxies (R2 >0.99). The causal estimates (beta) are expressed as ln(OR) per ln(nM), where OR=Odds ratio; and nM=concentration of vitamin D in nmol/L. For each genetic variant that is associated with the exposure of interest (eg, vitamin D), the causal effect of the exposure of interest on the outcome of interest (eg, SARS-CoV-2 risk or COVID-19 severity) can be estimated by calculating the Wald ratio, which is the effect of the variant on the outcome divided by the effect of the variant on the exposure. If there are multiple independently inherited variants associated with the exposure, as in this case, the inverse variance weighted MR (IVW-MR) method is used to provide an overall estimate of the causal effect by calculating a weighted average of the Wald ratios. However, in the presence of pleiotropy (ie, a genetic variant is associated with the outcome through a pathway that does not include the exposure of interest), the estimate from the IVW-MR method may be biased. The MR Egger method models this possible violation of the assumption through the intercept of a linear model between the effect of the instruments on the exposure and outcome. In the absence of pleiotropy, the value of this intercept does not differ from zero. However, if the pleiotropic effects of the variants are related to their effects on the exposure (ie, a violation of the INSIDE assumption), the MR Egger and IVW-MR methods are both susceptible to bias. In this case, the weighted median, simple mode and weighted mode MR methods are used. These methods use the median or mode of the Wald ratios to provide robust estimates in cases where some of the genetic instruments violate the pleiotropy assumption.34 In order to use two-sample MR to estimate the causal effect of vitamin D deficiency on the aforementioned outcomes, we needed to obtain new instruments for vitamin D deficiency. We therefore carried out a GWA analysis in the UKB using PLINK V.1.9,32 adjusted for the first four principal components for the genetic variability of the genome, age at baseline, sex and the genotyping array used. The associated genes for each variant were obtained from National Center for Biotechnology Information Single Nucleotide Polymorphism (NCBI SNP). As the COVID-19 Host Genetics Initiative uses data from the UKB, there is a possibility that the estimates from two-sample MR analyses may be biased; Burgess et al 35 suggest that this bias can be minimised by generating exposure summary statistics using control samples only, so we excluded individuals who had been tested for COVID-19 from our GWA analysis. The summary statistics for vitamin D deficiency were then filtered using a p value threshold of 5\u00d710\u22128 and clumped using the \u2018clump_data\u2019 function, which finds the variant with the smallest p value, removes any variants that are in linkage disequilibrium (R2 >0.001) and repeats this process until there are no variants remaining. It is possible that the effects of the genetic variants associated with vitamin D deficiency may vary by season, so we repeated the genetic association analyses using winter samples only and used the effect sizes from these to carry out two-sample MR to test the sensitivity of our results to this possibility.\n\nResults For SARS-CoV-2 infection susceptibility (phenotype C1), summary statistics from 11 181 cases and 116 456 controls were available as of 30 September 2020. For COVID-19 severity (phenotype B1), summary statistics from 1389 cases and 5879 controls were available as of 30 September 2020. For the vitamin D deficiency phenotype in the UKB, there were 35 079 cases and 140 898 controls. Please see the Methods section for the phenotype definitions. The genetic risk score generated using the six variants from Jiang et al 19 (online supplemental table 1B) explained 2.518% of the variance in vitamin D concentration and the genetic risk score generated using the newly identified 17 variants associated with vitamin D deficiency (online supplemental table 1A) explained 2.108% of the variance in vitamin D deficiency (note: the latter is an approximation based on McFadden\u2019s R2). Table 1 shows the distribution of vitamin D levels and the prevalence of vitamin D deficiency by genetic risk score category (1st quartile vs 4th quartile), together with the changes associated with known factors affecting vitamin concentration, such as obesity and season. Table 1 Distribution of vitamin D levels and prevalence of vitamin D deficiency by obesity status, season and the genetic risk scores in the UK Biobank We estimated the causal effect of vitamin D levels on the risk of SARS-CoV-2 infection and severe COVID-19 using two-sample MR. We found no evidence in the existing data that vitamin D levels causally affect the risk of SARS-CoV-2 infection (IVW: ln(OR)=0.17 (95% CI \u22120.22 to 0.57, p=0.39)) nor did we find evidence that vitamin D levels causally affect COVID-19 severity (IVW: ln(OR)=0.36 (95% CI \u22120.89 to 1.61, p=0.57)). We also used four other more robust MR methods and we still did not find any evidence in the existing data to suggest that vitamin D levels causally affect SARS-CoV-2 risk or COVID-19 severity (figure 1). Testing for the presence of pleiotropy for our genetic instruments using the MR Egger method suggests that our estimates are not biased due to pleiotropy (online supplemental table 2). Figure 1 Log ORs (beta) and 95% CIs from a two-sample MR analysis of the effect of vitamin D levels on SARS-CoV-2 risk and COVID-19 severity. MR, Mendelian randomisation. It is possible that simply having vitamin D levels that are lower, but still within the optimal range, may not affect SARS-CoV-2 risk nor COVID-19 severity. We therefore performed a GWA analysis of vitamin D deficiency in the UKB, found that 17 independent variants were associated with this phenotype (online supplemental table 1A) and used these variants in a two-sample MR analysis to estimate the causal effect of vitamin D deficiency on the risk of SARS-CoV-2 infection and COVID-19 severity. We found no evidence that vitamin D deficiency causally affects the risk of SARS-CoV-2 infection (IVW: ln(OR)=\u22120.04 (95% CI \u22120.1 to 0.03, p=0.25)) nor did we find evidence that vitamin D deficiency causally affects COVID-19 severity (IVW: ln(OR)=\u22120.24 (95% CI \u22120.55 to 0.08, p=0.14)). We also used four other robust MR methods and we still did not find any evidence to suggest that vitamin D deficiency causally affects SARS-CoV-2 risk or COVID-19 severity (figure 2). Again, we did not detect any evidence of pleiotropy bias in our results using the MR Egger method (online supplemental table 3). We repeated the two-sample MR using effect sizes from the winter samples only (online supplemental table 4) as a sensitivity analysis, and our results did not differ (online supplemental table 5). Figure 2 Log ORs (beta) and 95% CIs from a two-sample MR analysis of the effect of vitamin D deficiency on SARS-CoV-2 risk and COVID-19 severity. MR, Mendelian randomisation.\n\nDiscussion Using previously published results for the genetics of vitamin D levels, UKB individual-level data for the genetics of vitamin D deficiency and the accumulating genetic results for susceptibility and severity of COVID-19, we tested the causal effect of vitamin D levels and deficiency on protection from SARS-CoV-2 infection and COVID-19 severity. We found no evidence that vitamin D is causally related to COVID-19 outcomes and there is no evidence to suggest that current NICE guidance should change to support the use of vitamin D supplementation against COVID-19. Previously published evidence,10 though criticised,11 supports the idea that increasing vitamin D levels are protective against acute respiratory tract infections, but these results do not appear to translate in the case of COVID-19. Studies specifically looking at the correlation of vitamin D with COVID-19 and its severity observed an inverse association between them,13 15\u201317 though, in one study,14 when a number of possible confounders were adjusted for, the correlation was no longer present. These studies, however, can only provide very limited information on causality and they are sensitive to uncontrolled confounders. Vitamin D is lower in hospitalised individuals and even more so in those in care homes with limited mobility and exposure to sunlight,36 both of which are much more common in the elderly. COVID-19, at least in the UK, has had a disproportional effect on older people and care homes, making it difficult to disentangle the complex relationships between age and disability on one hand and diet and sunlight exposure affecting vitamin D on the other. Our approach uses genetic information to avoid the problem of unobserved confounders, a method that has rapidly gained popularity for the estimation of causal effects based on observational studies.37 Our results are based on GWA studies combining data for tens of thousands of individuals from different sources of information to ensure an unbiased estimate and a result that provides the best chance to detect an effect, if present. However, our work is not without limitations. The most common problem of MR analyses is the presence of pleiotropy. Although this is more likely to cause false positives, rather than false negatives, no evidence for pleiotropy was detected in our analyses. We also used multiple MR models that make slightly different assumptions and provide a more pleiotropy robust result with all of them providing the same conclusion. Vitamin D levels were represented by measures of 25(OH)D which, despite being the most commonly assessed vitamin D metabolite in a clinical setting, does not directly measure the activated form of vitamin D and its measurement and relevance to health are under discussion.38 Our results also cannot be used to comment on the relationship between vitamin D and COVID-19 in non-Europeans. Finally, the available data for SARS-CoV-2 infection or severe COVID-19 disease are still limited and a more precise picture will emerge as more information becomes available. To summarise, using a two-sample MR method, GWA studies of vitamin D and the latest data from tens of thousands of individuals courtesy of the international COVID-19 Host Genetics Initiative, we found no evidence of vitamin D being protective against SARS-CoV-2 infection or severe COVID-19. Our results support the recent statement by NICE that the use of vitamin D supplementation to mitigate COVID-19 is not supported by the available data.",
            "published_at": "2021-01-07T00:00:00"
        },
        {
            "authors": [
                "Sharon Weinberger"
            ],
            "title": "Hollywood and hyper-surveillance: the incredible story of Gorgon Stare",
            "contents": "An MQ-9 Reaper drone, used for surveillance by the US Air Force.Credit: Staff Sgt. John Bainter/U.S. Air Force\n\nEyes in the Sky: The Secret Rise of Gorgon Stare and How It Will Watch Us All Arthur Holland Michel Houghton Mifflin Harcourt (2019)\n\nIn the 1998 Hollywood thriller Enemy of the State, an innocent man (played by Will Smith) is pursued by a rogue spy agency that uses the advanced satellite \u201cBig Daddy\u201d to monitor his every move. The film \u2014 released 15 years before Edward Snowden blew the whistle on a global surveillance complex \u2014 has achieved a cult following.\n\nIt was, however, much more than just prescient: it was also an inspiration, even a blueprint, for one of the most powerful surveillance technologies ever created. So contends technology writer and researcher Arthur Holland Michel in his compelling book Eyes in the Sky. He notes that a researcher (unnamed) at the Lawrence Livermore National Laboratory in California who saw the movie at its debut decided to \u201cexplore \u2014 theoretically, at first \u2014 how emerging digital-imaging technology could be affixed to a satellite\u201d to craft something like Big Daddy, despite the \u201cnightmare scenario\u201d it unleashes in the film. Holland Michel repeatedly notes this contradiction between military scientists\u2019 good intentions and a technology based on a dystopian Hollywood plot.\n\nGovernments want your smart devices to have stupid security flaws\n\nHe traces the development of that technology, called wide-area motion imagery (WAMI, pronounced \u2018whammy\u2019), by the US military from 2001. A camera on steroids, WAMI can capture images of large areas, in some cases an entire city. The technology got its big break after 2003, in the chaotic period following the US-led invasion of Iraq, where home-made bombs \u2014 improvised explosive devices (IEDs) \u2014 became the leading killer of US and coalition troops. Defence officials began to call for a Manhattan Project to spot and tackle the devices.\n\nIn 2006, the cinematically inspired research was picked up by DARPA, the Defense Advanced Research Projects Agency, which is tasked with US military innovation (D. Kaiser Nature 543, 176\u2013177; 2017). DARPA funded the building of an aircraft-mounted camera with a capacity of almost two billion pixels. The Air Force had dubbed the project Gorgon Stare, after the monsters of penetrating gaze from classical Greek mythology, whose horrifying appearance turned observers to stone. (DARPA called its programme Argus, after another mythical creature: a giant with 100 eyes.)\n\nSome books use blockbuster action films to demonstrate \u2014 or exaggerate \u2014 a technology\u2019s terrifying potential. Here, Enemy of the State shows up repeatedly because it is integral to the development of Gorgon Stare. Researchers play clips from it in their briefings; they compare their technology to Big Daddy (although their camera is so far only on aircraft, not a satellite). At one point, incredibly, they consult the company responsible for the movie\u2019s aerial filming. (It set me wondering \u2014 which government lab out there is currently building the Death Star from Stars Wars?)\n\nA camera on an MQ-9 Reaper drone.Credit: A1c Aaron Montoya/Planet Pix via ZUMA\n\nHolland Michel\u2019s book is not the first to look at technologies intended to achieve omniscience, but it is among the best. Writers examining the intersection of technology and privacy often repeat well-worn tropes, claiming that every novelty is the new Big Brother. But Eyes in the Sky is that rare creature: a deeply reported and deftly written investigation that seeks to understand both the implications of a technology and the motivations of its creators. Holland Michel notes tensions between security and privacy without hyping them.\n\nMasters of war\n\nAnd he gets those responsible for building WAMI to speak to him candidly \u2014 sometimes shockingly so. Take, for example, the former US military officer who touts the \u2018benefits\u2019 of the colonial subjugation of India (which he bizarrely claims created order among the country\u2019s ethnic groups) to justify mass surveillance in the United States.\n\nThis potential for domestic mass surveillance becomes a key point. As the story proceeds, WAMI\u2019s creators start looking for ways to use the battlefield technology at home: having built a new hammer, they search for more nails. Here, the story takes an even more dystopian turn. John Arnold, \u201ca media-shy billionaire\u201d, uses his own money to help secretly deploy a WAMI system to assist the police in tracking suspects in crime-ridden Baltimore, Maryland. Arnold, who has funded other \u201cnew crime-fighting technologies\u201d, first learnt about WAMI\u2019s use overseas from a podcast, and decided to debut it stateside. \u201cEven the mayor was kept in the dark,\u201d Holland Michel writes.\n\nPrivate interests\n\nIs this our future? A world in which billionaires fund the police to record entire cities from above? That plot twist is less Enemy of the State than Batman, although it\u2019s hard to know who the hero is. (At least the fictional Big Daddy was funded by Congress, even if its supporters had to kill one stubborn lawmaker to get the job done.) It\u2019s enough to make us all reach for tinfoil hats, which could come in handy to block what Holland Michel warns is coming next: infrared imaging that can detect people inside their homes. WAMI, if deployed above your city, already has the capacity to track your daily commute and errands, and allow those watching to retrace your steps for days or weeks.\n\nDreaming of death rays: the search for laser weapons\n\nTo his credit, Holland Michel\u2019s interviews with surveillance technologists are reported with context but without commentary, allowing readers to draw their own conclusions. In one understated episode, he reveals that \u2014 after the Baltimore project was exposed \u2014 the owner of the company that built and deployed the WAMI system there had \u201cpersonally\u201d provided gifts to a community organizer. The organizer was working to convince Baltimore residents that a sky-borne Big Brother might be in their interests.\n\nOne unanswered, and perhaps unanswerable, question is how successful WAMI was at its original purpose: preventing insurgent bomb attacks in Iraq and Afghanistan. Holland Michel isn\u2019t sure, because the answer is classified. Although investment in WAMI is \u201cfurious and ongoing\u201d, he notes, \u201cthe Air Force declined repeated requests for even an approximate indication of WAMI\u2019s impact on the battlefield\u201d.\n\nWhat we do know is that Afghanistan, one of the most surveilled countries on Earth, is slipping further into chaos. That can\u2019t be blamed on WAMI, but it does indicate that the tech is not today\u2019s Manhattan Project.\n\nThe long entanglement of war and astrophysics\n\nThere are other questions. By focusing on a specific technology, does Holland Michel miss a bigger picture? Is the more serious threat the access of governments and corporations to our electronic devices? The answer to both is no, because he also traces how meshing WAMI with other sensors, including those on smartphones, will eventually create \u201ca fully fused city\u201d where \u201cthere may be nowhere to hide\u201d. In the end, Eyes in the Sky transcends its title by using Gorgon Stare as a window into our future. And that is bleak.\n\nWhen Gorgon Stare is completed, Michael Meermans, an executive at Sierra Nevada (the company in Sparks, Nevada, that built it) asks himself rhetorically whether the task is over. Of course not. \u201cWhen it comes to the world of actually collecting information and creating knowledge,\u201d Meermans says, \u201cyou can never stop.\u201d"
        },
        {
            "authors": [],
            "title": "XTerm does graphics! (sort of)",
            "contents": "As everyone know, the XTerm is a terminal emulator. It emulates (among others) the ancient DEC VT102 text-only terminal. But that\u2019s not all. It can also do some graphics because it can also emulate the Tektronix 4014 terminal. And these Tektronix ones were actualy able not only to do text but also some points and lines! The main limitation is that Tektronix emulation is \u201cblack and white\u201d only (it recognises just two XTerm colors: the foreground one and the background one).\n\nThis emulation is a bit limited but it\u2019s fully functional. To enable the Tektronix emulation mode it is only needed to use the -t command line parameter:\n\n\n\nxterm -t\n\n\n\nThen is is useful to set the TERM variable to something like \u201ctek\u201d,\n\n\u201c4014\u201d or \u201ctek4014\u201d (the \u201ctek\u201d one worked for me very well):\n\n\n\nexport TERM=tek\n\n\n\nBy the way, this stuff also worked for remote connection. So you can set the Tektronix mode for your local XTerm and generate graphics at the opposite end of your connection. For example you can run the Gnuplot on the SDF and see the graphs on your local XTerm.\n\nThe Gnuplot can produce results in Tektronix-compatible format. Just\n\nset the proper terminal:\n\n\n\nexport TERM=xterm\n\n\n\nThere are also other Gnuplot terminals (\u201cvttek\u201d, \u201ctek40xx\u201d and so) but the \u201cxterm\u201d one is the best suited for the XTerm \u2013 it actually opens two XTerm windows. The first is the Tektronix emulation one for graphics and the second is a VT102-compatible one for the Gnuplot command interface. So you can work in the same way as if you are using the default X11 output window. The main difference is that thus way is more resources-friendly and work wonderfully also for slow remote connections (and there is no need for the remote server to have anything realted to the X11/Xorg. The SSH/telnet connection is just enough).\n\nThere are other programs which can save in Tektronix-compatible format. Their output can be plotted with help of the \u201cplot\u201d program from the GNU Plotutils (the old UNIX \u201cplot\u201d or \u201ctplot\u201d program should work, too):\n\n\n\ncat yourfile.out | plot -Ttek\n\n\n\nThe Plotutils also include modern versions of other classical UNIX tools. There is a \u201cgraph\u201d program, for example. But you can find more in this great\n\nold tutorial from the Oregon State University. The whole \u201cCoping with Unix, a Survival Guide\u201d tutorial is great reading, by the way.\n\nI have to thank to the Mastodon user niconiconi who tooted about this long-forgotten ability of the XTerm. I was aware of that but never tried it before.",
            "published_at": "2019-07-17T00:00:00"
        },
        {
            "authors": [],
            "title": "Chatmosphere",
            "contents": ""
        },
        {
            "authors": [],
            "title": "Stablecoins through history \u2014 Michigan Bank Commissioners report, 1839",
            "contents": "A \u201cstablecoin\u201d is a token that a company issues, claiming that the token is backed by currency or assets held in a reserve. The token is usually redeemable in theory \u2014 and sometimes in practice.\n\nStablecoins are a venerable and well-respected part of the history of US banking! Previously, the issuers were called \u201cwildcat banks,\u201d and the tokens were pieces of paper.\n\nGenuine as a three-dollar bill \u2014 from the American Numismatic Society blog.\n\nThe wildcat banking era, more politely called the \u201cfree banking era,\u201d ran from 1837 to 1863. Banks at this time were free of federal regulation \u2014 they could launch just under state regulation.\n\nUnder the gold standard in operation at the time, these state banks could issue notes, backed by specie \u2014 gold or silver \u2014 held in reserve. The quality of these reserves could be a matter of some dispute.\n\nThe wildcat banks didn\u2019t work out so well. The National Bank Act was passed in 1863, establishing the United States National Banking System and the Office of the Comptroller of the Currency \u2014 and taking away the power of state banks to issue paper notes.\n\nAdvocates of Austrian economics often want to bring back \u201cfree banking\u201d in this manner, because they despise the Federal Reserve. They come up with detailed theory as to how letting free banking happen again will surely work out well this time. [Mises Institute search]\n\nOn 15 March 1837, the \u201cgeneral banking law\u201d was passed in Michigan. Bray Hammond\u2019s classic \u201cBanks and Politics in America\u201d from 1957 (UK, US) tells how this all worked out (p. 601):\n\nOf her free-banking measure, Michigan\u2019s Governor said: \u201cThe principles under which this law is based are certainly correct, destroying as they do the odious features of a bank monopoly and giving equal rights to all classes of the community.\u201d Within a year of the law\u2019s passage, more than forty banks had been set up under its terms. Within two years, more than forty were in receivership. Thus America grew great.\n\nHammond quotes another source on the notes themselves: \u201cGet a real furioso plate, one that will take with all creation \u2014 flaming with cupids, locomotives, rural scenery, and Hercules kicking the world over.\u201d The ICO white papers of their day.\n\nAfter the Michigan law allowing free banking had been in effect for two years, Michigan\u2019s state banking commissioners reported to the legislature on how it was all going.\n\nThe whole report is available as a scan in Google Books \u2014 Documents Accompanying the Journal of the House of Representatives of the State of Michigan, pp. 226-258. [Google Books] There\u2019s also a bad OCR \u2014 ctrl-F to \u201cBank Commissioners\u2019 Report.\u201d [Internet Archive]\n\nThis is not your normal tedious report from civil servants to the legislature. It\u2019s a work of thundering Victorian passion, excoriating the criminals and frauds the commissioners found themselves responsible for dealing with.\n\nWe should have more official reports that you could do a dramatic reading of:\n\nThe peculiar embarrassments which they have had to encounter, and the weighty responsibilities consequent thereupon, clothes this duty with a new character. It becomes an act of justice to themselves, and to those who have honored them with so important a trust. At the period the commissioners entered upon their labors, every portion of the state was flooded with a paper currency, issued by the institutions created under the general banking law. New organizations were daily occurring, and the public mind was everywhere agitated with apprehension and distrust. The state was in the midst of the evils consequent upon an excessive and doubtful circulation. Rumors of the most frightful and reckless frauds were daily increasing. In this emergency, prompt and vigorous action was imperiously demanded, as well by the public voice as the urgent necessity of the case. Upon a comparison of opinions, the commissioners united in the conclusion that their duty was of a two fold character. The first, and most obvious one, was to take immediate and decided measures in ascertaining and investigating the affairs of every institution suspected of fraud, and closing the door against the evil without delay. The second was a duty of far more difficult and delicate a nature, and involving the assumption of a deep responsibility.\n\nThe report outlines the problems in each particular district, and lists the local troubled banks. The commissioners tried to distinguish fraudulent banks from merely inept ones, and help the second sort get back on their feet for the public good.\n\nMost of it\u2019s tedious detail. But there\u2019s considerable parallels to our wonderful world of crypto:\n\nThe loan of specie from established corporations, became an ordinary traffic, and the same money, set in motion a number of institutions. Specie certificates, verified by oath, were every where exhibited, although these very certificates had been cancelled at the moment of their creation, by a draft for a similar amount; and yet such subterfuges were pertinaciously insisted upon, as fair business transactions, sanctioned by custom and precedent. Stock notes were given, for subscriptions to stock, and counted as specie, and thus not a cent of real capital actually existed, beyond the small sums paid in by the upright and unsuspecting farmer and mechanic, whose little savings and honest name were necessary to give confidence and credit. The notes of institutions thus constituted, were spread abroad upon the community, in every manner, and through every possible channel; property, produce, stock, farming utensils, every thing which the people of the country were tempted, by advanced prices, to dispose of, were purchased and paid for in paper, which was known by the utterers to be absolutely valueless. Large amounts of notes were hypothecated for small advances, or loans of specie, to save appearances. Quantities of paper were drawn out by exchange checks, that is to say, checked out of the banks, by individuals who had not a cent in bank, with no security, beyond the verbal understanding that notes of other banks should be returned, at some future time.\n\nThe banking system at the time featured barrels of gold that were carried to other banks, just ahead of the inspectors:\n\nThe singular spectacle was presented, of the officers of the state, seeking for banks in situations the most inaccessible and remote from trade, and finding at every step, an increase of labor, by the discovery of new and unknown organizations. Before they could be arrested, the mischief was done; large issues were in circulation, and no adequate remedy for the evil. Gold and silver flew about the country with the celerity of magic; its sound was heard in the depths of the forest, yet like the wind, one knew not whence it came or whither it was going. Such were a few of the difficulties against which the Commissioners had to contend. The vigilance of a regiment of them would have been scarcely adequate, against the host of bank emissaries, which scoured the country to anticipate their coming, and the indefatigable spies which hung upon their path, to which may be added perjuries, familiar as dicers\u2019 oaths, to baffle investigation.\n\nBray Hammond\u2019s book elaborates on these stories:\n\nTheir cash reserves were sometimes kegs of nails and broken glass with a layer of coin on top. Specie exhibited to the examiners at one bank was whisked through the trees to be exhibited at another the next day.\n\nBanknotes increased liquidity\u2014 they helped value flow faster through the economy. Who benefited from this increase in liquidity? Mostly the fraudulent banknote issuers:\n\nIt has been said, with some appearance of plausibility, that these banks have at least had the good effect of liquidating a large amount of debt. This may be true; but whose debts have they liquidated? Those of the crafty and the speculative \u2014 and by whom? Let every poor man, from his little clearing and log hut in the woods, make the emphatic response by holding up to view, as the rewards of his labor, a handful of promises to pay, which, for his purposes, are as valueless as a handful of the dry leaves at his feet. Were this the extent of the evil, the indomitable energy and spirit of our population, who have so manfully endured it, would redeem the injury. But when it is considered how much injury is inflicted at home, by the sacrifice of many valuable farms, and the stain upon the credit of the state abroad, the remedy is neither so easy nor so obvious. When we reflect, too, that the laws are ineffective in punishing the successful swindler, and that the moral tone of society seems so far sunk as to surround and protect the dishonest and fraudulent with countenance and support, it imperatively demands that some legislative action should be had, to enable the prompt and rigorous enforcement of the laws, and the making severe examples of the guilty, no matter how protected and countenanced.\n\nPassing around the corporate shell after you\u2019ve scoured it has long been the fashion:\n\nSo that the singular exhibition has been made of banks passing from hand to hand like a species of merchandize, each successive purchaser less conscientious than the preceding, and resorting to the most desperate measures for reimbursement on his speculation.\n\nThe stablecoins of the day depreciated horribly, even while the institutions were still up and running, and it was the innocent members of the public stuck with the tokens who paid:\n\nUnder the present law, the order in which the means and securities are to be realized and exhausted, will protract the payment of their liabilities to an indefinite period, and make them utterly useless to the great body of the bill holders, whose daily necessities compel them to sell at an enormous loss. The banks themselves, through their agents, are thus enabled to buy up their circulation at an immense depreciation, and their debtors to pay their liabilities in the notes of banks, purchased at a great discount. The daily advertisements for the purchase of safety fund notes in exchange for land and goods, and the placards every where to be seen in the windows of merchants and brokers, is a sufficient argument for the necessity of the measure proposed.\n\nThe Commissioners pause here to examine the rationale for having free banking at all \u2014 principles of freedom, versus how it actually worked out in practice. They quote William M. Gouge\u2019s A Short History of Paper-money and Banking in the United States, 1833, p. 230: [Google Books]\n\nA reform will not be accomplished in banking, as some suppose, by granting charters to all who apply for them. It would be as rational to abolish political aristocracy, by multiplying the number of nobles. The one experiment has been tried in Germany, the other in Rhode Island. Competition in that which is essentially good, in farming, in manufactures, and in regular commerce, is productive of benefit ; but competition in that which is essentially evil, may not be desirable. No one has yet proposed to put an end to gambling by giving to every man the privilege of opening a gambling house.\n\nThis story reminds me of recent stablecoin \u201cattestations,\u201d and documents waved at apparently credulous journalists:\n\nThe Farmers\u2019 and Mechanics\u2019 bank of Pontiac, presented a more favorable exhibit in point of solvency, but the undersigned having satisfactorily informed himself that a large proportion of the specie exhibited to the commissioners, at a previous examination, as the bona fide property of the bank, under the oath of the cashier, had been borrowed for the purpose of exhibition and deception; that the sum of ten thousand dollars which had been issued for \u201cexchange purposes,\u201d had not been entered on the books of the bank, reckoned among its circulation, or explained to the commissioners.\n\nWhat do you do with bankers so bad you can\u2019t tell if they\u2019re crooks or just bozos? You put their backsides in jail, with personal liability for making good:\n\nUpon officially visiting the Berrien county bank, the undersigned found its operations suspended by his predecessor, Col. Fitzgerald. On investigation of its affairs, with that gentleman, much was exhibited betraying either culpable mismanagement, or gross ignorance of banking. Col. Fitzgerald, however, with the usual vigilance and promptitude characteristic of all his official acts, had, previous to my arrival, caused the arrest of some of the officers of the institution, under the provisions of the act of December 30th, 1837; and required of the proprietors to furnish real estate securities to a considerable amount, conditioned to be released on the entire re-organization of the bank, and its being placed on a sound and permanent basis, or suffer a forfeiture of the lands pledged, which, together with their assets in bank, individual responsibility and the real estate security, given in conformity to law, must in the worst event, be more than sufficient to satisfy and pay all their liabilities.\n\nIn crypto, not even the frauds are new. Fortunately, we have suitable remedies to hand \u2014 such as the STABLE Act.",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [],
            "title": "Agendar\u2122 by Outright Mental\u2122",
            "contents": "In a world of remote meetings, challenges arise between the focus of working without distraction, and showing up on time for events.\n\nIt may be especially challenging for a person using a computer to shift gears from deep focus without being a second late.\n\nAssume that I already use Google Calendar to keep track of the events I will be attending. In order to optimize my chance of success, to cut through the tunnel vision and remain aware of events throughout the day, I mount a tablet on an arm on my desk, dedicated to displaying my agenda:\n\nI tried this, and there was still something lacking. I want the absolute minimum amount of information, tightly centered around these principles:\n\nFull-screen app has a dark backdrop.\n\nEvents are displayed in monochrome type with minimal decoration.\n\nDisplay a local time zone 24-hour clock with seconds.\n\nShow events from all calendars approaching within the next 24 hours.\n\nDisplay event time in terms of how far its beginning occurs from now.\n\nWithin 1 hour of occurring, an event is highlighted in lighter gray.\n\nWithin 10 minutes of occurring, an event is highlighted in yellow, to draw visual attention.\n\nWithin 1 minute of occurring, an event is highlighted with an animated rainbow of all hues, to demand attention.\n\nOnce it begins, an event is highlighted green as visual confirmation of attendance.\n\nConsistent progression of visual cues conditions for enhanced punctuality.\n\nMost crucially, there is a security requirement:\n\nDon't share my Google data! Use the latest Google APIs to authenticate directly from a web browser, and display events, without touching my data.\n\nThat's exactly what Agendar\u2122 is!\n\nIt's required to have Google Calendar in order to run the app in your web browser, authenticate directly with the latest Google APIs, and display events, without touching your data:\n\nWithin 1 hour of occurring, an event is highlighted in lighter gray, and then within 10 minutes of occurring, an event is highlighted in yellow, to draw visual attention:\n\nFinally, as shown in the main photo, within 1 minute of occurring, an event is highlighted with an animated rainbow of all hues, to demand attention."
        },
        {
            "authors": [
                "Oleg \u0160elajev"
            ],
            "title": "Java on Truffle \u2014 Going Fully Metacircular",
            "contents": "Up until now, GraalVM has offered two ways to run Java programs: using the Java HotSpot VM with the GraalVM JIT (just-in-time) compiler, and compiled to a native executable using GraalVM Native Image.\n\nToday, we\u2019re happy to announce a new way to run Java on GraalVM. GraalVM 21.0 introduces a new installable component, named espresso , which provides a JVM implementation written in Java.\n\nJava on Truffle place in the GraalVM architecture\n\nEspresso makes it possible to run Java code via the Truffle framework elevating Java to the level of the other languages supported on GraalVM.\n\nTry it out\n\nTrying Java on Truffle out is extraordinary straightforward. It is available as a component installable into a base GraalVM distribution with a gu command.\n\ngu install espresso\n\ngu install -L espresso-installable.jar # if downloaded manually\n\nAfter the installation to run your favorite app on Java on Truffle you just need to pass -truffle to the java command.\n\njava -truffle -jar myApp.jar\n\nDownload Java on Truffle and give it a spin! There are some example apps with the instructions that illustrate the particular capabilities of Java on Truffle.\n\nNote that current raw performance of Java on Truffle isn\u2019t representative of what it will be capable of in the near future. The peak performance is several times lower than running the same code in the usual JIT mode. The warmup also hasn\u2019t been optimized yet. We focused in this initial release entirely on the functionality, compatibility, and making Java on Truffle open source available for a broader community.\n\nExpect performance, both warmup and peak performance to increase rapidly in each of our upcoming 21.x releases.\n\nLet\u2019s now look in more details what Java on Truffle is, explore some notable use cases where it can help you, and try to place the project into the larger GraalVM and Java ecosystems.\n\nJava on Truffle\n\nJava on Truffle is a JVM implemented using the Truffle language implementation framework. It provides all core components of a Java virtual machine:\n\nBytecode interpreter\n\nBytecode verifier\n\nSingle-pass .class file parser\n\nSimple object model\n\nJava Native Interface (JNI) implementation in Java\n\nVirtual Machine Implementation in Java\n\nJava Debug Wire Protocol (JDWP)\n\nA very important detail of this implementation is that it\u2019s implemented in Java. Java on Truffle is Java on Java! Self-hosting is the holy grail of Java virtual machine research and development.\n\nWhat it can do is run Java programs. Well, programs written in other JVM languages too of course. As you can see from the list above, it also supports the debug protocol, so you can debug Java applications using it too.\n\nJava on Truffle is available for both Java 8 and Java 11 based GraalVM distributions, so technically you can use it as a replacement for the JVM of your choice. Java on Truffle is currently experimental and not very fast yet, so it is not recommended for production workloads today, but let\u2019s explore what you can get from running applications with Java on Truffle.\n\nJava on Java\n\nAs we mentioned previously Java on Truffle is implemented in Java. It\u2019s a virtual machine implementation, so in order to actually run Java code it needs access to the class library and the native libraries and methods JDK provides. Java on Truffle reuses the JARs and native libraries from the GraalVM distribution.\n\nJava on Truffle is a metacircular Java VM.\n\nBeing implemented in Java and being able to run Java gives Java on Truffle a very interesting property: it can run itself. Indeed, Java on Truffle is a metacircular VM, it can run itself several levels deep (albeit slower and slower every time).\n\nBeing a Java program gives a number of advantages. One of them is ability to be compiled to the native executable with the native image, we\u2019ll explore an interesting use case for that in the following chapter.\n\nAnother advantage is that the code is nice, familiar and understandable to Java developers. Consider going to the GitHub repository and looking at the source code for it. Your day-to-day tools work for it, your IDE supports it, you can explore the code base the same way you explore any other Java dependecies. This transparency and familiarity should allow Java on Truffle be efficient at rapidly changing to the better.\n\nEmbedding 11 in 8\n\nJava on Truffle is an actual JVM, and it\u2019s also a Java program, which means you can run it within another Java program. This opens very interesting avenues for compartmentalisation of different components in your applications. For example, if you point Java on Truffle to a JDK11 distribution it can run Java 11. With access to Java 8, it becomes Java 8. When you have both distributions available, you can run Java on Truffle in the context of a Java 8 app and use it to run Java 11 byte code, and vice versa. If there\u2019s a library that is only available for Java 8, you can migrate to a newer base JDK and still run that particular library, with some programmatic efforts to establish the interoperability, in the compatible JDK 8 within the same Java process.\n\nMixing AOT and JIT\n\nSince Java on Truffle, Truffle, the GraalVM compiler and all other necessary components to run Java on Truffle efficiently are all written in Java, it is possible to build a native image executable with the infrastructure to run Java on Truffle.\n\nThis means that you can take a Java app, build a JVM into it, and then run that app either on a JVM or as a native image. Note that in the latter case, Java on Truffle can actually execute arbitrary Java code which doesn\u2019t necessarily need to be known at the build time.\n\nThat\u2019s right, Java on Truffle can bring the JIT compiler and the dynamic Java runtime to an ahead-of-time compiled binary.\n\nJava on Truffle allows adding dynamic language features to apps build with native image\n\nWe\u2019ve prepared a sample application to illustrate this concept. There\u2019s a JShell implementation example that takes a normal JShell app, which consists of two separate parts: frontend CLI app and the backend computation engine, and replaces the latter with the Java on Truffle implementation.\n\nIt actually very neatly reuses all the classes from the original implementation by just loading them. So the original part of the sample application is the \u201cglue\u201d code that connects the host Java part to the Java on Truffle part of the app.\n\nThe sample can be compiled as a native executable, resulting in a nice binary that starts faster than the usual JShell because of the native executable performance characteristics and still can execute the Java code we throw at it.\n\nHere\u2019s a screenshot of the Tetris game loaded and started from the JShell implemented with Java on Truffle.\n\nMixing AOT and JIT is a fascinating option for applications that cannot leverage the native image performance improvements because their functionality depends on dynamic code which does not work easily with Native Image.\n\nAdvanced class redefinition\n\nAnother really cool feature where Java on Truffle is more powerful than HotSpot is the enhanced Hot Swap ability \u2014 changing classes at runtime during a debugging session.\n\nAs of GraalVM 21.0 the following changes are supported:\n\nAdd and remove methods Add and remove constructors Add and remove methods from interfaces Change access modifiers of methods Change access modifiers of constructors Changes to Lambdas Add new anonymous inner classes Remove anonymous inner classes\n\nWhat will make hot swap even more powerful is the ability to make changes to class fields. It\u2019s in the works and will be added in a future release.\n\nReload changed classes action in the IntelliJ IDEA debugger.\n\nThe setup is the same as with HotSpot: you start the debugger, change the code, recompile the class, hit \u201cReload the classes\u201d in your IDE debugger and resume the program with the new code running next time the changed class is used.\n\nGraalVM ecosystem support\n\nJava on Truffle benefits out of the box from the developer tooling support that GraalVM languages get from the Truffle framework.\n\nFor example, you can run your application with some java -truffle --cpusampler and have the sampling profiler run on your code. You can enable the tracing profiler or memory tracer to tell you which parts of the code generate more memory pressure than others.\n\nAnother facet of the ecosystem is the supported languages. Java on Truffle allows you to create polyglot programs where different components are written in different languages. The details about how to load code written in other languages, export and import objects between languages, and so on are a little bit more involved than the scope of the current article, but details can be found from the docs.\n\nNext steps\n\nGraalVM 21.0 is the initial release of Java on Truffle. It\u2019s an experimental component right now and there are major improvements planned for it in upcoming releases.\n\nThere are many things to improve, from supporting javaagents, to having a better implementation of the interop protocol with other languages, major performance improvements, and so on.\n\nWe\u2019ll be working on these and other improvement, and would be absolutely thrilled to hear any and all feedback, feature requests, potential use cases, discovered issues, and shortcomings of the current version. You can share your feedback via Slack, GitHub, or Twitter. To get started, head to graalvm.org/java-on-truffle.\n\nJava on Truffle is a new and very exciting way to run your Java code. Take a look and consider the possibilities!",
            "published_at": "2021-01-20T13:38:35.965000+00:00"
        },
        {
            "authors": [
                "Wednesday",
                "January",
                "February",
                "September",
                "Tuesday",
                "December"
            ],
            "title": "Rust in Production: 1Password",
            "contents": "Rust has taken the programming language world by storm. Since its 1.0 release in 2015, it has been one of the most loved programming languages with a loyal following of developers and contributors.\n\nTo learn why this language is favored so much between developers, we have started a new series on Rust in production. In it, we\u2019ll interview people that have used Rust for significant projects: apps, services, startup MVPs, and others.\n\nFor the first installment of the series, we interview Michael Fey, VP of Engineering at 1Password. Read further to find out why they chose Rust for their product, the benefits of Rust for security-centered applications, and what cool libraries you should look into if you\u2019re developing something similar in Rust.\n\nCould you tell us a little about your company and your role there?\n\n1Password is a password manager trusted by millions of people and 70,000 businesses to secure their sensitive data. It remembers all your passwords so you don\u2019t have to, and comes with apps for all major browsers, plus desktop and mobile.\n\nI am the VP of Engineering for Client Apps here at 1Password. If you have ever had the pleasure to use 1Password on your Mac, Windows PC, iPhone, iPad, Android phone, or tablet, or in your browser, then you\u2019ve been lucky enough to use something my team has built. We\u2019ve been around since 2004, and we take a lot of pride in building a well-crafted experience and keeping people safe online.\n\nCan you talk about the stack of 1Password? How big a part of your codebase is written in Rust?\n\nWe\u2019ve been using Rust in production at 1Password for a few years now. Our Windows team was the frontrunner on this effort to the point where about 70% of 1Password 7 for Windows is written in Rust. We also ported the 1Password Brain \u2013 the engine that powers our browser filling logic \u2013 from Go to Rust at the end of 2019 so that we could take advantage of the speed and performance of deploying Rust to WebAssembly in our browser extension.\n\nThese have been in production for the last few years and we\u2019ve seen great success. So much so that we\u2019re now in the midst of a complete rewrite of nearly our entire product lineup, and Rust is a major part of that story. We are using Rust to create a headless 1Password app that encompasses all of the business logic, cryptography, database access, server communication, and more wrapped in a thin UI layer that is native to the system on which we\u2019re deploying.\n\nDid any of Rust\u2019s advantages like speed or type/memory-safety influence the choice of using Rust for 1Password?\n\nOne of the main things that drew us to Rust initially was the memory safety; it definitely excites us knowing that Rust helps us maximize our confidence in the safety of our customers\u2019 secrets. Beyond memory safety, though, there\u2019s so much more we love about the Rust ecosystem. There is a significant performance benefit to the lack of a traditional runtime; we don\u2019t have to worry about the overhead of a garbage collector, for instance. Rust offers a form of \u201cprogram correctness\u201d and many guarantees against undefined behaviour at runtime. The strong type system enforces these rules at compile-time. Carefully aligning application logic with Rust\u2019s strong type rules makes APIs difficult to use incorrectly and results in simpler code that\u2019s free from runtime checking of constraints and invariants; the compiler can guarantee there are no invalid runtime code paths that will lead your program astray before it executes. Having to perform less runtime state validation leads to cleaner, more efficient, more focused, and higher quality code. Rust requires very little runtime debugging compared to other languages. If it compiles, you can be fairly sure it won\u2019t exhibit unexpected behaviour. It may not be what you want but it will be \u201ccorrect\u201d. \ud83d\ude42\n\nAnother very powerful (and often overlooked) feature of Rust is its procedural macro system, which has allowed us to write a tool that automatically shares types defined in Rust with our client-side languages (Swift, Kotlin, and TypeScript). The output from this tool handles the serialization/deserialization process automatically, meaning our client-side devs can continue to work in their language of choice while interacting with the Rust library and can be free from the concerns of JSON parsing over the foreign function interface (FFI). We get all of this while enjoying the benefits of compile-time type checking in every one of our target languages. We\u2019ve integrated this tool into our continuous integration server as well, meaning that changes to the Rust models can result in compilation failures in the client applications that are caught during our review process.\n\nThis tool has been an integral component in our development process, allowing us to move much more quickly than ever before. Once our types are defined in Rust, we are able to immediately generate equivalent types in our client-side languages. This allows our devs to focus on solving problems without having to hand-roll boilerplate code to communicate over the FFI.\n\nHow good is Rust\u2019s support (library and otherwise) for developing security-centric applications like 1Password?\n\nThere is more than enough to build a majority of the base that security-centered applications require. There are two large, prominent cryptography platforms (ring and the Rust Crypto group) that together provide a wealth of functionality. As I mentioned above, writing with Rust itself gives you much greater confidence in your memory usage and makes it much harder to accidentally introduce a memory-related exploit into your application. There is also a wonderful system in place for keeping track of vulnerabilities that do show up from time to time in Rust crates: the RustSec database, which is community-sourced by other Rust developers and is updated frequently with new information that can be consumed in CI audit scans. The batteries-included test framework that Rust and Cargo also include mean that you always have an easy way to write unit-test suites for correct behavior in critical code, like any cryptographic functions that you write.\n\nWhile safe native Rust libraries for everything are the dream (and they will come in time), there is always the option to dip down and easily consume something in C or from native platform libraries. We use this to great effect in our Rust code for things like calling out to the native implementations of biometric unlock (Touch ID, Face ID, Windows Hello) and platform-specific settings implementations like NSUserDefaults on Apple platforms.\n\nAny particular Rust libraries that you want to feature?\n\nAbsolutely. Tokio, Hyper/Reqwest, Ring, and Neon all have a home in 1Password and are fundamental in allowing us to tackle this ambitious project at all. You should also check out our password-rules-parser on crates.io, which is based on a spec primarily being backed by Apple. Their tools and docs can be found here.\n\nWhere is Rust great to use, and where does it fall short in your stack?\n\nRust has fulfilled 90% of what we were hoping for when we started this project. We\u2019ve been able to deploy it to nearly every one of our target platforms in some way shape or form (with the exception of Apple Watch). The language itself has been designed with modern sensibilities and is improving with every release. It has great documentation and an active community.\n\nAlthough there are countless crates available for use, we did have to roll our own logging and tracing tools to ensure that they were safe to use in 1Password. Additionally, we constructed a substantial localization implementation to meet the requirements of our products.\n\nIt did fall short for us in one key area: We were hoping WebAssembly would take us further in the browser and our browser extension than it has. WebAssembly has been great as a function library, but attempting to stand up an entire runtime in WASM has been a challenge. Many of the issues we ran into, however, were not limitations of Rust but of WebAssembly as a deployment platform.\n\nWhat was the biggest challenge while developing 1Password with Rust?\n\nMany of the folks on our team were new to Rust, and they experienced the typical learning curve that comes with its memory management and ownership model. We are also finding the compile times to be pretty beefy; our CPUs and fans are definitely getting a workout. \ud83d\ude04\n\nAre you satisfied with the result?\n\nAbsolutely.\n\nAny key takeaways that you would like to share with our audience?\n\nIf you\u2019re new to Rust, start small and build on top of that. We ran a large number of experiments when we were getting started to try and find the edges of what a Rust-based solution could provide. When your experiments pan out, try to reimagine the ways you used to work with other languages and see if your code can benefit from Rust\u2019s philosophy.\n\nIf you\u2019re new to 1Password, you can sign up today with this link and save 50% off your first year for family and individual accounts. If you\u2019re working on an open source project, you can get a 1Password Teams account for free. Head on over to our GitHub repo to learn more.\n\nI would like to thank Michael for the interview, and wish 1Password the best of luck in creating the most awesome password manager out there!\n\nTo read more about programming languages like Rust, be sure to follow us on Medium, DEV, or Twitter. If you are looking for the next read, we have an article on 9 other Rust production stories that you might want to check out."
        },
        {
            "authors": [],
            "title": "Nim Community Survey 2020 Results",
            "contents": "Nim Community Survey 2020 Results\n\nThe Nim community survey 2020 has been open for one month, and we have received 769 responses - we think this is a large enough sample to draw conclusions about our users and their habits. Before we go into details, we would like to thank all the people who took the time to respond. We really appreciate the feedback!\n\nThe goal of this survey was primarily to determine how our community is using Nim, in order to better understand how we should be improving it.\n\nDo you use Nim?\n\nBased on the answer to this question, the respondents were divided into two groups that each received a separate set of questions.\n\nApproximately three quarters of the respondents use Nim (31% frequently, 41% occasionally), while the remaining quarter is divided between people who never used Nim (16%) and people who stopped using Nim (11%).\n\nOf those people who don\u2019t use Nim, the most frequent reasons are: \u201cNim seems immature, not ready for production\u201d (36%), \u201cNim doesn\u2019t have libraries I need\u201d (33%), \u201cNim seems too risky for production\u201d (25%), and \u201cNim doesn\u2019t have enough learning materials\u201d (24%).\n\nFor people who stopped using Nim, the most common reasons were (answering a free-form question): lack of libraries (small and not mature ecosystem), incomplete documentation, bad editor support, compiler crashes, coworkers don\u2019t use it, etc.\n\nUsers\n\nWe have approximately the same amount (around 40%) of new Nim users (people who started using Nim in the last 6 months) and experienced users (between 6 months and 2 years). Nim veterans (more than 2 years of Nim experience) are about 20% of our users.\n\nA typical Nim user would be a software developer from Europe.\n\nBesides Europe (more than half of our users), our users mostly come from North America and Asia. Almost half of our users are software developers (47%), and the next largest group are students (17%).\n\nWe have users from all age groups and all levels of programming experience, proving that Nim can be used both by beginners and veteran programmers.\n\nThings that people like about Nim the most are: performance/speed (86%), ease of use (76%), syntax (73%), and self-contained binaries (63%).\n\nIt\u2019s no surprise that the most used editor is VS Code, with 62% of Nim users choosing it in the survey. In the second place is Vim/Neovim (37% combined), followed by Sublime Text and Emacs (10% each).\n\nNim versions\n\nWe\u2019re glad to see that the large majority of users are using the latest stable version (1.4.x).\n\nWe can partially attribute this to the mostly painless upgrade process:\n\nUsing Nim\n\nThe situation is slightly better than in 2019, but far from what we would like to see. We hope that we\u2019ll see even larger improvements in the future. The majority of users use Nim only sporadically (25% of time or less).\n\nNim is mostly used for writing command-line programs and libraries, followed by automation/scripts, GUI, web services, and data processing.\n\nThe large majority of Nim users are targeting Linux, followed by Windows and macOS. Compared to 2019, we see an increase in JavaScript, Android, and iOS.\n\nAbout one third of Nim users use it at work, either exclusively (7%) or from time to time (26%).\n\nOf people who don\u2019t use Nim at work, one third can\u2019t use it because their company doesn\u2019t allow it. Three quarters of people who are allowed to use Nim at work plan to use it in 2021.\n\nPeople mention stability, better tooling, more third-party libraries, having to mature, and better documentation as the main areas where Nim should improve to be more accepted in their companies.\n\nLearning Nim\n\nAlmost all of our users have read the official tutorials, and about half of them have read \u201cNim by Example\u201d and/or the \u201cNim in Action\u201d book. Rosetta Code examples are also a popular choice as a learning resource, followed by \u201cNim Basics\u201d, \u201cNim Days\u201d and \u201cNim Notes\u201d.\n\nIf anyone would like to create some more learning materials for Nim (which is very high on the community priority list - see below under the \u201cNim in 2021 and beyond\u201d section), the most wanted types of learning materials are code examples and written tutorials.\n\nNim community and contributions\n\nCompared to 2019, we see a 20% increase of people who have already contributed to Nim! And there are almost three times as many people who haven\u2019t contributed to Nim so far but plan to do that in the future. We\u2019re looking forward to this :)\n\nThe most frequent reasons for not contributing (yet) are: lack of time, and lack of skill/experience.\n\nIn the past we haven\u2019t stressed enough the importance of community donations, which is reflected in this graph: only 10% of our users have made a donation in the past. We are happy to see that two thirds of our users plan to do it in the future, which will help us to have more manpower to fix the most important bugs more promptly and work on some exciting new features.\n\nWe\u2019ve asked you how satisfied you are with the Nim tooling:\n\nMost of our users use Nimble package manager and they are very satisfied with it. Fewer users picked Choosenim as their way of installing and updating Nim, but those who did are quite satisfied with what it has to offer.\n\nNim doc is generally liked by those who use it, as is Nimpretty. Nimsuggest is liked a bit less than other tools (high CPU and memory consumption), but you\u2019ll be glad to hear that our plans for 2021 include making Nimsuggest more robust, and less CPU and memory hungry.\n\nNim in 2021 and beyond\n\nAccording to the votes, fixing compiler bugs should be our first priority: 55% of our users find it very important, with only 18% of votes for \u201clow priority\u201d and \u201ccurrent situation is ok\u201d. We will continue to work on it, and we appreciate any community effort on this: we already have several individuals continuously helping us with fixing compiler bugs (thank you!), and we\u2019d benefit from more.\n\nImproving the tooling is very high on the priority list for our users (higher than in 2019), and especially for those users who use Nim professionally. We\u2019re happy to announce that, thanks to our partnership with Status, improving the developer tooling is one of the goals for 2021.\n\nBack in 2019 we improved the documentation of the most used standard library modules. There is still room for improvement, but we now see that more users (compared to the 2019 survey) think the current situation is ok. When it comes to learning materials, where we mostly lean on the community content (for this to improve we need your involvement!), people want to see more written tutorials and code examples.\n\nAccording to respondents, fixing bugs in the existing standard library modules should be higher priority than expanding the standard library. One of the reasons that expanding the standard library is less of a priority could be that Nimble now has more than 1500 packages, adding significant functionality that is not included in the standard library.\n\nTo summarize, 2021 is the year of stabilization: our priorities will be fixing the compiler and standard library bugs, improving tooling and documentation, and improving and expanding the learning materials.\n\nLast words\n\nThank you to each and every one of you who took the time to answer this survey. Your time is precious and we are deeply thankful that you used it to share your feedback.\n\nPlease remember that you do not need to wait for a survey in order to give us feedback; of course you\u2019re more than welcome to wait if you wish, but ideally share your feedback with us immediately. We can be found in various places - see the community page for links and instructions on how to get in touch.\n\nIf you would like to help us in our 2021 plans, consider contributing or making a donation. You can donate via:\n\nIf you are a company, we also offer commercial support. Please get in touch with us via [email protected]. As a commercial backer, you can decide what features and bugfixes should be prioritized.",
            "published_at": "2021-01-20T00:00:00"
        },
        {
            "authors": [
                "Amos Chapple"
            ],
            "title": "In Living Color: Georgia Before The Soviets Arrived",
            "contents": "The people and spectacular cityscapes of tsarist Georgia captured in vivid color by a world famous photographer.\n\nThis is one of more than 100 images made by the Russian photographer Sergei Prokudin-Gorsky on the territory of today\u2019s Georgia -- then a part of the Russian Empire. The astonishing color photos were made shortly before Vladimir Lenin\u2019s revolutionaries seized power in Russia in 1917 and, later, sent their conquering Red Army to impose communist rule over the Caucasus.\n\nProkudin-Gorsky (1863\u20131944) first traveled through the Caucasus with his camera in 1905, then returned to the sun-drenched region in 1912 as one of the greatest early practitioners of color photography.\n\nProkudin-Gorsky perfected a complex early method of color photography that required three separate images of each scene to be shot, with color filters placed over the lens. When the three black-and-white photos were sandwiched together and had red, green, and blue light shone through them, a color image could be projected.\n\nAlthough he worked in at least 16 countries, more than one-quarter of the photos Prokudin-Gorsky took outside of his native Russia were made on the territory of today\u2019s Georgia."
        },
        {
            "authors": [],
            "title": "treenotation/dumbdown: The dumb alternative to markdown. The keyword for title is title.",
            "contents": "Dumbdown - The dumb alternative to markdown\n\nFirst, an example\n\ntitle This is Dumbdown. The keyword for title is title. subtitle Dumbdown compiles to html. paragraph Dumbdown is an alternative to Markdown. paragraph Blank lines get turned into <br> link https://treenotation.org/ It is built on Tree Notation code alert(`The keyword for code blocks is \"code\"`) list item Dumbdown supports lists item For now the keyword for an item is \"item\" item This is a very early version item Dumbdown is released to the public domain item If you want to make it better, please do!\n\nTry it Now\n\nThe original prototype: https://jtree.treenotation.org/designer/#standard%20dumbdown\n\nAn actual v1 spec is now in the discussion phase.\n\nInstall the Dumbdown Static Blog tool\n\nIn addition to containing the Dumbdown spec, this repo contain a simple static blog software as a demo app using Dumbdown.\n\nLink: https://www.npmjs.com/package/dumbdown-js\n\nnpm install -g dumbdown-js\n\nCLI Usage\n\ndumbdown\n\nWhy Dumbdown?\n\nDo you want a markup language that doesn't require memorizing esoteric symbols but uses words instead?\n\nDo you want a markup langauge where you not only don't have to remember which order brackets go in\u2014 is it ()[] or \u2014but that doesn't use brackets at all?!\n\nDo you want a markup language that is extensible, so you can store your own custom config data right alongside your content?\n\nDo you want a markup language where it is super easy to embed any kind of data or code without doing adding escape characters?\n\nDo you want a markup language that you could write your own parser for without having to learn complex parsing techniques?\n\nIf you answered YES to the questions above, then Dumbdown is for you?!\n\nFeatures\n\nKeywords instead of key characters. ie \"title\" instead of \"#\". No brackets. Links are just \"link\", or type the full url for inline links. Stick your own custom config data in. Every file parses to a map. i.e. \"published true\". No need to escape characters for snippets. Just indent blocks. Very easy to write your own parsers for. It's just Tree Notation.\n\nProject Status\n\nCurrently: Discussing spec for v1.\n\nGoal by January, 2022: Ship a final v1 spec\n\nNote: this Readme.md file is written in markdown, but if someone wants to work on Dumbdown syntax highlighting for GitHub, once we have a spec, that would be great!\n\nIssue is here: https://github.com/treenotation/dumbdown/issues/1\n\nFAQ\n\nHow do I do inline formatting like bold, italics, and links?\n\nTree Languages are very different than traditional languages.\n\nThey combine exceptionally well.\n\nYou can literally just copy and paste 2 grammars together, and then change just 1 or 2 lines of code and get a new \"3rd\" language that incorporates both.\n\nThis enables many approaches to supporting any style of node that would support inline formats.\n\nFor example, I could create a node type called markdownParagraph and then a user could use normal markdown inside a Dumbdown document:\n\nmarkdownParagraph Put **bold** or _italic_ text here, or [links](/link).\n\nBut that's just the beginning! Imagine extending Dumbdown with brand new ideas, never before tried:\n\nemojiParagraph In this hypothetical emoji paragraph you can wrap a sentence in <b></b> simply by ending it with\u2757\n\nWhy not use a shorter syntax?\n\nAlthough you can use Dumbdown with any text editor, since it is so simple, Dumbdown is not designed with today's editors in mind.\n\nIt's designed for a new upcoming wave of so called \"2-Dimensional\" editors. Think of Spreedsheets, with grids, instead of IDEs.\n\nDumbdown is designed to be best used in editors that support advanced autocomplete, type checking, syntax highlighting, additional secondary notations, and real time projections.\n\nCan I compile my dumbdown docs into markdown instead of HTML?\n\nYes. The first prototype compiled to markdown. That was just dropped for simplicity in the second prototype which just compiles to HTML. But actual real implementations will likely compile to both.\n\nRelease Notes\n\nVersion 0.2 01/20/2021 -- Let's make this a real thing. Readme and GitHub project started.\n\nVersion 0.1 09/01/2019 -- Prototype and idea first launched.\n\nLeadership\n\nbreck7 is currently the BDFL: Benevolent Dummy For Life. But if you feel like you can be a better Dummy, please either fork this project and prove it, or just get involved and stage a peaceful coup. Breck would happily relinguish the BDFL title if a better Dummy comes along.\n\nImportant\n\nIt should go without saying, but this is public domain.\n\nDumbdown is made with love in memory and honor of Aaron Swartz.\n\nhttp://www.aaronsw.com/weblog/001189"
        },
        {
            "authors": [],
            "title": "The Irrevocable SSL certificates of CloudFlare",
            "contents": "The Irrevocable SSL certificates of CloudFlare\n\nJan 20, 2021\n\nI moved this website \u201cworldofmatthew.com\u201d away from CloudFlare back in August 2020.\n\nThat should have been the end of the relation with CloudFlare for my website.\n\nUnfortunately, while I was moving the L7 Anti-DDOS protection to path.net (via a reseller), I checked the SSL Transparency and noticed that CloudFlare still had a vaild SSL certificate for this website.\n\nEven worse, this CloudFlare certificate lasts for a year, meaning they have a SSL certificate for this website, until June 2021.\n\nIrrevocable SSL Certificates by CloudFlare\n\nThis website currently uses path.net for DDOS migration. Just like CloudFlare, they have their own SSL certificate for this website. As they require it to MITM website requests to block anyone trying to perform an L7 attack.\n\nThe difference is that path.net uses Let\u2019s Encrypt, which would allow me to revoke their SSL certificate for this website. All I would have to do is to download the public key and present that with an Acme challenge and then I can revoke it.\n\nYou would think that it would be possible to get a CloudFlare SSL certificate revoked?\n\nHow about No?\n\nWell. Unless you want to pay CloudFlare $10 per month to gain access to the \u201cAdvanced Certificate Manager\u201d.\n\nThis would give access to use a custom Let\u2019s Encrypt. This would also allow you to set your SSL certificate validity period to as low as 14 days, rather than a year on the free \u201cUniversal SSL\u201d.\n\nBasically, it is a case of pay CloudFlare $10 per month or they get 100% control over your SSL certificate and will deny your requests to revoke.\n\nIn fact, the official stance of the SSL team at CloudFlare is that they won\u2019t revoke unless the team has \u201cdetermined the private key was compromised.\u201d\n\nIsn\u2019t this sounding pretty corrupt?\n\nYes\u2026.\n\nYou blindly trust that CloudFlare will delete the SSL certificate upon leaving. They will provide you zero evidence of them of actually deleting the certificate.\n\nThis is from a CDN who has a level of access unmatched by their rivals. CloudFlare are becoming the defualt DNS for browsers and have a VPN service.\n\nYou have to trust them to never abuse that power.\n\nEven worse, as they forcibly keep a working SSL certificate, they don\u2019t even have to generate a new one. Meaning, SSL transparency WILL NOT show anything if CloudFlare ever decide to hijack and MITM traffic to your website.\n\nIf someone asks that a SSL certificate is revoked after leaving it, revoke it. The webmaster is no longer giving them permission to have a SSL certificate for their domain.\n\nBy not allowing for SSL certificates to be revoked, CloudFlare is in effect holding them hostage. It must not be the job of a Certificate authority to refuse such requests.\n\nMost Certificate authorities would never lock a webmaster out of control of a SSL certificate for their domain.\n\nThey also would not tell people to pay of $10/month for get rights to revoke a Certificate.\n\nThis comes to a question of whether CloudFlare should even be in SSL chain-of-trust.\n\nI requested for them to revoke, by not revoking Cloudflare are keeping an active SSL Certificate without my permission. I think such behavior should make browsers question if they can keep Cloudflare in their chain-of-trust."
        },
        {
            "authors": [
                "Dan Rockmore",
                "Dan Rockmor",
                "David Kaise",
                "Patrick Di Just"
            ],
            "title": "Three Mathematicians We Lost in 2020",
            "contents": "Life in the real world is complicated. It\u2019s much simpler on the computer. As the Game of Life begins, the screen is filled with a vast latticework of squares, only a few of them filled in. The magic is in the algorithms, which determine, on the basis of the current pattern, what will happen next. As time ticks on, whether any given square will be vacant or occupied, dead or alive, depends on its present state, as well as the states of its nearest neighbors, and possibly of their neighbors, and of their neighbors twice or three times removed. Change the first pattern, rewrite, delete, or add an algorithmic rule, and the pattern may grow unbounded and crenellated or recede to a tiny, moving archipelago, or evolve only to cycle back to its initial configuration, so that it can start over again. The wheel spins on.\n\nThe \u201cmoral\u201d that excited everyone when this computer game was invented, in 1970, was that simplicity could beget complexity. It provided users with a computational version of the molecular primordial soup, with its ramifications intact but time collapsed. These days, of course, it\u2019s hard not to see our real lives in the game\u2019s simple, abstract terms. Variations on Life are used to model epidemics like the one we\u2019re experiencing. The coronavirus is among the simplest life forms, living only to reproduce, occupying each host only to find the next, through some form of basic contact transmission. We are the squares, some of us occupied and some vacant, all of us doing what we can to avoid being a nearest, or even second-nearest, neighbor.\n\nThe inventor of Game of Life, John Conway, is among those we\u2019ve lost to the coronavirus this year. (He died, in April, of COVID-19.) I sometimes wonder if, given his ironic and dark sense of humor, he would\u2019ve appreciated the symmetry. He was an extraordinarily creative mathematician, one who needed to see a problem as a puzzle or a game in order for it to seize his interest.\n\nEarly on, Conway made his name by solving a complicated puzzle about symmetries in twenty-four dimensions. For most of us, it\u2019s easier to start with two. Imagine that you have a pile of identical Frisbees. You want to lay as many of them down on the floor as possible\u2014no stacking allowed! Try it, and you\u2019ll probably find quite quickly that setting them out in rows that are shifted just a bit, so that the boundary of one Frisbee dips into the cleavage between the two below, is the best that you can do. Nestle a few rows together in this way, and you\u2019ll find that any given Frisbee is surrounded by six others. At this point, a mathematician might place an imaginary peg in the center of a group of six Frisbees, then connect the pegs with imaginary lines. Do this, and you get a perfect hexagon, a shape that\u2019s symmetrical in a number of ways: you can flip it across various axes or rotate it around its center in steps of sixty degrees, and, except for which corners are where, it remains unchanged.\n\nIn mathematical lingo, we\u2019ve taken a few interesting steps. We started with a \u201cpacking problem\u201d; by solving it, we uncovered a symmetrical shape; that shape, in turn, contains its own \u201cgroup\u201d of symmetries. We could take the same steps in three dimensions. Suppose that you replace the Frisbees with perfectly spherical, identical oranges. (A mathematician\u2019s oranges are always perfect spheres.) Now we\u2019re contemplating the so-called greengrocer\u2019s problem\u2014the question of the best way to stack fruit in a market. In the usual greengrocer\u2019s arrangement, layers of oranges are stacked such that each orange touches twelve others. The three-dimensional polyhedron created when we connect the centers of the neighboring oranges also has its own, much larger, group of symmetries.\n\nFor John Conway, mathematics was a passion and an escape. Photograph by Denise Applewhite / Princeton University\n\nThe mathematics of symmetry is called \u201cgroup theory,\u201d and its modern origins are generally traced back to the nineteenth-century mathematician Evariste Galois, who\u2014in one of the most romantic of mathematical legends\u2014is said to have feverishly organized his definitive manuscripts the night before a duel in which he died. Galois wasn\u2019t interested in symmetries in space, but in symmetries among and within solutions to equations. A given solution to an equation might have a mirror solution, differing only in its sign: \u221a2 and -\u221a2, for instance. Galois realized that the complexity involved in solving an equation was intimately related to the complexity of the \u201cgroup\u201d of its solutions\u2019 symmetries. His discovery initiated over a century\u2019s worth of work aimed at ferreting out groups of symmetries hidden in ever-more-complicated mathematical and geometric structures. By the late nineteen-sixties, mathematicians were racing to fill out a complete catalog.\n\nConway, who had been hunting around for a good problem, had followed the work of the British mathematician John Leech, who had explored the packing of spheres in twenty-four-dimensional space. Leech had found that, in this fantastical grocery store, each sphere simultaneously touches 196,560 others. But the complete symmetries of the gemlike object obtained by connecting the centers of those neighbors were still unknown, and Conway decided to take a crack at finding them. He set out a deliberate schedule of work, anticipating that it would go on for weeks, but then blazed to a solution in a single, Galois-worthy night of intellectual frenzy: he found that his twenty-four-dimensional crystal was symmetrical in 8,315,553,613,086,720,000 distinct ways. This work would ultimately find applications in the creation of codes useful for communication between satellites and Earth. The so-called Conway Group, meanwhile, paved the way for the uncovering of an even larger group, which mathematicians call the Monster\u2014a group of symmetries as large as the number of kilograms of matter in the observable universe, which lives in a space of over a hundred and ninety thousand dimensions. The Monster has helped mathematicians to understand prime numbers, and given physicists new insights into quantum gravity.\n\nConway was also a showman and a showoff and an intellectual competitor. A favorite parlor trick of his was to tell you the day of the week on any date, something he could do faster than anyone else. At Princeton, he could usually be found not in his office\u2014which resembled a mathematical apothecary shop hit by a tornado\u2014but in the large and somewhat soulless common room of Fine Hall, the massive looming tower, on the edge of the Princeton campus, that is the home of the mathematics department. The common room would come to life only in the mid to late afternoon, just as things were revving up for the daily \u201ctea,\u201d a small box-cookie reception roughly marking the time when most classes had ended and a few seminars were about to start. Conway would often hold court there, hard to miss, a cross between Rasputin and a Middle Ages minstrel, loudly talking philosophy and mathematics, playing the board game Go, or engaging in some other kind of mathematical competition, surrounded by adoring and admiring students, faculty, and visitors.\n\nIn 1989, I was in the audience at a conference at M.I.T., where Conway gave a lecture, titled \u201cComputers and Frivolity,\u201d to a packed house. I remember little about the content\u2014something about the way in which a spirit of curiosity and fun, mixed with a little computing, could be a pathway to some deep mathematics. What I do remember quite clearly was that Conway gave the talk using an overhead projector with a single transparency; each time he filled the transparency, he picked it up and then, to the horrified delight of the audience, licked it clean, then resumed writing. To Conway, mathematics was a game\u2014so much so that he discovered, or invented, a new class of numbers that can be infinitely large and infinitely small: the \u201csurreal numbers,\u201d which include the real ones.\n\nConway had both a disciplined and an undisciplined mind. He was childlike in many ways, and he took advantage of the kind of leeway that we grant to geniuses. His method was to make mathematics out of whatever caught his fancy, but to do so with laser focus. You and I might notice that brickwork often has a pattern to it; only Conway could turn that into a deep exploration of symmetry. (After I heard him speak on this subject, walks through Central Park were never the same.) Some say that his contributions to mathematics peaked with his discovery of the Monster. That\u2019s a little like saying that, after \u201cAnna Karenina,\u201d it was all downhill for Tolstoy; still, Conway himself often fretted about losing his mathematical powers. In the early two-thousands, Conway was among the mathematicians my co-producers and I interviewed for a documentary, \u201cThe Math Life.\u201d We had a broad and fascinating conversation, ranging from deep mathematics to word origins (\u201cnumb\u201d and \u201cnumber\u201d are very likely connected!) and wordplay. He reflected on a life of intellectual privilege, the joys of teaching, and the wild highs and dark lows of perpetual thinking. Our talk of his achievements was tinged with melancholy as he reminisced about the \u201cwhite hot\u201d creative moments now in his rearview mirror. While Conway\u2019s life was full of honors and mathematical achievement, it was also just a life, and a complicated one, one as messy as his office: several marriages, bouts of depression, and even a suicide attempt. Mathematics was both a passion and an escape. \u201cYou know the saying \u2018Euclid alone has looked on beauty bare\u2019?\u201d he asked me. (It\u2019s a line from Edna St. Vincent Millay.) \u201cWell, what does that mean? I think it means that, you know, in Euclidean geometry, because it\u2019s stripped\u2014stripped of cats and twigs and palaver\u2014there\u2019s just something pure, and clean, and simple, and exact, and precise.\u201d\n\nIn Conway\u2019s Game of Life, chaos emerges from order. In reality, it\u2019s generally the other way around: life is lived forward and understood backward, which is to say that the search for order in randomness is a very human endeavor. In that sense, we\u2019re all mathematicians\u2014pattern seekers and pattern creators at heart, from the search for meaning in our terrestrial wanderings to the imposition of constellational structures as we scan the night sky. In the vast heavens over a pitch-black town in ancient Greece, how could your eye not connect a line of stars into Orion\u2019s Belt, or a group of them into the Big and Little Dippers? There are mathematical manifestations of this impulse. The game of connect-the-dots that our forebears played with the stars is a precursor to a zone of inquiry that mathematicians call Ramsey Theory\u2014named for the British mathematician Frank Ramsey\u2014which explores the inevitability of finding preordained structures in collections of random dots. Essentially, it investigates the conditions under which structure is unavoidable.\n\nSocial events can sometimes feel like settings populated by random people (or random points, to a mathematician). Here\u2019s a question: how many people do you need to invite to a party to guarantee that three of them will be either mutual friends or mutual strangers? You might visualize a gathering as a network, with lines connecting friends or strangers; in either case, their constellations will form triangles. The \u201cRamsey number\u201d associated with this scenario tells us the minimum gathering size we need in order for those triangles to emerge. In this case, five people is too few, but six will do the trick\u2014so the Ramsey number for this odd social-engineering task is six.\n\nFor more complex scenarios, Ramsey numbers are notoriously difficult to calculate. They seem to require the listing out, for each guest, of the other guests they do and don\u2019t know\u2014an enumeration that quickly becomes an unmanageable task. Instead of making lists, mathematicians have tended to reframe the question in terms of an upper bound: we might conclude that the Ramsey number, whatever it is, is no higher than a certain other number. Finding these bounds can quickly take us into the numerical stratosphere. It was through such a quest that Ron Graham, who also died this year, arrived at Graham\u2019s number, once called \u201cthe largest number ever to have a use.\u201d\n\nGraham\u2019s early life was one of great peregrination in which a love of mathematics was a steady and portable source of comfort. A few years of school here and a few there led to entry into the University of Chicago at fifteen, through a program for precocious teen-agers; he studied philosophy and literature in the school\u2019s \u201cgreat books\u201d program\u2014Carl Sagan was a classmate\u2014then left, a few credits shy of a degree, to study mathematics at Berkeley. He left Berkeley early, too, to enlist in the Air Force\u2014\u201cThe brochures looked great!\u201d he told me, when I interviewed him for \u201cThe Math Life\u201d\u2014and, while stationed in Fairbanks, Alaska, earned a degree in physics as a part-time student. Later, he would return to Berkeley to finish his doctorate in math, becoming one of the great \u201ccombinatorialists\u201d of our time. Many parlor-room questions are combinatorial: \u201cHow many ways can we seat these people at this table so that no one is sitting next to someone she knows?\u201d But there are less familiar questions, and the beautiful formulas that answer them inform probability theory and computer science.\n\nRon Graham had a sharklike intellect, ravenous and always on the move. Photograph by Cheryl Graham\n\nGraham\u2019s number mixes party planning with geometry. Imagine a party held on a jungle gym with eight guests; each guest sits on a corner of a cube. By slicing the cube through any two parallel edges, it\u2019s possible to isolate a four-person \u201ctable\u201d\u2014a plane on which four guests sit. Six of these \u201cfour-tops\u201d are made by the sides of the cube; six more are made by diagonal slices through it. You might ask yourself whether, at such a party, you\u2019re guaranteed to find that the guests at any of these four-tops will either all know one another or all be strangers. The answer is no: with eight guests at a cubical party, such a social arrangement isn\u2019t guaranteed."
        },
        {
            "authors": [],
            "title": "TSP #181 - Starlink Dish Phased Array Design, Architecture & RF In-depth Analysis",
            "contents": ""
        },
        {
            "authors": [
                "Cbc News"
            ],
            "title": "Bison rangers wanted to oversee U.K. herd \u2014 no bison experience necessary",
            "contents": "Read Story Transcript\n\nIt's a job in the great outdoors that doesn't require applicants to have done it before, but the successful candidates will need to be comfortable with horns, hooves and fur.\n\nA conservation project in England is seeking to hire two rangers to oversee a small herd of four to 10 European bison that will be introduced to the ancient Blean Woods near Canterbury in the spring of 2022. The area hasn't seen bison roaming the wild for an estimated 6,000 years.\n\nStan Smith is the wilder landscapes manager at the Kent Wildlife Trust, a group that has partnered with the Wildwood Trust to create the Wilder Blean project to reintroduce the bison. He hopes the initiative will increase ecological diversity there and is looking for humans to help make it happen.\n\n\"We're not expecting people to have bison experience in the U.K., that's for sure, because you couldn't get any yet. There aren't any here,\" he told As It Happens host Carol Off from Hastings, England.\n\n\"But what we're looking for in people is somebody who has a really good sort of empathy with animals, that they're probably used to managing large grazing herbivores, things like cattle or horses, and somebody who really understands ... the ecology of the area,\" he said.\n\nThe West Blean Woods will be home to the bison herd, one of the biggest areas of ancient woodland in the United Kingdom, according to the Kent Wildlife Trust. (Ray Lewis)\n\nSmith says that previously, the area would have been home to the steppe bison, which is extinct.\n\nThe new bison herd will have about 200 hectares of land to graze \u2014 about half of the woodland's 400 hectares. They'll be kept in by an electric fence, according to Smith.\n\n\"It's a really nice big size of woodland, one of our largest areas of ancient woodland that we still have ... in the south of England,\" said Smith.\n\nBison are 'ecosystem engineers'\n\nThe hope is that the animals will benefit the woodland's ecosystem through their natural grazing behaviours.\n\nSmith says bison are natural \"ecosystem engineers\" that modify their habitats just by being there.\n\nThey also obviously produce a lot of dung. And that's fantastic for invertebrates and for fungi. - Stan Smith, Kent Wildlife Trust\n\n\"They are Europe's largest remaining land mammal and ... they love to do things like eat bark off trees, which can sort of selectively kill off certain trees, allowing more light to the woodland floor \u2014 the sort of thing that we try to do with chainsaws quite frequently,\" said Smith.\n\n\"They also obviously produce a lot of dung. And that's fantastic for invertebrates and for fungi and things like that,\" he said.\n\nThe bison may also benefit a type of woodland butterfly, the Heath Fritillary, which emerges in late spring and early summer and relies on a plant called the common cow-wheat for food.\n\nSmith explained that the plant requires freshly-cleared woodland areas to grow, which the bison could provide through grazing on vegetation and stripping bark.\n\n\"So you've got Europe's largest land mammal having a beneficial impact on this on this tiny, tiny little butterfly,\" he said.\n\nThe Netherlands have had wild bison for about 15 years now, Smith says, and he personally noticed a huge improvement in habitat health when he visited bison projects there.\n\n\"You just can't help but notice the birdsong that's there ... the mushrooms that are growing everywhere and the lizards and reptiles that are all around, \" he said.\n\nBison at Slikken van de Heen Nature Reserve in Zeeland, Netherlands. (Amanda Fegan)\n\nDung samples key to herd management\n\nA key part of the job for the rangers will be to keep the animals wild.\n\n\"We want them to be behaving entirely naturally and not [become] used to humans,\" said Smith.\n\nThe bison managers will monitor the animals from a distance, collecting dung samples to assess their health without approaching them. Smith said the managers will have to track and follow the herd to keep up with what they're doing and where they're going.\n\nSmith says they are starting small at first to make sure the bison will have enough food year-round.\n\n\"That way we can really test the impact that they're having on the environment and tweak things as we go along if we have to,\" he said.\n\nHe also noted that bison breed slowly, at most one calf per female, per year, so they don't expect the herd size to balloon rapidly \u2014 one of many factors he says has helped foster support for the project locally.\n\n\"Everyone's actually very, very happy with it. It seems everyone's really excited to see the bison come.\"\n\nWritten by Andrea Bellemare. Interview produced by Chloe Shantz-Hilkes."
        },
        {
            "authors": [],
            "title": "Cruzcampo | As\u00ed se hizo #ConMuchoAcento",
            "contents": ""
        },
        {
            "authors": [],
            "title": "Tell HN: Today is the 21st day of the 21st year of the 21st century",
            "contents": ""
        },
        {
            "authors": [
                "Samantha Rose Hill",
                "Andy Wimbush",
                "Catherine Stinson",
                "Daniel Putnam",
                "Luara Ferracioli"
            ],
            "title": "For Hannah Arendt, totalitarianism is rooted in loneliness",
            "contents": "What prepares men for totalitarian domination in the non-totalitarian world is the fact that loneliness, once a borderline experience usually suffered in certain marginal social conditions like old age, has become an everyday experience \u2026\n\n\u2013 From The Origins of Totalitarianism (1951) by Hannah Arendt\n\n\u2018Please write regularly, or otherwise I am going to die out here.\u2019 Hannah Arendt didn\u2019t usually begin letters to her husband this way, but in the spring of 1955 she found herself alone in a \u2018wilderness\u2019. After the publication of The Origins of Totalitarianism, she was invited to be a visiting lecturer at the University of California, Berkeley. She didn\u2019t like the intellectual atmosphere. Her colleagues lacked a sense of humour, and the cloud of McCarthyism hung over social life. She was told there would be 30 students in her undergraduate classes: there were 120, in each. She hated being on stage lecturing every day: \u2018I simply can\u2019t be exposed to the public five times a week \u2013 in other words, never get out of the public eye. I feel as if I have to go around looking for myself.\u2019 The one oasis she found was in a dockworker-turned-philosopher from San Francisco, Eric Hoffer \u2013 but she wasn\u2019t sure about him either: she told her friend Karl Jaspers that Hoffer was \u2018the best thing this country has to offer\u2019; she told her husband Heinrich Bl\u00fccher that Hoffer was \u2018very charming, but not bright\u2019.\n\nArendt was no stranger to bouts of loneliness. From an early age, she had a keen sense that she was different, an outsider, a pariah, and often preferred to be on her own. Her father died of syphilis when she was seven; she faked all manner of illnesses to avoid going to school as a child so she could stay at home; her first husband left her in Berlin after the burning of the Reichstag; she was stateless for nearly 20 years. But, as Arendt knew, loneliness is a part of the human condition. Everybody feels lonely from time to time.\n\nWriting on loneliness often falls into one of two camps: the overindulgent memoir, or the rational medicalisation that treats loneliness as something to be cured. Both approaches leave the reader a bit cold. One wallows in loneliness, while the other tries to do away with it altogether. And this is in part because loneliness is so difficult to communicate. As soon as we begin to talk about loneliness, we transform one of the most deeply felt human experiences into an object of contemplation, and a subject of reason. Language fails to capture loneliness because loneliness is a universal term that applies to a particular experience. Everybody experiences loneliness, but they experience it differently.\n\nAs a word, \u2018loneliness\u2019 is relatively new to the English language. One of the first uses was in William Shakespeare\u2019s tragedy Hamlet, which was written around 1600. Polonius beseeches Ophelia: \u2018Read on this book, that show of such an exercise may colour your loneliness.\u2019 (He is counselling her to read from a prayer book, so no one will be suspicious of her being alone \u2013 here the connotation is of not being with others rather than any feeling of wishing that she was.)\n\nThroughout the 16th century, loneliness was often evoked in sermons to frighten churchgoers from sin \u2013 people were asked to imagine themselves in lonely places such as hell or the grave. But well into the 17th century, the word was still rarely used. In 1674, the English naturalist John Ray included \u2018loneliness\u2019 in a list of infrequently used words, and defined it as a term to describe places and people \u2018far from neighbours\u2019. A century later, the word hadn\u2019t changed much. In Samuel Johnson\u2019s A Dictionary of the English Language (1755), he described the adjective \u2018lonely\u2019 solely in terms of the state of being alone (the \u2018lonely fox\u2019), or a deserted place (\u2018lonely rocks\u2019) \u2013 much as Shakespeare used the term in the example from Hamlet above.\n\nUntil the 19th century, loneliness referred to an action \u2013 crossing a threshold, or journeying to a place outside a city \u2013 and had less to do with feeling. Descriptions of loneliness and abandonment were used to rouse the terror of nonexistence within men, to get them to imagine absolute isolation, cut off from the world and God\u2019s love. And in a certain way, this makes sense. The first negative word spoken by God about his creation in the Bible comes in Genesis after he made Adam: \u2018And the Lord God said, \u201cIt is not good that man is alone; I shall make him a helpmate opposite him.\u201d\u2019\n\nTotalitarianism found a way to crystallise occasional loneliness into a permanent state of being\n\nIn the 19th century, amid modernity, loneliness lost its connection with religion and began to be associated with secular feelings of alienation. The use of the term began to increase sharply after 1800 with the arrival of the Industrial Revolution, and continued to climb until the 1990s until it levelled off, rising again during the first decades of the 21st century. Loneliness took up character and cause in Herman Melville\u2019s \u2018Bartleby, the Scrivener: A Story of Wall Street\u2019 (1853), the realist paintings of Edward Hopper, and T S Eliot\u2019s poem The Waste Land (1922). It was engrained in the social and political landscape, romanticised, poeticised, lamented.\n\nBut in the middle of the 20th century, Arendt approached loneliness differently. For her, it was both something that could be done and something that was experienced. In the 1950s, as she was trying to write a book about Karl Marx at the height of McCarthyism, she came to think about loneliness in relationship to ideology and terror. Arendt thought the experience of loneliness itself had changed under conditions of totalitarianism:\n\nWhat prepares men for totalitarian domination in the non-totalitarian world is the fact that loneliness, once a borderline experience usually suffered in certain marginal social conditions like old age, has become an everyday experience of the ever-growing masses of our century.\n\nTotalitarianism in power found a way to crystallise the occasional experience of loneliness into a permanent state of being. Through the use of isolation and terror, totalitarian regimes created the conditions for loneliness, and then appealed to people\u2019s loneliness with ideological propaganda.\n\nBefore Arendt left to teach at Berkeley, she\u2019d published an essay on \u2018Ideology and Terror\u2019 (1953) dealing with isolation, loneliness and solitude in a Festschrift for Jaspers\u2019s 70th birthday. This essay, alongside her book The Origins of Totalitarianism, became the foundation for her oversubscribed course at Berkeley, \u2018Totalitarianism\u2019. The class was divided into four parts: the decay of political institutions, the growth of the masses, imperialism, and the emergence of political parties as interest-group ideologies. In her opening lecture, she framed the course by reflecting on how the relationship between political theory and politics has become doubtful in the modern age. She argued that there was an increasing, general willingness to do away with theory in favour of mere opinions and ideologies. \u2018Many,\u2019 she said, \u2018think they can dispense with theory altogether, which of course only means that they want their own theory, underlying their own statements, to be accepted as gospel truth.\u2019\n\nArendt was referring to the way in which \u2018ideology\u2019 had been used as a desire to divorce thinking from action \u2013 \u2018ideology\u2019 comes from the French id\u00e9ologie, and was first used during the French Revolution, but didn\u2019t become popularised until the publication of Marx and Friedrich Engels\u2019s The German Ideology (written in 1846) and later Karl Mannheim\u2019s Ideology and Utopia (1929), which she reviewed for Die Gesellschaft in 1930.\n\nIn 1958, a revised version of \u2018Ideology and Terror\u2019 was added as a new conclusion to the second edition of The Origins of Totalitarianism.\n\nOrigins is a 600-page work divided into three sections on antisemitism, imperialism and totalitarianism. As Arendt worked on it, the text changed over time, to incorporate new information about Hitler and Stalin as it emerged from Europe. The initial conclusion, published in 1951, reflected on the fact that, even if totalitarian regimes disappeared from the world, the elements of totalitarianism would remain. \u2018Totalitarian solutions,\u2019 she wrote, \u2018may well survive the fall of totalitarian regimes in the form of strong temptations which will come up whenever it seems impossible to alleviate political, social, or economic misery in a manner worthy of man.\u2019 When Arendt added \u2018Ideology and Terror\u2019 to Origins in 1958, the tenor of the work changed. The elements of totalitarianism were numerous, but in loneliness she found the essence of totalitarian government, and the common ground of terror.\n\nWhy loneliness is not obvious.\n\nArendt\u2019s answer was: because loneliness radically cuts people off from human connection. She defined loneliness as a kind of wilderness where a person feels deserted by all worldliness and human companionship, even when surrounded by others. The word she used in her mother tongue for loneliness was Verlassenheit \u2013 a state of being abandoned, or abandon-ness. Loneliness, she argued, is \u2018among the most radical and desperate experiences of man\u2019, because in loneliness we are unable to realise our full capacity for action as human beings. When we experience loneliness, we lose the ability to experience anything else; and, in loneliness, we are unable to make new beginnings.\n\nTotalitarianism destroys man\u2019s ability to think, while turning each in his lonely isolation against all others\n\nIn order to illustrate why loneliness is the essence of totalitarianism and the common ground of terror, Arendt distinguished isolation from loneliness, and loneliness from solitude. Isolation, she argued, is sometimes necessary for creative activity. Even the mere reading of a book, she says requires some degree of isolation. One must intentionally turn away from the world to make space for the experience of solitude but, once alone, one is always able to turn back:\n\nIsolation and loneliness are not the same. I can be isolated \u2013 that is in a situation in which I cannot act, because there is nobody who will act with me \u2013 without being lonely; and I can be lonely \u2013 that is in a situation in which I as a person feel myself deserted by all human companionship \u2013 without being isolated.\n\nTotalitarianism uses isolation to deprive people of human companionship, making action in the world impossible, while destroying the space of solitude. The iron-band of totalitarianism, as Arendt calls it, destroys man\u2019s ability to move, to act, and to think, while turning each individual in his lonely isolation against all others, and himself. The world becomes a wilderness, where neither experience nor thinking are possible.\n\nTotalitarian movements use ideology to isolate individuals. Isolate means \u2018to cause a person to be or remain alone or apart from others\u2019. Arendt spends the first part of \u2018Ideology and Terror\u2019 breaking down the \u2018recipes of ideologies\u2019 into their basic ingredients to show how this is done:\n\nideologies are divorced from the world of lived experience, and foreclose the possibility of new experience;\n\nideologies are concerned with controlling and predicting the tide of history;\n\nideologies do not explain what is, they explain what becomes;\n\nideologies rely on logical procedures in thinking that are divorced from reality;\n\nideological thinking insists upon a \u2018truer reality\u2019, that is concealed behind the world of perceptible things.\n\nThe way we think about the world affects the relationships we have with others and ourselves. By injecting a secret meaning into every event and experience, ideological movements are forced to change reality in accordance with their claims once they come to power. And this means that one can no longer trust the reality of one\u2019s own lived experiences in the world. Instead, one is taught to distrust oneself and others, and to always rely upon the ideology of the movement, which must be right.\n\nBut in order to make individuals susceptible to ideology, you must first ruin their relationship to themselves and others by making them sceptical and cynical, so that they can no longer rely upon their own judgment:\n\nJust as terror, even in its pre-total, merely tyrannical form ruins all relationships between men, so the self-compulsion of ideological thinking ruins all relationship with reality. The preparation has succeeded when people have lost contact with their fellow men as well as the reality around them; for together with these contacts, men lose the capacity of both experience and thought. The ideal subject of totalitarian rule is not the convinced Nazi or the convinced Communist, but people for whom the distinction between fact and fiction (ie, the reality of experience) and the distinction between true and false (ie, the standards of thought) no longer exist.\n\nOrganised loneliness, bred from ideology, leads to tyrannical thought, and destroys a person\u2019s ability to distinguish between fact and fiction \u2013 to make judgments. In loneliness, one is unable to carry on a conversation with oneself, because one\u2019s ability to think is compromised. Ideological thinking turns us away from the world of lived experience, starves the imagination, denies plurality, and destroys the space between men that allows them to relate to one another in meaningful ways. And once ideological thinking has taken root, experience and reality no longer bear upon thinking. Instead, experience conforms to ideology in thinking. Which is why when Arendt talks about loneliness, she is not just talking about the affective experience of loneliness: she is talking about a way of thinking. Loneliness arises when thought is divorced from reality, when the common world has been replaced by the tyranny of coercive logical demands.\n\nWe think from experience, and when we no longer have new experiences in the world to think from, we lose the standards of thought that guide us in thinking about the world. And when one submits to the self-compulsion of ideological thinking, one surrenders one\u2019s inner freedom to think. It is this submission to the force of logical deduction that \u2018prepares each individual in his lonely isolation against all others\u2019 for tyranny. Free movement in thinking is replaced by the propulsive, singular current of ideological thought.\n\nIn one of her thinking journals, Arendt asks: \u2018Gibt es ein Denken das nicht Tyrannisches ist?\u2019 (Is there a way of thinking that is not tyrannical?) She follows the question with the statement that the point is to resist being swept up in the tide at all. What allows men to be carried away? Arendt argues that the underlying fear that attracts one to ideology is the fear of self-contradiction. This fear of self-contradiction is why thinking itself is dangerous \u2013 because thinking has the power to uproot all of our beliefs and opinions about the world. Thinking can unsettle our faith, our beliefs, our sense of self-knowledge. Thinking can strip away everything that we hold dear, rely upon, take for granted day-to-day. Thinking has the power to make us come undone.\n\nBut life is messy. Amid the chaos and uncertainty of human existence, we need a sense of place and meaning. We need roots. And ideologies, like the Sirens in Homer\u2019s Odyssey, appeal to us. But those who succumb to the siren song of ideological thinking, must turn away from the world of lived experience. In doing so, they can\u2019t confront themselves in thinking because, if they do, they risk undermining the ideological beliefs that have given them a sense of purpose and place. Put very simply: people who subscribe to ideology have thoughts, but they are incapable of thinking for themselves. And it is this inability to think, to keep one\u2019s self company, to make meaning from one\u2019s experiences in the world, that makes them lonely.\n\nShe was unable to find the private, self-reflective space necessary for thinking\n\nArendt\u2019s argument about loneliness and totalitarianism is not an easy one to swallow, because it implies a kind of ordinariness about totalitarian tendencies that appeal to loneliness: if you are not satisfied with reality, if you forsake the good and always demand something better, if you are unwilling to come face-to-face with the world as it is, then you will be susceptible to ideological thought. You will be susceptible to organised loneliness.\n\nWhen Arendt wrote to her husband: \u2018I simply can\u2019t be exposed to the public five times a week \u2013 in other words, never get out of the public eye. I feel as if I have to go around looking for myself,\u2019 she wasn\u2019t vainly complaining about the limelight. The constant exposure to a public audience made it impossible for her to keep company with herself. She was unable to find the private, self-reflective space necessary for thinking. She was unable to people her solitude.\n\nThis is one of the paradoxes of loneliness. Solitude requires being alone whereas loneliness is felt most sharply in the company of others. Just as much as we rely upon the public world of appearances for recognition, we need the private realm of solitude to be alone with ourselves and think. And this is what Arendt was stripped of when she lost the space to be alone with herself. \u2018What makes loneliness so unbearable,\u2019 she said \u2018is the loss of one\u2019s own self which can be realised in solitude \u2026\u2019\n\nIn solitude, one is able to keep oneself company, to engage in a conversation with oneself. In solitude, one doesn\u2019t lose contact with the world, because the world of experience is ever-present in our thinking. To quote Arendt, quoting Cicero: \u2018Never is a man more active than when he does nothing, never is he less alone than when he is by himself.\u2019 This is what ideological thinking and tyrannical thinking destroy \u2013 our ability to think with and for ourselves. This is the root of organised loneliness."
        },
        {
            "authors": [
                "Sasha Khomenko",
                "Institute For Global Health",
                "Isglobal",
                "Barcelona",
                "Department Of Experimental",
                "Health Sciences",
                "Universitat Pompeu Fabra",
                "Ciber Epidemiolog\u00eda Y Salud P\u00fablica",
                "Ciberesp",
                "Madrid"
            ],
            "title": "Premature mortality due to air pollution in European cities: a health impact assessment",
            "contents": "A considerable proportion of premature deaths in European cities could be avoided annually by lowering air pollution concentrations, particularly below WHO guidelines. The mortality burden varied considerably between European cities, indicating where policy actions are more urgently needed to reduce air pollution and achieve sustainable, liveable, and healthy communities. Current guidelines should be revised and air pollution concentrations should be reduced further to achieve greater protection of health in cities.\n\nCompliance with WHO air pollution guidelines could prevent 51 213 (95% CI 34 036\u201368 682) deaths per year for PM 2\u00b75 exposure and 900 (0\u20132476) deaths per year for NO 2 exposure. The reduction of air pollution to the lowest measured concentrations could prevent 124 729 (83 332\u2013166 535) deaths per year for PM 2\u00b75 exposure and 79 435 (0\u2013215 165) deaths per year for NO 2 exposure. A great variability in the preventable mortality burden was observed by city, ranging from 0 to 202 deaths per 100 000 population for PM 2\u00b75 and from 0 to 73 deaths for NO 2 per 100 000 population when the lowest measured concentrations were considered. The highest PM 2\u00b75 mortality burden was estimated for cities in the Po Valley (northern Italy), Poland, and Czech Republic. The highest NO 2 mortality burden was estimated for large cities and capital cities in western and southern Europe. Sensitivity analyses showed that the results were particularly sensitive to the choice of the exposure response function, but less so to the choice of baseline mortality values and exposure assessment method.\n\nWe did a quantitative health impact assessment for the year 2015 to estimate the effect of air pollution exposure (PM 2\u00b75 and NO 2 ) on natural-cause mortality for adult residents (aged \u226520 years) in 969 cities and 47 greater cities in Europe. We retrieved the cities and greater cities from the Urban Audit 2018 dataset and did the analysis at a 250 m grid cell level for 2015 data based on the global human settlement layer residential population. We estimated the annual premature mortality burden preventable if the WHO recommended values (ie, 10 \u03bcg/m 3 for PM 2\u00b75 and 40 \u03bcg/m 3 for NO 2 ) were achieved and if air pollution concentrations were reduced to the lowest values measured in 2015 in European cities (ie, 3\u00b77 \u03bcg/m 3 for PM 2\u00b75 and 3\u00b75 \u03bcg/m 3 for NO 2 ). We clustered and ranked the cities on the basis of population and age-standardised mortality burden associated with air pollution exposure. In addition, we did several uncertainty and sensitivity analyses to test the robustness of our estimates.\n\nAmbient air pollution is a major environmental cause of morbidity and mortality worldwide. Cities are generally hotspots for air pollution and disease. However, the exact extent of the health effects of air pollution at the city level is still largely unknown. We aimed to estimate the proportion of annual preventable deaths due to air pollution in almost 1000 cities in Europe.\n\nWe did a quantitative health impact assessment (HIA) to estimate the annual preventable premature mortality burden in 969 European cities and 47 greater cities in 31 European countries if WHO-recommended air pollution concentrations for PM 2\u00b75 and NO 2 were achieved. In addition, because of the association between air pollution and mortality at levels below WHO air quality guidelines, we evaluated the annual premature mortality burden preventable if feasibly lower levels of air pollution were attained. Our goal was to provide local mortality estimates for more targeted and health preserving urban and transport planning policies to promote sustainable, liveable, and healthy communities in European cities.\n\nMost of the estimates of the health effects of air pollution exposure are calculated on a global or country level.However, this level of analysis provides little indication of where actions are more urgently needed to reduce the adverse health outcomes associated with air pollution. There is a need for local estimates that are more relevant for targeted policy action, and cities could represent a more appropriate unit of analysis. Cities are home to 72% of the European populationand offer a good opportunity for policy change because of direct local accountability, better responsiveness than national governments, and faster actions than national governments.In addition, cities are often hotspots for air pollution and air pollution-related disease.In cities, motorised traffic is a major contributor to high outdoor levels of air pollution.In Europe, the contribution of traffic to urban PMconcentrations is estimated at an average of 14% of total urban PMconcentrations, going up to 39% for particular cities, and to NOconcentrations of 47%, reaching up to 70% for particular cities.In addition, local fuel combustion (eg, household heating, industrial combustion, and wood burning) also contributes to high PMconcentrations, with an average contribution to urban PMconcentrations of 13%, reaching up to 48% in several eastern European cities.\n\nEstimates and 25-year trends of the global burden of disease attributable to ambient air pollution: an analysis of data from the Global Burden of Diseases Study 2015.\n\nOur study estimated higher preventable mortality burdens for PM 2\u00b75 and NO 2 compared with previous EU-wide and country assessments. Additionally, we highlight local differences in the preventable mortality burden that have not been accounted for by previous national-level estimates. Our findings have great implications for policy implementations in cities, as we provide local administrations with comprehensive local estimates of the effects of air pollution on health, allowing for more targeted actions to reduce air pollution concentrations. Further research at the city level is needed to estimate the effects of distinct adverse environmental and lifestyle exposures prevalent in cities (eg, air pollution, noise, shortage of green spaces, heat, and sedentary behaviour) in Europe and globally.\n\nTo our knowledge, this study is the first to estimate the premature mortality burden due to air pollution in nearly 1000 cities in Europe. The main strengths of the study include the use of a fine resolution scale of 250 m, city-specific mortality rates, the inclusion of uncertainty, and a considerable number of sensitivity analyses and the inclusion of a wide range of European cities, particularly in eastern Europe where research is scarce. Our results indicate that a considerable proportion of premature deaths in European cities could be avoided annually by decreasing air pollution concentrations, particularly below WHO guidelines. Notably, we show that the preventable mortality burden varies greatly by city, reaching up to 15% for PM 2\u00b75 and 7% for NO 2 of annual premature mortality. In addition, our sensitivity analyses show that the results are particularly sensitive to the choice of the exposure response function, but less so to the choice of baseline mortality values and exposure assessment method.\n\nWe searched the PubMed and Google Scholar databases, without language or publication date restrictions, for estimates of the effects of air pollution exposure on health. Our search terms were: \u201cair pollution\u201d OR \u201cPM 2\u00b75 \u201d OR \u201cNO2\u201d or \u201cparticulate matter\u201d OR \u201cnitrogen dioxide\u201d AND \u201cmortality\u201d OR \u201cpremature mortality\u201d OR \u201chealth impact\u201d OR \u201crisk\u201d AND \u201ccity\u201d OR \u201ccities\u201d OR \u201cEurope\u201d. We included only health impact assessment and burden of disease studies specifically on air pollution up to the year 2019 and for the European region. We excluded any epidemiological studies from the search (ie, cohort studies, case-control studies, and cross-sectional studies). Previous studies, such as the Global Burden of Disease, Injuries, and Risk Factors Study 2017, have assessed the adverse health effects associated with air pollution exposure at the global and country level. However, this level of analysis provided little indication for cities where actions are more urgently needed to reduce air pollution and its adverse health outcomes. City-level estimates are needed for more targeted policy actions as cities offer a good opportunity for policy change because of direct local accountability, better responsiveness than national governments, and faster actions than national governments. In addition, cities are generally hotspots for air pollution and disease due to high level of motorised traffic and local fuel combustion. However, the exact extent of the effects of air pollution on health at the city level are still largely unknown. Previously, a few studies estimated the health effects of air pollution exposure for several selected cities in Europe, such as Barcelona (Spain), Vienna (Austria), and Bradford (UK). Only one large city-level study estimated the health effects of PM 2\u00b75 exposure in 250 major cities worldwide. Nevertheless, this study did not consider city-specific mortality rates and used cruder air pollution estimates at 10 km resolution scale.\n\nIn Europe, levels of air pollution are decreasing below the EU and WHO air quality guidelines.The EU directive sets the annual mean limits of ambient pollution at 25 \u03bcg/mfor PMand 40 \u03bcg/mfor NO, whereas the WHO recommendations are set at 10 \u03bcg/mfor PMand 40 \u03bcg/mfor NONevertheless, studies have reported associations between air pollution and mortality at concentrations below these guidelines, with no evidence of a safe exposure threshold.Reductions in air pollution below the values set by both guidelines are expected to offer a greater protection of population health, particularly for NO, for which the WHO guideline and EU limit have been shown to be inadequate for protecting health in previous studies.Accordingly, it was estimated that, in 2016, more than 400 000 deaths (equating to 7% of annual mortality) in Europe were attributable to PMexposure and more than 70 000 deaths (equating to 1% of annual mortality) were attributable to NOexposure. Moreover, these mortality estimates were when concentrations of air pollution were below the recommendations given in the EU and WHO guidelines.\n\nAmbient air pollution is a major environmental cause of morbidity and mortality worldwide. Long-term exposure to ambient particulate matter (PM) with diameter less than or equal to 2\u00b75 \u03bcm (PM) was estimated to cause between 4 and 9 million premature deaths in 2015 globally, ranking PMas the fifth greatest risk factor for global mortality in the Global Burden of Disease, Injuries, and Risk Factors Study (GBD) 2015.\n\nEstimates and 25-year trends of the global burden of disease attributable to ambient air pollution: an analysis of data from the Global Burden of Diseases Study 2015.\n\nThe funder of the study had no role in study design, data collection, data analysis, data interpretation, or writing of the report. All authors had full access to all the data in the study and had final responsibility for the decision to submit for publication.\n\nFor city comparisons, we first did a cluster analysis to identify clusters of cities with similar preventable mortality patterns ( appendix pp 65\u201367 ). Cluster analysis was chosen over a simple city ranking because of the sensitivity of the cities' position in the ranking to the air pollution model data, air pollution reduction scenario, and the mortality variables chosen to score the cities (ie, different rankings are obtained depending on the variable chosen to score the cities). To avoid the repetition of cities that overlapped with the greater cities, we kept the greater cities and excluded the smaller size cities that corresponded to them. Overall, the clustering was done for 811 cities and 47 greater cities. We used the K-means clustering algorithmand established the optimal number of clusters at five clusters for PMand at four clusters for NO appendix p 66 ). The clusters were ordered from highest to lowest mortality burden ( table 1 ). To compare the cities within each cluster, we ranked the cities according to a mortality burden score, which was calculated through a principal component analysis on the mortality variables associated with air pollution exposure ( appendix pp 72, 106\u201357 ).\n\nWe did sensitivity analyses to evaluate the effect of changes in input model variables on the magnitude of our final mortality estimations ( appendix pp 36\u201364 ). We tested the effects of using distinct ERFs ( appendix p 36 ),distinct air pollution estimates, country-level mortality rates (instead of city-level mortality rates), average city-level population weighted air pollution concentrations (instead of 250 m grid cell values), and finally, European Environment Agency (EEA) HIA model assumptions ( appendix pp 36\u201364 ).\n\nWe did uncertainty analyses for 15 selected cities to evaluate the effect on the CIs of our estimates of the uncertainty distributions of the parameters included in the quantitative HIA analysis (ie, city-specific mortality, city population age structures, air pollution models, and ERFs [appendix pp 28\u201335]). We then constructed uncertainty distributions for these variables and obtained point estimates and CIs in our final estimations using Monte Carlo simulations ( appendix pp 28\u201329 ). We did the first round of Monte Carlo sampling while considering the uncertainty in all four variables. Afterwards, we did a new round of Monte Carlo sampling considering the uncertainty in only one of the four variables simultaneously.\n\nWe used three air pollution models to estimate baseline annual mean PMand NOconcentrations at 250 m grid cell level for the year 2015. For 802 cities and 46 greater cities, annual mean PMand NOestimates were retrieved from land use regression (LUR) models developed on a 100 m by 100 m grid cell scale for 2010 as part of the Effects of Low-Level Air Pollution: a Study in Europe (ELAPSE) project ( appendix pp 11\u201316 ).For 167 cities and one greater city for which the ELAPSE model was unavailable, the annual mean PMvalues were extracted from the Ensemble model developed on a 10 km by 10 km scale for the year 2015.Annual mean NOestimates were retrieved from the global LUR model for NOdeveloped on a 100 m by 100 m grid cell scale for the year 2011 ( appendix pp 17\u201320 ).The modelled values were contrasted with measured time series air pollution data for the year 2015 from the European air quality database (AirBase)and temporal adjustments were done when appropriate ( appendix pp 11\u201320 ). Model comparisons showed high correlations between the three models (r=0\u00b794 for PMand r=0\u00b781 for NO); however, the ELAPSE estimates were higher overall by 4\u20135% ( appendix p 20 ).\n\nCity-specific all-cause mortality counts for 2015 were available through Eurostat.City-specific mortality, rather than country-specific mortality, was chosen because of the difference and considerable variability (ie, with a median variance of \u00b122% and an IQR of 9\u201332%) in the city-specific mortality ( appendix p 6 ). Overall, 127 cities and 15 greater cities had missing mortality counts. In these instances, all-cause mortality counts were estimated with the corresponding Nomenclature of Territorial Units for Statistics (NUTS) 3 (n=131), NUTS2 (n=1), or country-level (n=10) all-cause age-specific mortality rates ( appendix p 4 ).To calculate the number of natural-cause deaths by age group for each city and greater city we retrieved NUTS3-level mortality counts by age group, NUTS2, and country-level mortality counts by age and cause of death.We calculated the external deaths fractions (defined by the International Classification of Diseases 10 mortality codes V01\u2013Y89) by age group and estimated the proportion of deaths by natural causes by age group at the NUTS3-level. These proportions were applied to the corresponding city-level total all-cause mortality counts ( appendix p 4 ).\n\nWe did a quantitative HIA at 250 m by 250 m grid cell level for 2015, based on the global human settlement layer (GHSL) residential population ( figure 1B appendix pp 2\u20134 ).The analysis estimated the effect of air pollution exposure (PMand NO) on natural-cause mortality for adult residents who were aged 20 years or older from the 969 cities and 47 greater cities. We followed the methods used for the Urban and Transport Planning Health Impact Assessment, which are based on the comparative risk assessment approach.We retrieved exposure response functions (ERFs) from several studies quantifying the strength of association between air pollution exposure and mortality ( appendix p 10 ). We set as counterfactual scenarios the WHO recommended values (ie, 10 \u03bcg/mfor PMand 40 \u03bcg/mfor NO) and the lowest measured values among the European cities in 2015 (ie, 3\u00b77 \u03bcg/mfor PMand 3\u00b75 \u03bcg/mfor NO).The steps were as follows: (1) we estimated the baseline levels of air pollution exposure for 2015; (2) we determined the difference in level exposure between the 2015 levels and counterfactual levels; (3) we used the ERFs to compute the relative risk associated to the exposure difference and; (4) we calculated the population attributable fraction for each exposure difference. We obtained point estimates and CIs in our final estimations by propagating the uncertainties in the ERFs using Monte Carlo simulations ( appendix pp 8\u20139 ). We added up the results by city and greater city and calculated the preventable age-standardised mortality per 100 000 population on the basis of the European standard populationand the percentage of annual preventable premature deaths for PMand NO. To complement the premature mortality estimates, we calculated the years of life lost due to the premature deaths ( appendix p 9 ). Given that the best available meta-analyses are based on single-pollutant models and because PMand NOare generally spatially correlated, the deaths associated with PMand NOexposures were not added up but instead were considered independently.The analysis was done in R-3.5.1, Python (version 3.7), and PostGIS (version 2.4).\n\nWe retrieved the European cities for the HIA from the Urban Audit 2018 dataset ( appendix p 2 ).This dataset contained 980 cities and 49 greater cities in 31 European countries. The 49 greater cities covered 160 cities either by representing a city of larger area than the defined city or by constituting a combination of several cities. We excluded Saint Denis (R\u00e9union) and Fort-de-France (Martinique) because of their location out of the European study area. Nine cities and two greater cities located in Madeira (Portugal), the Azores (Portugal), and the Canary Islands (Spain) were excluded because no air pollution estimates were available. The analysis was done for the remaining 969 cities and 47 greater cities ( figure 1A ).\n\nLocation and population size of the 969 cities and 47 greater cities defined in the Urban Audit 2018 and included in the health impact assessment analysis (A). An example of the Global Human Settlement Layer residential population grid is shown for Barcelona (Spain; B)\n\nFigure 1 Location and population size of the 969 cities and 47 greater cities defined in the Urban Audit 2018 and included in the health impact assessment analysis (A). An example of the Global Human Settlement Layer residential population grid is shown for Barcelona (Spain; B)\n\nGenerally, the cities with the lowest mortality burden due to air pollution were located in northern Europe. The cities with the lowest PMmortality burden (ie, those grouped in cluster 5) were: (1) Reykjav\u00edk (Iceland), (2) Troms\u00f8 (Norway), (3) Ume\u00e5 (Sweden), (4) Oulu (Finland), (5) Jyv\u00e4skyl\u00e4 (Finland), (6) Uppsala (Sweden), (7) Trondheim (Norway), (8) Lahti (Finland), (9) \u00d6rebro (Sweden), and (10) Tampere (Finland; figure 3A appendix pp 106\u201331 ). Finally, the cities with the lowest NOmortality burden (ie, cluster 4) were: (1) Troms\u00f8 (Norway), (2) Ume\u00e5 (Sweden), (3) Oulu (Finland), (4) Kristiansand (Norway), (5) Pula (Croatia), (6) Link\u00f6ping (Sweden), (7) Galway (Ireland), (8) J\u00f6nk\u00f6ping (Sweden), (9) Alytus (Lithuania), and (10) Trondheim (Norway; figure 3B appendix pp 132\u201357 ).\n\nFor PM, cluster 1 contained 38 cities with the highest mortality burden, including cities in the Po Valley (northern Italy), southern Poland, and eastern Czech Republic ( table 1 appendix pp 106\u201331 ). Among these cities, the top ten cities with the highest burden were: (1) Brescia (Italy), (2) Bergamo (Italy), (3) Karvin\u00e1 (Czech Republic), (4) Vicenza (Italy), (5) G\u00f3rno\u015bl\u0105sko-Zag\u0142\u0119biowska Metropolia (Poland), (6) Ostrava (Czech Republic), (7) Jastrz\u0119bie-Zdr\u00f3j (Poland), (8) Saronno (Italy), (9) Rybnik (Poland), and (10) Havirov (Czech Republic; Table 2 Table 3 ). For NO, cluster 1 included six cities with the highest mortality burden due to exceedances in the WHO guideline and cluster 2 included 32 cities with the highest mortality burden due to exposures below the WHO recommendation. These clusters generally included large cities and capital cities in western and southern Europe, as well as smaller cities located in their vicinity ( figure 3B appendix pp 132\u201357 ). Among these cities, the top ten cities with the highest burden were: (1) Madrid (Spain), (2) Antwerp (Belgium), (3) Turin (Italy), (4) Paris (France), (5) Milan (Italy), (6) Barcelona (Spain), (7) Mollet del Vall\u00e8s (Spain), (8) Brussels (Belgium), (9) Herne (Germany), and (10) Argenteuil\u2013Bezons (France; Table 4 Table 5 ).\n\nPercentage of preventable annual mortality and YLL per 100 000 population in the ten European cities with the highest (top) and lowest (bottom) NO 2 mortality burden\n\nTable 5 Percentage of preventable annual mortality and YLL per 100 000 population in the ten European cities with the highest (top) and lowest (bottom) NO 2 mortality burden\n\nThe sensitivity analyses indicated greatest changes in our final estimations upon changes in the ERFs. For PM, the use of the European Study of Cohorts for Air Pollution Effects ERF and the global exposure mortality model led to almost a doubling in the estimated preventable mortality burden ( appendix pp 36\u201343 ). The use of ensemble PMor NOglobal LUR models resulted in lower estimated preventable mortality burdens ( appendix pp 44\u201347 ). The use of single 2015 and 2018 AirBase measurements or city-level averages led to greatest reductions in the estimated preventable mortality burden for NOin the WHO scenario ( appendix pp 48\u201353, 58\u201361 ). The adoption of EEA HIA model assumptions led to an increase in the estimated preventable mortality burden for PMand to a decrease for NO appendix pp 62\u201364 ). Finally, the use of country-level mortality rates led to slightly higher estimated preventable mortality burdens for PMand NO appendix pp 54\u201357 ). In addition, the use of country-level mortality rates led to changes in the ranking position for the studied cities, particularly those with more central positions ( appendix p 72 ).\n\nFor PM, the uncertainty analysis indicated that the primary source of uncertainty was the variability in the age structure of the city populations, followed by the city-specific mortality, the ERF, and the PMmodel data ( appendix pp 29\u201331 ). For NO, the uncertainty analysis indicated that the main source of uncertainty was the uncertainty in the ERF, followed by the age structure of the city populations, city-specific mortality rates, and ultimately, the NOmodel data ( appendix pp 32\u201335 ). Notably, for cities with low PMand NOconcentrations, the analysis showed that not accounting for the uncertainty in the air pollution model estimates can lead to underestimations in the final outcome ( appendix pp 30\u201333 ).\n\nA great variability in the preventable mortality burden was observed by city. At the city level, compliance with WHO air pollution guidelines could prevent between 0 and 138 deaths per 100 000 population (ie, 0\u201311% of annual mortality), with an average of 26 (95% CI 18\u201336) deaths per 100 000 population for PMexposure, and between 0 and 5 deaths per 100 000 population (ie, 0\u20131% of annual mortality), with an average of 0\u00b71 (0\u20130\u00b73) deaths per 100 000 population for NOexposure. The reduction of air pollution to the lowest measured concentrations could prevent between 0 and 202 deaths per 100 000 population (ie, 0 to 15% of annual mortality), with an average of 68 (48\u201393) deaths per 100 000 population for PMexposure, and between 0 and 73 deaths per 100 000 population (ie, 0 to 7% of annual mortality), with an average of 37 (0\u2013107) deaths per 100 000 population for NOexposure ( figure 2 ). The correlation between the estimated preventable mortality burden associated with PMexposure and NOexposure by city and greater city varied between r=0\u00b736 and r=0\u00b756, depending on the outcome measure ( appendix p 27 ).\n\nHistograms showing the variability in the estimated preventable mortality burden associated with PM 2\u00b75 (A) and NO 2 (B) exposures by city and greater city. The mortality parameters are shown for the WHO and the lowest levels air pollution reduction scenarios\n\nFigure 2 Histograms showing the variability in the estimated preventable mortality burden associated with PM 2\u00b75 (A) and NO 2 (B) exposures by city and greater city. The mortality parameters are shown for the WHO and the lowest levels air pollution reduction scenarios\n\nAcross all cities, 84% of the population were exposed to PM 2\u00b75 concentrations above the WHO guideline and 9% of the population were exposed to NO 2 concentrations above the WHO guideline. Compliance with the WHO air pollution guideline could prevent 51 213 (95% CI 34 036\u201368 682) annual premature deaths (ie, 2% [1\u20133%] of annual mortality) for PM 2\u00b75 exposure and 900 (0\u20132476) annual premature deaths (ie, 0\u00b704% [0\u20130\u00b71] of annual mortality) for NO 2 exposure. The reduction of air pollution to the lowest measured concentrations could prevent 124 729 (83 332\u2013166 535) annual premature deaths (ie, 6% [4\u20137] of annual mortality) for PM 2\u00b75 exposure and 79 435 (0\u2013215 165) annual premature deaths (ie, 4% [0\u201310] of annual mortality) for NO 2 exposure. The estimated preventable mortality burden showed highest positive correlation with PM 2\u00b75 and NO 2 concentrations (ie, r\u22480\u00b79 [the r value is approximate because it is based on the correlations of various outcome measures, such as mortality rates and percentage of mortality and years of life lost for the WHO scenario and lowest concentrations scenarios, with air pollution concentrations and baseline mortality]), and a moderate positive correlation with city-level mortality (ie, r\u22480\u00b74).\n\nCity population counts ranged from 8307 (Suwalki, Poland) to 8 542 705 (London, UK), with a median population size of 127 002 inhabitants ( appendix pp 75\u2013105 ). In total, 168 180 047 adults aged 20 years or older resided in the 969 European cities and 47 greater cities, representing 32% of the population in the 31 European countries. Overall, at the 250 m grid cell level, PMconcentrations ranged from 0\u00b77 \u03bcg/mto 30\u00b78 \u03bcg/m, with a median value of 12\u00b73 \u03bcg/m. NOconcentrations, at the 250 m grid cell level, varied between 0\u00b77 \u03bcg/mand 84\u00b75 \u03bcg/m, with a median value of 20\u00b77 \u03bcg/m appendix pp 106\u201357 ). The correlation between both air pollutants was r=0\u00b750.\n\nDiscussion\n\n1 Cohen AJ\n\nBrauer M\n\nBurnett R\n\net al. Estimates and 25-year trends of the global burden of disease attributable to ambient air pollution: an analysis of data from the Global Burden of Diseases Study 2015. , 2 Burnett R\n\nChen H\n\nSzyszkowicz M\n\net al. Global estimates of mortality associated with long-term exposure to outdoor fine particulate matter. , 3 European Environment Agency\n\nAir quality in Europe\u20142019 report. 2\u00b75 and 7% for NO 2 of annual premature mortality in the cities with the highest pollution concentrations. In addition, we show that our results are mainly sensitive to the choice of the ERF, but less so to the choice of baseline mortality values and exposure assessment method. To our knowledge, this is the first study to estimate the premature mortality burden due to air pollution at a city level in Europe. Previously, the effects of air pollution exposure on health have been mainly evaluated on a global or country level.Our results indicate that a considerable proportion of premature deaths in European cities could be avoided annually by lowering air pollution levels, particularly below WHO guidelines. Notably, we show that the mortality burden varies considerably between European cities, reaching up to 15% for PMand 7% for NOof annual premature mortality in the cities with the highest pollution concentrations. In addition, we show that our results are mainly sensitive to the choice of the ERF, but less so to the choice of baseline mortality values and exposure assessment method.\n\n2\u00b75 in the cities included in our study was higher than the EU region. The EEA estimated an average 74 deaths per 100 000 population among the EU28 countries for the year 2017, equating to 7% of annual premature mortality. 3 European Environment Agency\n\nAir quality in Europe\u20142019 report. 2\u00b75 mortality burden in urban areas. In addition, the preventable mortality burden varied considerably by country and city within each country. We estimated the highest PM 2\u00b75 mortality burden for cities in Italy, Czech Republic, Poland, Greece, Hungary, Slovakia, Slovenia, Croatia, Bulgaria, Romania, and Malta. We thus identified similar regions to the EEA as the ones bearing the highest burden (ie, primarily cities within eastern Europe). 3 European Environment Agency\n\nAir quality in Europe\u20142019 report. 2\u00b75 exposure. 3 European Environment Agency\n\nAir quality in Europe\u20142019 report. Compared with previous studies, the estimated average preventable mortality for PMin the cities included in our study was higher than the EU region. The EEA estimated an average 74 deaths per 100 000 population among the EU28 countries for the year 2017, equating to 7% of annual premature mortality.Our sensitivity analyses with the EEA model's assumptions resulted in an average of 99 deaths per 100 000 population, equating to 8% of annual premature mortality, showing a higher PMmortality burden in urban areas. In addition, the preventable mortality burden varied considerably by country and city within each country. We estimated the highest PMmortality burden for cities in Italy, Czech Republic, Poland, Greece, Hungary, Slovakia, Slovenia, Croatia, Bulgaria, Romania, and Malta. We thus identified similar regions to the EEA as the ones bearing the highest burden (ie, primarily cities within eastern Europe).However, we additionally highlight local differences in the mortality burden that are not accounted for by national-level estimates. For instance, we estimated the highest mortality burden for cities in northern Italy, although country-level estimates did not place Italy among the countries with the highest mortality burden due to PMexposure.\n\n2\u00b75 . 1 Cohen AJ\n\nBrauer M\n\nBurnett R\n\net al. Estimates and 25-year trends of the global burden of disease attributable to ambient air pollution: an analysis of data from the Global Burden of Diseases Study 2015. 37 Institute for Health Metrics and Evaluation\n\nGBD compare. 34 Burnett R\n\nChen H\n\nSzyszkowicz M\n\net al. Global estimates of mortality associated with long-term exposure to outdoor fine particulate matter. 38 Korhonen A\n\nLehtom\u00e4ki H\n\nRumrich I\n\net al. Influence of spatial resolution on population PM 2\u00b75 exposure and health impacts. 39 Anenberg SC\n\nAchakulwisut P\n\nBrauer M\n\nMoran D\n\nApte JS\n\nHenze DK Particulate matter-attributable mortality and relationships with carbon dioxide in 250 urban areas worldwide. 2\u00b75 (ie, 13\u2013125 deaths vs 0\u2013202 deaths per 100 000 population), even though they included cities with higher annual mean PM 2\u00b75 concentrations than those reported in Europe. Similarly, the GBD study also identified mainly eastern European countries as those bearing the highest mortality burden due to PMNevertheless, GBD country-level mortality estimates were overall lower than our results (eg, 45 vs 92 deaths per 100 000 population for Italy and 57 vs 114 deaths per 100 000 population for Poland),probably because we accounted for all natural-cause deaths instead of six specific causes of mortality as done in GBD and used a higher resolution for the exposure assessment (ie, 250 m vs 10 km scale). Because of the absence of city-level cause-specific mortality data, we could not assess the effect of GBD model assumptions on our results. Nevertheless, Burnett and colleaguesshowed that using the global exposure mortality mode, which accounts for all non-communicable causes of deaths, leads to a more than doubling in the health burden estimates compared with the integrated exposure response used by GBD. In addition, lower resolutions tend to underestimate exposure by averaging concentrations from high and low exposure areas, thus leading to underestimations in mortality burden.Accordingly, the large city-level study by Anenberg and colleagues,which followed the GBD approach, using country-specific mortality data and cruder air pollution estimates (ie, at 10 km resolution) also estimated a smaller mortality burden range due to PM(ie, 13\u2013125 deaths vs 0\u2013202 deaths per 100 000 population), even though they included cities with higher annual mean PMconcentrations than those reported in Europe.\n\n2 , the EEA estimated an average of 13 deaths per 100 000 population among the EU28 countries, corresponding to 1% of annual premature mortality. 3 European Environment Agency\n\nAir quality in Europe\u20142019 report. 2 in urban areas (ie, 26 deaths per 100 000 population, equating to 2% of annual premature mortality), possibly due to a higher resolution (ie, 250 m vs 1 km scale) and higher NO 2 concentrations in urban areas. Reductions below the EEA counterfactual scenario of 20 \u03bcg/m3 resulted in an even larger preventable mortality burden of 37 deaths per 100 000 population. Given the evidence, there is no basis to assume that there is no risk of mortality below 20 \u03bcg/m3. 7 Cesaroni G\n\nBadaloni C\n\nGariazzo C\n\net al. Long-term exposure to urban air pollution and mortality in a cohort of more than a million adults in Rome. , 8 Fischer PH\n\nMarra M\n\nAmeling CB\n\net al. Air pollution and mortality in seven million adults: the Dutch Environmental Longitudinal Study (DUELS). 2 exposure. In addition, a fine exposure resolution is relevant for a pollutant with strong local source influences, such as NO 2 . 25 WHO\n\nWHO expert meeting: methods and tools for assessing the health risks of air pollution at local, national and international level. Meeting report. 2 has larger small-scale spatial contrast with higher concentrations near sources (eg, major roads) than PM 2\u00b75 . 25 WHO\n\nWHO expert meeting: methods and tools for assessing the health risks of air pollution at local, national and international level. Meeting report. 3 European Environment Agency\n\nAir quality in Europe\u20142019 report. As for NO, the EEA estimated an average of 13 deaths per 100 000 population among the EU28 countries, corresponding to 1% of annual premature mortality.In our sensitivity analyses with EEA model assumptions we estimated a higher preventable mortality burden for NOin urban areas (ie, 26 deaths per 100 000 population, equating to 2% of annual premature mortality), possibly due to a higher resolution (ie, 250 m vs 1 km scale) and higher NOconcentrations in urban areas. Reductions below the EEA counterfactual scenario of 20 \u03bcg/mresulted in an even larger preventable mortality burden of 37 deaths per 100 000 population. Given the evidence, there is no basis to assume that there is no risk of mortality below 20 \u03bcg/mThus, testing the effects of air pollution reductions to the lowest measured concentrations provides a more comprehensive overview of the mortality burden associated with NOexposure. In addition, a fine exposure resolution is relevant for a pollutant with strong local source influences, such as NONOhas larger small-scale spatial contrast with higher concentrations near sources (eg, major roads) than PMThus, by using a fine resolution, we were able to account for the extent and local variability in the mortality burden that is not accounted for by lower resolutions.\n\n2\u00b75 and NO 2 . For PM 2\u00b75 , we estimated the highest mortality burden for cities in northern Italy, southern Poland, and eastern Czech Republic. Ambient PM 2\u00b75 originates from diverse sources, including fossil fuel combustion and biomass burning. 25 WHO\n\nWHO expert meeting: methods and tools for assessing the health risks of air pollution at local, national and international level. Meeting report. 2\u00b75 are traffic (ie, on average by 14% and up to 39% of all contributors of PM 2\u00b75 ), domestic fuel burning (ie, on average by 13% and up to 48%), and industrial activities (ie, on average by 20% and up to 47%). 14 Thunis P\n\nDegraeuwe B\n\nPisoni E\n\net al. PM 2\u00b75 source allocation in European cities: a SHERPA modelling study. 2\u00b75 concentration is estimated at 26% of all potential spatial source contributors (eg, regional, national, and transboundary pollution sources), stressing the importance of not only local source contributions but also regional and national source contributions. 14 Thunis P\n\nDegraeuwe B\n\nPisoni E\n\net al. PM 2\u00b75 source allocation in European cities: a SHERPA modelling study. 2\u00b75 concentrations in the region. 40 Bigi A\n\nGhermandi G Trends and variability of atmospheric PM 2\u00b75 and PM 10\u20132\u00b75 concentration in the Po Valley, Italy. 2\u00b75 concentrations. 41 J\u0119druszkiewicz J\n\nCzernecki B\n\nMarosz M The variability of PM 10 and PM 2\u00b75 concentrations in selected Polish agglomerations: the role of meteorological conditions, 2006\u20132016. 2\u00b75 for cities located in the Po Valley, southern Poland, and eastern Czech Republic is thus consistent with the higher degree of anthropogenic emissions and unfavourable climatic conditions in these areas. We estimated the highest preventable mortality burden for cities that had the highest air pollution concentrations, consistent with the highest positive correlation found between air pollution concentrations and the estimated preventable mortality burdens for PMand NO. For PM, we estimated the highest mortality burden for cities in northern Italy, southern Poland, and eastern Czech Republic. Ambient PMoriginates from diverse sources, including fossil fuel combustion and biomass burning.In European cities, the main contributors to PMare traffic (ie, on average by 14% and up to 39% of all contributors of PM), domestic fuel burning (ie, on average by 13% and up to 48%), and industrial activities (ie, on average by 20% and up to 47%).In addition, the average city contribution to PMconcentration is estimated at 26% of all potential spatial source contributors (eg, regional, national, and transboundary pollution sources), stressing the importance of not only local source contributions but also regional and national source contributions.In northern Italy, the Po Valley is a highly urbanised area characterised by high emissions from traffic and industries and frequently stagnant meteorological conditions related to the valley, leading to increased PMconcentrations in the region.The southern Poland and eastern Czech Republic regions are characterised by coal mining industry, and domestic coal burning is frequent throughout the winter for heat production, contributing to high PMconcentrations.The high mortality burden due to PMfor cities located in the Po Valley, southern Poland, and eastern Czech Republic is thus consistent with the higher degree of anthropogenic emissions and unfavourable climatic conditions in these areas.\n\n2 , we estimated the highest mortality burden for large cities and capital cities in western and southern Europe. NO 2 is an important surrogate for traffic emissions. 25 WHO\n\nWHO expert meeting: methods and tools for assessing the health risks of air pollution at local, national and international level. Meeting report. 2 concentrations in European cities has an average of 47% and goes up to 70% of total NO 2 . 13 Degraeuwe B\n\nPisoni E\n\nPeduzzi E\n\net al. Urban NO 2 atlas. 2 concentrations are highly dependent on city design, traffic density, and vehicle fleet (ie, the type of vehicles driven). Generally, densely populated cities with high traffic volumes tend to have high NO 2 concentrations. 42 Beelen R\n\nHoek G\n\nVienneau D\n\net al. Development of NO 2 and NOx land use regression models for estimating air pollution exposure in 36 study areas in Europe\u2014the ESCAPE project. 2 was the highest for the cities that were highly populated and capital cities, such as Paris, Madrid, Barcelona, Milan, Brussels, and Antwerp, as well as for smaller size cities located in their vicinity with potentially increased car use for commuting from smaller to bigger size cities. For NO, we estimated the highest mortality burden for large cities and capital cities in western and southern Europe. NOis an important surrogate for traffic emissions.The contribution of road transport to NOconcentrations in European cities has an average of 47% and goes up to 70% of total NONOconcentrations are highly dependent on city design, traffic density, and vehicle fleet (ie, the type of vehicles driven). Generally, densely populated cities with high traffic volumes tend to have high NOconcentrations.Accordingly, the mortality burden due to NOwas the highest for the cities that were highly populated and capital cities, such as Paris, Madrid, Barcelona, Milan, Brussels, and Antwerp, as well as for smaller size cities located in their vicinity with potentially increased car use for commuting from smaller to bigger size cities.\n\nIn addition, although air pollution concentrations had the highest correlation with the estimated preventable mortality burden for PM 2\u00b75 and NO 2 , the baseline mortality and age structure of the city populations should also be considered as relevant variables influencing the preventable mortality burden estimates. We found a modest positive correlation between the estimated preventable mortality burden and increasing baseline mortality rates. In addition, our uncertainty analyses showed that the main sources of uncertainty were the variability in underlying city-level age structures and mortality rates. Thus, changes in city age structures and baseline mortality are likely to have an effect on adverse health burden estimations.\n\n43 Nieuwenhuijsen M\n\nKhreis H\n\nVerlinghieri E\n\nMueller N\n\nRojas-Rueda D The role of health impact assessment for shaping policies and making cities healthier. 18 Mueller N\n\nRojas-Rueda D\n\nBasaga\u00f1a X\n\net al. Urban and transport planning related exposures and mortality: a health impact assessment for cities. , 19 Mueller N\n\nRojas-Rueda D\n\nKhreis H\n\net al. Socioeconomic inequalities in urban and transport planning related exposures and mortality: a health impact assessment study for Bradford, UK. , 21 Khomenko S\n\nNieuwenhuijsen M\n\nAmbr\u00f2s A\n\nWegener S\n\nMueller N Is a liveable city a healthy city? Health impacts of urban and transport planning in Vienna, Austria. The main strengths of our study include the use of a fine spatial scale of 250 m, the use of city-specific mortality rates, the inclusion of uncertainty, and a considerable number of sensitivity analyses and the inclusion of a large proportion of eastern European cities, for which there has been little research. HIA studies at the city level promote targeted evidence-based policies for healthy urban environmentsand their advantages have been described elsewhere.We used the best available and most recent data for all cities and we believe that our estimates provide a robust indication of the magnitude and variability of the adverse health effects associated with air pollution exposure among European cities.\n\n2\u00b75 and NO 2 exposure concentrations; thus, slight overestimation in our results is possible compared with more recent years. In addition, although we did uncertainty analyses for selected cities, we did not propagate all sources of error to our final point estimates and CIs for all cities. As a result, the uncertainty in our results is somewhat underestimated. Furthermore, we used three distinct air pollution models and used temporal adjustments to estimate air pollution concentrations. Although all three models were contrasted with the air pollution concentrations from 2015 and adjusted accordingly when appropriate, model comparisons indicated that ELAPSE estimates were overall 4\u20135% higher than the ensemble and global LUR model estimates. The ensemble model had a considerably lower spatial scale (ie, 10 km vs 250 m), which potentially led to the underestimation of PM 2\u00b75 concentrations, 38 Korhonen A\n\nLehtom\u00e4ki H\n\nRumrich I\n\net al. Influence of spatial resolution on population PM 2\u00b75 exposure and health impacts. 31 Larkin A\n\nGeddes JA\n\nMartin RV\n\net al. Global land use regression model for nitrogen dioxide air pollution. 2 concentrations and spatial variability. 42 Beelen R\n\nHoek G\n\nVienneau D\n\net al. Development of NO 2 and NOx land use regression models for estimating air pollution exposure in 36 study areas in Europe\u2014the ESCAPE project. 2\u00b75 and NO 2 with the ensemble and global LUR models than the ELAPSE models. Given these methodological issues, direct city-to-city comparisons should be done with caution. As shown in the sensitivity analyses, it is likely that the mortality burden due to air pollution exposure, although indicative of the extent, was underestimated for the cities for which the ensemble and global LUR models were used. For city comparisons, city clusters should be considered first. Variation within the proposed city ranking is plausible upon the use of more homogeneous air pollution data. However, we found robust agreement in the cities' classification by cluster upon the use of distinct air pollution models, indicating the higher reliability of cities' categorisation by clusters ( Nevertheless, our study also has several limitations. Because of limitations in the availability of data, we could not assess air pollution exposure for a more recent year than 2015. Our sensitivity analyses with 2018 Airbase data suggested a slight decrease from 2015 to 2018 in the mortality burden associated with changes in PMand NOexposure concentrations; thus, slight overestimation in our results is possible compared with more recent years. In addition, although we did uncertainty analyses for selected cities, we did not propagate all sources of error to our final point estimates and CIs for all cities. As a result, the uncertainty in our results is somewhat underestimated. Furthermore, we used three distinct air pollution models and used temporal adjustments to estimate air pollution concentrations. Although all three models were contrasted with the air pollution concentrations from 2015 and adjusted accordingly when appropriate, model comparisons indicated that ELAPSE estimates were overall 4\u20135% higher than the ensemble and global LUR model estimates. The ensemble model had a considerably lower spatial scale (ie, 10 km vs 250 m), which potentially led to the underestimation of PMconcentrations,whereas the global LUR model was constructed only using global predictors (eg, distance to major roads),thus, not accounting for local predictors within the European region (eg, local traffic density and land use) that could better describe NOconcentrations and spatial variability.Accordingly, our sensitivity analyses showed a lower estimated preventable mortality burden for PMand NOwith the ensemble and global LUR models than the ELAPSE models. Given these methodological issues, direct city-to-city comparisons should be done with caution. As shown in the sensitivity analyses, it is likely that the mortality burden due to air pollution exposure, although indicative of the extent, was underestimated for the cities for which the ensemble and global LUR models were used. For city comparisons, city clusters should be considered first. Variation within the proposed city ranking is plausible upon the use of more homogeneous air pollution data. However, we found robust agreement in the cities' classification by cluster upon the use of distinct air pollution models, indicating the higher reliability of cities' categorisation by clusters ( appendix pp 68\u201369 ).\n\n17 European Commission\n\nGlobal human settlement. 2 exposure, for which resolving small-scale spatial contrasts is relevant because of local source contributions. Despite these results, the continued use of a fine scale resolution for city-level HIAs is important, particularly when mortality and age-structure data become available at a finer scale and can be incorporated into the HIA models for more refined results. In addition, the use of city-level mortality rates adds an additional level of precision in relation to previous HIAs done on a large scale, for which country-level mortality rates were generally used. 1 Cohen AJ\n\nBrauer M\n\nBurnett R\n\net al. Estimates and 25-year trends of the global burden of disease attributable to ambient air pollution: an analysis of data from the Global Burden of Diseases Study 2015. , 3 European Environment Agency\n\nAir quality in Europe\u20142019 report. , 39 Anenberg SC\n\nAchakulwisut P\n\nBrauer M\n\nMoran D\n\nApte JS\n\nHenze DK Particulate matter-attributable mortality and relationships with carbon dioxide in 250 urban areas worldwide. , 44 Stanaway JD\n\nAfshin A\n\nGakidou E\n\net al. Global, regional, and national comparative risk assessment of 84 behavioural, environmental and occupational, and metabolic risks or clusters of risks for 195 countries and territories, 1990\u20132017: a systematic analysis for the Global Burden of Disease Study 2017. Moreover, we could not account for mortality and age structure variability within cities. Although population counts were available through the GHSL at 250 m resolution,no such layer was available for death counts or age groups. Accordingly, our sensitivity analyses with city-level population weighted air pollution concentrations led to almost identical estimations as those using the 250 m grid cells, with the exception of the preventable mortality burden associated with NOexposure, for which resolving small-scale spatial contrasts is relevant because of local source contributions. Despite these results, the continued use of a fine scale resolution for city-level HIAs is important, particularly when mortality and age-structure data become available at a finer scale and can be incorporated into the HIA models for more refined results. In addition, the use of city-level mortality rates adds an additional level of precision in relation to previous HIAs done on a large scale, for which country-level mortality rates were generally used.Our results showed considerable variability between city-level and country-level mortality rates, as well as an effect of the use of country-level mortality rates instead of city-level ones on health burden estimations and city comparisons ( appendix p 72 ).\n\n2 . As shown in our sensitivity analyses, the ERF has the greatest effect on the final outcome, indicating the importance of the ERF choice. We chose our ERFs based on WHO recommendations (for PM 2\u00b75 ) and most recent and best available meta-analysed evidence (for NO 2 ). 24 Atkinson RW\n\nButland BK\n\nAnderson HR\n\nMaynard RL Long-term concentrations of nitrogen dioxide and mortality: a meta-analysis of cohort studies. , 25 WHO\n\nWHO expert meeting: methods and tools for assessing the health risks of air pollution at local, national and international level. Meeting report. 45 Hajat A\n\nHsia C\n\nO'Neill MS Socioeconomic disparities and air pollution exposure: a global review. , 46 European Commission\n\nLinks between noise and air pollution and socioeconomic status. 3 European Environment Agency\n\nAir quality in Europe\u20142019 report. Finally, further insights into the ERFs that relate air pollution with premature mortality are needed, particularly for NO. As shown in our sensitivity analyses, the ERF has the greatest effect on the final outcome, indicating the importance of the ERF choice. We chose our ERFs based on WHO recommendations (for PM) and most recent and best available meta-analysed evidence (for NO).Nevertheless, both ERFs were pooled ERFs for non-accidental mortality, assuming equivalent mortality risk for diverse settings and populations. Risk estimate extrapolation to other settings has a potential for bias as the true ERF might vary from sub-population to sub-population, particularly if cause of death composition varies greatly between locations used to derive the concentration response associations and those where estimates are applied. As of June, 2020, there were no specific meta-analysed ERFs for distinct age, sex, or socioeconomic status categories for the European region. Nevertheless, previous research has suggested differential exposure to air pollution based on the socioeconomic groupand greater adverse health effects for people aged older than 65 years.Further research is needed so that future HIAs can account for the differential health effects that are based on region, age, sex, and socioeconomic status. Such analyses will provide a deeper understanding on how adverse health effects vary within the population and will inform more targeted policy actions where they are needed the most."
        },
        {
            "authors": [],
            "title": "Capitol Attack Was Months in the Making on Facebook",
            "contents": "Facebook suspended President Trump following the mob attack on Congress. But the platform allowed organizing for the pro-Trump rally, as well as the spread of conspiracy theories and militant extremism that drove the rioters.\n\nFacebook Chief Operating Officer Sheryl Sandberg made headlines for saying the mob attack on the U.S. Capitol was \u201clargely organized\u201d on other platforms, suggesting Facebook had done better than others at taking down dangerous content.\n\nNot only is that assertion false, according to research by the Tech Transparency Project (TTP), but it ignores the fact that Facebook spent the past year allowing election conspiracies and far-right militia activity to proliferate on its platform, laying the groundwork for the broader radicalization that fueled the Capitol insurrection in the first place.\n\nFor months, TTP has watched extremist groups use Facebook to organize and incite members, fueled by President Trump\u2019s baseless allegations of voter fraud and a \u201crigged\u201d election. Despite Facebook\u2019s new move to suspend Trump\u2019s account and other recent actions, the militant movement it allowed to flourish for so long threatens to continue its campaign of violence heading into President-elect Joe Biden\u2019s inauguration, and beyond.\n\nFor TTP, one of the first signs of mounting danger came from \u201cboogaloo\u201d groups, which we reported in April were using Facebook to prepare for a second civil war, often citing conspiratorial fears about coronavirus lockdowns. Members of private boogaloo groups flagged by TTP later engaged in real or attempted violence\u2014an ominous warning of how online radicalization can spin out of control.\n\nBut that was just the beginning. Since last fall, TTP has documented numerous instances of domestic extremists discussing weapons and tactics, coordinating their activities, and spreading calls to overthrow the government on Facebook, up to and including the mob attack on the Capitol, which left at least five people dead. Much of the activity took place in private Facebook groups\u2014insulated communities that allow people to organize out of the public eye while still having access to a large online following.\n\nHere are some of the key takeaways from that research:\n\nMilitant groups had planned a nationwide effort to \u201cback up\u201d police on Election Day against supposed antifa and Black Lives Matter protests. The event carried the logos of the Proud Boys and anti-government militias and was circulated in private far-right Facebook groups with thousands of members.\n\nSelf-declared \u201cpatriot\u201d groups on Facebook have ramped up their recruiting efforts tied to the election. Some of these groups promoted the Jan. 6 event at the Capitol.\n\nTalk of overthrowing the U.S. government increased dramatically in Facebook groups monitored by TTP following the declaration of Biden as the winner of the 2020 vote.\n\nA pro-Trump Facebook group required prospective members to declare if they would be willing to die for their country in order to join, in what may be a sign of growing extremism.\n\nCalls to \u201coccupy Congress\u201d were rampant on Facebook in the weeks leading up to the deadly Capitol riot, making no secret of the event's aims. Two different \u201coccupy\u201d event listings were written in a Nazi-style font and began circulating on Facebook in December.\n\nSince the insurrection, new posts promoting violence, including on Inauguration Day, have popped up on Facebook.\n\nBelow is a more detailed rundown of Facebook activity spotted by TTP before and after the Nov. 3 vote.\n\nThe Pre-Election Period\n\nIn the weeks preceding the November election, members of various \u201cpatriot\u201d Facebook groups organized efforts to intimidate voters at the polls\u2014an early sign of trying to shape the outcome of the election by force.\n\nOn October 27, an individual posted to the private Facebook group Ohio Patriot Action Network, as well as to other militia and pro-Trump groups and his personal profile, that veterans should bring their \u201ciron\u201d to the polls\u2014a winking reference to weapons. The same user would go on to post threats to assassinate Joe Biden and other elected officials on his personal Facebook profile, saying \u201cfair warn to ALL Military Snipers! Biden wins\u2026 they all GO DOWN!! Say farewell now before you can\u2019t!!\u201d (TTP reported these threats to appropriate authorities at the time).\n\nSlideshow: Bring weapons to polls and assassination threats\n\nThat same week, the administrator of a private Facebook group called \u201cPatriot Riders\u201d posted a Facebook event for an \u201cElection Day Evening Block Biden Ride\u201d in Johnson City, Tennessee, aimed at disrupting a local Democratic party event at voting precincts. The cover photo for the event featured the logo of the Three Percenters, an anti-government militia movement with a \u201ctrack record of criminal activity ranging from weapons violations to terrorist plots and attacks,\u201d according to the Anti-Defamation League.\n\nReports from the Capitol attack have highlighted the military- and police-style preparation of many of the rioters, who were kitted out in bulletproof vests, helmets and batons. Videos show the organized nature of some in the crowd, with one group moving in a disciplined line toward the Capitol building. According to a report from The Guardian, some rioters communicated on a walkie-talkie app, with one saying, \u201cThis is what we fucking lived up for. Everything we fucking trained for.\" This kind of activity was not a surprise to TTP, which has been monitoring how patriot and militia groups on Facebook have put a growing emphasis on tactical training and weaponry.\n\nThe activity in a Facebook group called \u201cFLORIDA PATRIOTS\u201d provides a good illustration of this trend. In October, one member called for \u201cwell armed\u201d citizens to \u201cjoin our emergency response unit in all zones,\u201d while another requested information on how to \u201ctrain and meet up and prep.\u201d Similar requests popped up in the \u201cOregon Patriots (save Oregon)\u201d Facebook group, with one member warning not to openly share logistical details about training sessions, saying \u201cthat\u2019s not really a Facebook conversation.\u201d\n\nSlideshow: Pre-election militia training and recruiting\n\nThese same groups appeared to ramp up their activity around Election Day.\n\nThe \u201cFLORIDA PATRIOTS\u201d posted a nationwide message outlining plans to provide backup to police confronting antifa and Black Lives Matter on Nov. 3. (False, right-wing rumors that antifa and BLM planned riots around the election were debunked by fact checkers.) The message featured the logos of the far-right Proud Boys, the Three Percenters and Oath Keepers, suggesting they were part of the effort. The Oath Keepers are a radical antigovernment group that \u201cclaims tens of thousands of present and former law enforcement officials and military veterans as members,\u201d according to the Southern Poverty Law Center. TTP reported this call to arms to appropriate authorities at the time.\n\nDuring this period, TTP also observed an escalation of threats to Biden and other Democratic politicians on Facebook. For example, a member of the \u201cPro-Police, Pro-Military, Pro-Trump\u201d group in early October said Minnesota Rep. Ilhan Omar should be \u201csent to Guantanamo Bay,\u201d a comment that sparked replies like \u201cJust shoot the bitch\u201d and \u201cshe needs a drone strike.\u201d The threats to Omar remained active on Facebook as of this writing despite TTP and BuzzFeed highlighting the threat back in October.\n\nAt the same time, the rhetoric among these far-right groups began to bend toward insurrection talk. In the giant \u201cStop the Steal\u201d group\u2014which Facebook only removed after the election\u2014members posted openly about overthrowing the government if Biden was declared the winner. \u201cSo IF they give this to Joe, how do we go about over throwing the government,\u201d one individual wrote to the group\u2019s 338,000-plus members on Nov. 4, prompting replies like, \u201cThey come for our guns, but we give them the ammo first.\u201d\n\nOne member of the \u201cStop the Steal\u201d group on Nov. 3 called on others to \u201cGather up arms and meet at election headquarters quickly,\u201d in response to claims that conservative poll watchers were not being permitted to view the counting of votes in swing states. This same individual had previously posted on Facebook about taking over election infrastructure and removing Michigan Gov. Gretchen Whitmer by force. (Whitmer was the target of a kidnapping plot last fall.) BuzzFeed reported on Jan. 8\u2014two days after the Capitol riot\u2014that copycat \u201cStop the Steal\u201d groups were still flourishing on Facebook, providing a continuing space for such threats.\n\nSlideshow: Stop the Steal group posts\n\nPost-Election\n\nNews of Biden\u2019s victory sparked increasingly violent talk in pro-Trump and militia groups on Facebook, which allowed the threats to freely circulate.\n\nThe Florida-based administrator of a Facebook militia group called \u201cEagle Team 1 LLC,\u201d for example, posted frequent calls to \u201ctake down\u201d the government. \u201cThe politicians in Washington DC are the biggest threat 2 our country and democracy than any foreign standing Army,\u201d read one typical post. Members replied with slogans like \u201clocked and loaded,\u201d indicating a willingness to take up arms.\n\nAnother group called \u201cTake America Back. California Chapter\u201d ramped up its recruitment drive, posting on Nov. 10 and Nov. 12 about an assembly of \u201cEcho Company, California State Militia, 2nd Regiment,\u201d which was described as a \u201cdefense\u201d resource for people who want to \u201cmaintain life, liberty and property during any \u2018Severe Controversy\u2019 that may arise.\u201d\n\nSlideshow: Post-election militia organizing and recruiting\n\nA member of the Facebook group \u201cPatriot Riders,\u201d meanwhile, called for people to organize in their local areas, posting on Nov. 7, \u201cI urge all PATRIOTS to develop rapid responce teams in your communities, neighbor hoods and cities to defend against all threats.\u201d\n\nThis individual also made violent threats on his profile, including a Nov. 10 post that stated, \u201cTRAITORS MUST HANG,\u201d with an image of nooses in the gallows titled \u201cGovernment Repair Kit.\u201d Rioters at the U.S. Capitol on Jan. 6 used this same kind of imagery, erecting a wooden frame with a noose dangling from it just outside the Capitol. (The individual described above remains active on Facebook and continues to post threats as of this writing.)\n\nTTP also observed militia members on Facebook using increasingly extreme language in describing their cause in the post-election period.\n\nA Facebook group created on Nov. 17 called \u201cWE THE PEOPLE\u201d required prospective members to answer a series of questions, including one that asked if they would be willing to be a martyr. The \u201cyes or no\u201d question read \u201cDo you support our Freedom , Rights , Constitution of the United States And are willing to fight or maybe even Die for YOUR COUNTRY ?\u201d\n\nDespite Sheryl Sandberg\u2019s effort to point the finger at other platforms for the U.S. Capitol riot, TTP identified numerous examples of Facebook users promoting the Jan. 6 rally as an opportunity to \u201c#OccupyCongress.\u201d \u201c[S]upporters should storm DC in January 6th #occupycongress and the senate, prevent the vote by a force of nature,\u201d read one post, which included the hashtag for Trump\u2019s Make America Great Again slogan.\n\nIn the \u201cTake America Back. California Chapter\u201d Facebook group, a member on Dec. 23 issued a call to \u201cOCCUPY THE CAPITAL JANUARY 6, 2021,\u201d with a poster that included details of the event. The poster included the words \u201cOperation Occupy The Capitol\u201d in the Fraktur font, which is commonly associated with the Nazi regime in Germany. Another post in a Facebook group called \u201cThe Patriot Party\u201d included the hashtag #OccupyCongress and the slogan \u201cThe Great Betrayal,\u201d alongside text that read, \u201cIf they won\u2019t hear us, they will fear us.\u201d\n\nSlideshow: January 6th posts\n\nSupporters of militia and extremist groups became increasingly overt about their role in the Jan. 6 rally in the days and hours leading up to the event. In a Facebook group called \u201cMouthy Patriots,\u201d a member posted an announcement stating that Oath Keepers would be deploying to Washington on Jan. 5 and 6 to \u201cProtect Events, Speakers, & Attendees.\u201d On the morning of Jan. 6, a member of the Facebook group \u201cUnder the Liberty Tree\u201d posted a meme featuring the Three Percenter logo that said \u201cWE CANNOT WE WILL NOT COMPLY. Prepare to Take America Back.\u201d\n\nIt was one of several militia-linked memes with calls to action that popped up in far-right Facebook groups hours before the attack on the Capitol, suggesting they may have served as dog whistle to supporters attending the rally in Washington that day. (Republican Rep. Lauren Boebert of Colorado also tweeted that morning, \"Today is 1776,\" echoing a slogan used by far-right groups to reference a new revolution and violent uprising against the government.)\n\nIn the days since Jan. 6, Facebook has continued to allow discussion of insurrection and is even profiting from it. Research by BuzzFeed and TTP found that Facebook was pushing ads for weapons accessories and military gear to users who engage in militia and far right groups on the platform. (Facebook later paused such ads until after the inauguration.)\n\nFollowing the Capitol riot, TTP also spotted a number of still-active Facebook posts that equated the insurrection with the American Revolution or tried to organize new militias.\n\nPerhaps more alarmingly, TTP found Facebook posts making new calls for violence, including around the Inauguration. One post referred to Jan. 20, Inauguration Day, as a \"Tiananmen Square moment\" for self-styled patriots. These messages remained active on Facebook despite the company's statement that its teams are \"working 24/7 to enforce our policies around the inauguration.\"\n\nCNN reported on the new posts identified by TTP on Jan. 18. In response to the report, Facebook said it removed the Tiananmen Square post, and told CNN it had \"proactively detected\" and was removing the \"Patriot Party\" group that featured the militia organizing message.\n\nConclusion\n\nFacebook not only facilitated organizing for the Jan. 6 event in Washington that culminated in the deadly Capitol riot; it has spent the past year failing to remove extremist activity and election-related conspiracy theories stoked by President Trump that have radicalized a broad swath of the population and led many down a dangerous path.\n\nThroughout 2020, Facebook\u2019s efforts to curb violent activity and disinformation were either too late or ineffective or both. It banned a boogaloo network in June after members of Facebook boogaloo groups were linked to a terrorism plot and murder, but its enforcement of the new policy fell short. Over the summer, Facebook banned militia groups, but it failed to take down the page for a militia event that led to the deadly shooting of two protesters in Kenosha, Wisconsin. The company\u2019s new crackdown on the phrase \u201cstop the steal\u201d came only after the mob attack on the Capitol.\n\nTTP watched with growing alarm starting this fall as a broad range of \u201cpatriot,\u201d militia, and pro-Trump groups grew increasingly bold on Facebook with their organizing efforts, recruitment and training of new members, and talk of weapons, violence, and government takeovers. As the nation heads toward Inauguration Day and contemplates life under a new Biden administration, Facebook\u2019s massive reach and patchy, inconsistent moderation mean it\u2019s likely to continue to be a gathering space for extremist groups that seek to do harm.\n\nTo view the full collection of images captured by TTP, click here and here.",
            "published_at": "2021-01-19T10:38:52-05:00"
        },
        {
            "authors": [],
            "title": "SOUL, a new language now in V1.0, makes audio coding vastly more accessible",
            "contents": "SOUL, a new language now in V1.0, makes audio coding vastly more accessible\n\nJanuary 20, 2021\n\nLondon \u2014 Today\u2019s release of SOUL, the new universal language for audio applications, will dramatically lower barriers and expand access for developers creating any apps that depend on audio, as well as providing a more efficient architecture for audio processing.\n\nSOUL has reached version 1.0 status with its language, compiler, and \u201cSOUL patch\u201d format all stable, tested, and ready to use in a wide variety of audio-related projects.. The team, led by JUCE and Tracktion creator Julian Storer, is already at work on an array of other SOUL tools including a visual editor and a developer portal.\n\n\u201cSOUL will revolutionize audio app development, eliminating challenges that have impeded developers for too long,\u201d said Julian Storer, Head of Software Architecture at ROLI. \u201cThe need for a radical rethink of how audio apps are made has only become more urgent since I started the SOUL project in 2016. I\u2019m tremendously excited about the V1.0 released today and the additional tools to come.\u201d\n\nBoth a programming language and a platform for creating audio plugins and apps, SOUL will solve four fundamental problems facing audio developers today:\n\nEasy to learn and use: C++, the standard language for audio app coding, is extremely difficult to learn. The SOUL language is much simpler and more intuitive. Browser-based tools and fast live testing make it even easier to use and master.\n\nMore secure: Sandboxing untrusted third-party code adds performance overheads that are a particular problem for audio coding. SOUL uses an intermediate assembly language called HEART that is safe to run without sandboxing, making SOUL code far more secure.\n\nOptimized for power and performance: CPU-intensive power consumption is a constant problem for developers. The SOUL language is far more efficient. It is specifically designed for performance, with its JIT engine matching an equivalent program written in native C++.\n\nAchieves lowest latency: Latency in audio apps is another deep-rooted issue that to date is solved by running audio on dedicated audio hardware. SOUL unlocks ultra-low latency on devices without the need for embedded CPUs and DSPs.\n\nSince debuting SOUL at ADC 2018, the team has continuously welcomed feedback through the open-source repository at https://soul.dev. This feedback has helped SOUL reach its V1.0 state today.\n\nThe SOUL team encourages developers to explore the language on soul.dev, read the documentation on the SOUL repository on Github, and give more feedback as the SOUL toolset continues to grow this year.\n\nFurther reading:"
        },
        {
            "authors": [
                "Thomson Reuters"
            ],
            "title": "Twitter locks account of Chinese Embassy in U.S. over 'dehumanization' of Uighurs in tweet",
            "contents": "Twitter has locked the account of the Chinese Embassy in the U.S. for a tweet that defended China's policies in the Xinjiang region, which the U.S. social media platform said violated the firm's policy against \"dehumanization.\"\n\nThe Chinese Embassy account, @ChineseEmbinUS, tweeted this month that Uighur women were no longer \"baby making machines,\" citing a study reported by state-backed newspaper China Daily.\n\nThe tweet was removed by Twitter and replaced with a label stating that it was no longer available. Although Twitter hides tweets that violate its policies, it requires account owners to manually delete such posts. The Chinese embassy's account has not posted any new tweets since Jan. 9.\n\n\"We've taken action on the Tweet you referenced for violating our policy against dehumanization, where it states: We prohibit the dehumanization of a group of people based on their religion, caste, age, disability, serious disease, national origin, race, or ethnicity,\" a Twitter spokesperson said on Thursday.\n\nThe Chinese embassy in Washington did not immediately reply to an e-mailed request for comment. Twitter is blocked in China.\n\nThe Biden administration did not immediately respond to a request for comment on Twitter's move.\n\nAllegations of genocide\n\nIn one of his final acts in office, now-former secretary of state Mike Pompeo declared Tuesday that China's policies against Muslims in its Xinjiang region constitute \"crimes against humanity\" and \"genocide.\" President Joe Biden's chosen successor to Pompeo, Antony Blinken, said he shared the same view.\n\nI have determined that the People\u2019s Republic of China is committing genocide and crimes against humanity in Xinjiang, China, targeting Uyghur Muslims and members of other ethnic and religious minority groups. \u2014@SecPompeo\n\nXinjiang, a far western region that borders Central Asia, is home to the predominantly Muslim Uighur ethnic group. China denies human rights violations and says its actions in Xinjiang are necessary to counter a separatist and terrorist threat.\n\nIn a striking repudiation of its relationship with Washington under Trump, the Chinese Foreign Ministry announced sanctions against \"lying and cheating\" Pompeo and 27 other top Trump administration officials in a statement that appeared on its website around the time that Biden was taking the presidential oath.\n\nPompeo and the others had \"planned, promoted and executed a series of crazy moves, gravely interfered in China's internal affairs, undermined China's interests, offended the Chinese people, and seriously disrupted China-U.S. relations,\" it said.\n\nThe 28 individuals and immediate family members would be banned from entering mainland China, Hong Kong or Macao, and companies and institutions associated with them restricted from doing business with China.\n\nWATCH | Aspects of Chinese treatment of Uighurs fits genocide definition: Bob Rae\n\nAspects of Chinese treatment of Uighurs fits genocide definition: Bob Rae CBC News Video 10:03 Canada's ambassador to the United Nations says he's asked the international organization to gather evidence and investigate whether China's persecution of Uighurs in Xinjiang province constitutes a genocide. 10:03\n\nChina has repeatedly rejected accusations of abuse in its Xinjiang region, where a United Nations panel has said at least one million Uighurs and other Muslims had been detained in camps.\n\nLast year, a report by German researcher Adrian Zenz published by the Washington-based Jamestown Foundation think-tank accused China of using forced sterilization, forced abortion and coercive family planning against minority Muslims. The Chinese foreign ministry said the allegations were groundless and false.\n\nChina's foreign ministry is lashing out at Canada after a House of Commons subcommittee concluded that the state's mistreatment of Uighurs living in Xinjiang province amounts to a policy of genocide.\n\nZhao Lijian, a spokesperson for the Chinese foreign ministry, said in November that this \"so-called genocide\" is \"a rumour and a farce fabricated by some anti-Chinese forces to slander China.\"\n\nSuspension of embassy's account follows removal of Trump's\n\nThe embassy's account suspension comes shortly after Twitter removed the account of former U.S. president Donald Trump, which had 88 million followers, citing the risk of violence after some of his supporters stormed the U.S. Capitol this month.\n\nTwitter had locked Trump's account, asking for deletion of some tweets, before restoring it and then removing it altogether after the former president violated the platform's policies again.\n\nWATCH | Twitter permanently suspends Trump's account:"
        },
        {
            "authors": [
                "David Atkins",
                "Kenneth S. Baer",
                "Nancy Letourneau",
                "Martin Longman",
                "Frank O. Bowman",
                "Luke"
            ],
            "title": "It Turns Out that Deplatforming Works",
            "contents": "After Twitter and Facebook banned Donald Trump and over 70,000 QAnon-related accounts, two things quickly became apparent: 1) it was the right thing to do and had a salutary effect on public discourse, and 2) tech moguls have a frightening amount of control over democracy and public discourse.\n\nOf course, most of the Right and parts of the libertarian left have strongly objected to the decision to deplatform Trump and right-wing conspiracists. But democracy depends largely on agreement on a basic set of facts, and widely shared conspiracy theories about stolen elections or cannibal pedophilia can lead to violence and authoritarianism. Social media has been primarily responsible for allowing those conspiracy theories to flourish, and social media has an obligation to fix the problem.\n\nAnd indeed, deplatforming conspiracy promoters has been proven to work, both now and in the past. In the wake of the recent bans of Trump and QAnon mavens, election misinformation online has dropped by over 70%:\n\nOnline misinformation about election fraud plunged 73 percent after several social media sites suspended President Trump and key allies last week, research firm Zignal Labs has found, underscoring the power of tech companies to limit the falsehoods poisoning public debate when they act aggressively. The new research by the San Francisco-based analytics firm reported that conversations about election fraud dropped from 2.5 million mentions to 688,000 mentions across several social media sites in the week after Trump was banned from Twitter . Election disinformation had for months been a major subject of online misinformation, beginning even before the Nov. 3 election and pushed heavily by Trump and his allies. Zignal found it dropped swiftly and steeply on Twitter and other platforms in the days after the Twitter ban took hold on Jan. 8.\n\nThe problem, of course, is that if a single push of a button by Mark Zuckerberg and Jack Dorsey can so profoundly affect democratic discourse and even election outcomes, then our democracy is functionally profoundly affected if not controlled outright by a few autocratic corporations. After all, what if Facebook and Twitter had taken these actions far earlier? What if their leadership, or the whims of their CEOs, changed to being sympathetic with autocracy? It\u2019s far too much power in too few hands, without democratic accountability.\n\nThe flip side of the argument, though, is that it would indeed be totalitarian for the government to dictate to a private company that it must allow certain speakers or irresponsible speech on its platforms, even when they run counter to its terms of service or even expose it to liability. It was comical to hear conservatives claim that \u201cTwitter censorship\u201d resembled \u201cCommunist China\u201d when the reality in China is that media networks are required to support its political leaders\u2013quite the opposite of being empowered to ban them.\n\nBoth sides of the debate over free speech and social media then culminate in unacceptable outcomes: either forcing these companies to continue to promote misinformation destructive to democracy or allowing their whimsical terms of service to promote or restrict the speech of any political actors they see fit.\n\nThe fundamental challenge is that these companies have too much control over the information economy to begin with. Facebook has more power over the news people see than the biggest newspapers in the country combined. Facebook and Google allegedly colluded to lock down ownership of the online advertising market, which in turn affects the financial incentives of journalism as a whole. Twitter has effectively become the public square in which political elites and influencers drive narratives.\n\nIt won\u2019t be easy to do, but creating a healthy and organic information environment will require significant public regulation of social media companies and content and renewed investment in publicly funded journalism as in many other major democracies. Antitrust action from the Biden administration can also help.\n\nAs long as we are stuck in this privately controlled social media dominated system, deplatforming the worst actors\u2013even if they\u2019re the president of the United States\u2013is the best of a set of imperfect options. But long-term, we will have to fix the system itself if we want a healthy democracy.",
            "published_at": "2021-01-17T00:00:00"
        },
        {
            "authors": [
                "Juan Jacobs"
            ],
            "title": "juanmjacobs/il2cpp-modder: Generate DLL injection templates for modding il2cpp games",
            "contents": "Generate DLL injection templates for reverse engineering and modding Unity il2cpp games.\n\nMotivation\n\nSo you've been researching how to do some hacky stuff in Unity games and stumbled upon the amazing IL2CppDumper. You have learned how to get and analyze the dump.cs and now you know exactly what you want to do, but you do not know how to do it.\n\nYou know that you have to override some method, replace the value of some field in an object or something like that, but your low level programming skills with all that nasty pointer arithmetic and assembly are not quite there yet (don't worry, mine barely are, just practice and you'll get better!).\n\nWell this is the project for you! il2cpp-modder lets you describre what to do and it will generate all the code to do it. That's it, you don't have to program at all!\n\nBuilt in mods\n\nMake a method return a fixed value. Some ideas: Make a validation always return true or false. Make a getter always return 0 or a really high number. Skip validation methods.\n\nReplace the arguments for a method call. Some ideas: Make a setter always set 999999 coins to your player.\n\nSet the value of an object field. Some ideas: Keep your player health always at 100%. Change your player height at any moment to any value.\n\nReplace the implementation of a method. If the other mods just don't cut it, you can just replace the whole thing for something different that suits your needs! The sky is the limit. You do need to program C++ for this though...\n\n\n\nRequirements\n\nNodeJS\n\nIL2CppDumper (add to PATH!)\n\nVisual Studio (Or any C++ compiler)\n\nInstallation\n\ngit clone https://github.com/juanmjacobs/il2cpp-modder cd il2cpp-modder sh install.sh\n\nUsage\n\nSee the usage documentation\n\nContributing\n\nIssues and PRs are welcome!"
        },
        {
            "authors": [
                "Jeff Bezanson",
                "Stefan Karpinski",
                "Viral Shah",
                "Alan Edelman",
                "Et Al."
            ],
            "title": "The 2020 Industry Julia Users Contributhon",
            "contents": "A few months ago, I wrote a blog post reflecting on an interesting JuliaCon 2020 discussion between a group of industry-focused Julia users that focused on the interplay between closed-source proprietary software development and Julia's open source software (OSS) ecosystem. The blog post summarized a few key issues that our discussion group felt served as real hurdles to meaningful OSS contribution from within private organizations. The post concluded by announcing our attempt to jump these hurdles: the Julia Industry Users Contributhon, a hackathon where participating industry organizations could come together to contribute back to the Julia ecosystem.\n\nThis event's goals were to...\n\n...foster/strengthen collaboration across organizational boundaries, and reduce potentially duplicated efforts.\n\n...both push forward and prove the Julia ecosystem\u2019s readiness for \u201cproduction\u201d use.\n\n...provide a nice promotional incentive for the involved organizations to dedicate time to and participate in OSS efforts.\n\n...provide promotional and technical benefits to the Julia community as whole.\n\n...and have a huge amount of fun!\n\nNow, in 2021, I'm happy to report on all of the great contributions made as part of the event last month by the wonderful folks at Beacon Biosignals, Invenia, TriScale innov, RelationalAI, and PumasAI. All shapes and sizes of contribution were welcome at the event: we released whole packages, made PRs, caught bugs, started new projects, and planned out our OSS roadmaps/backlogs for future work.\n\nHere is a sampling of the standouts:\n\n...and so many more issues/PRs/etc. that I couldn't fit here - especially the important work that went into public OSS backlog creation.\n\nOn top of the contributions themselves, this event taught us about (or in some cases, reminded us of) a few valuable points:\n\nJulia's package manager works seamlessly even when a single package's registered versions are split across private and public registries. In a lot of cases, this made it super easy to upgrade downstream internal packages to the now-open versions of their upstream dependencies - it just required a version bump!\n\nHaving a well-documented, battle-tested internal process for safely open-sourcing private code drastically lowers the barrier to actually doing so. Creating an explicit (and if possible, public) \"OSS backlog\" is a great way to kickstart the development of an internal process.\n\nGather is actually a pretty useful platform for collaboration! Just like at JuliaCon, it was fun to meet and hang out with the actual people behind the GitHub handles you come across day-to-day in the online Julia community. Also, adding a spacial component to video chat really goes a long way towards recreating the vibe of a shared workspace. After having a great experience using it to host the event, we at Beacon Biosignals created our own internal Gather space that we hang out in daily.\n\nCross-org collaboration has always been a huge development driver in the Julia community, and this event was no different! It was fun to discover the various points of intersection between our orgs' tech stacks, and start working together directly on these tools. The JuliaCloud ecosystem (especially AWS.jl) turned out to be of particular interest across participants.\n\nThere are a lot of amazing engineers out there who, before joining an OSS-friendly company, never had an opportunity to contribute to OSS as part of their day-job. This event served as a great OSS onboarding ramp by enabling these individuals to translate pre-existing internal work into impactful external contributions.\n\nAll in all, I'd say the Contributhon was a success, and established some key points of collaboration between industry Julia users that I suspect will carry on for quite some time. More importantly, it was a whole lot of fun. Thanks to everybody that attended, everybody that facilitated, and all the wonderful folks in OSS community whose projects support Julia's continued rise in industry!"
        },
        {
            "authors": [
                "Eric Newcomer"
            ],
            "title": "The Unauthorized Story of Andreessen Horowitz",
            "contents": "Benedict Evans, Andreessen Horowitz\u2019s former in-house analyst, has mused over the years that \u201cA16Z is a media company that monetizes through VC.\u201d\n\nThat observation becomes truer by the day.\n\nWhile there\u2019s a lot of loose talk on Twitter about cutting out the media and \u201cgoing direct\u201d \u2013 publishing your own story to the world without the press as an intermediary \u2013 Andreessen Horowitz is really doing it, consciously and methodically. The firm\u2019s strategy has dramatic implications for the future of media and the venture capital industry.\n\nThis is the story of how Andreessen Horowitz disrupted the world of venture capital by cozying up to the media and then, how they purposefully threw that relationship away.\n\nLet me tell you the story from the beginning.\n\nAbout a decade ago, Margit Wennmachers sent an email to reporter Kara Swisher.\n\nSwisher took a break from horseback riding to write that Wennmachers was leaving Outcast, the communications agency she co-founded with Caryn Marooney. Wennmachers was joining the one-year-old venture capital firm Andreessen Horowitz as a full partner \u2013 a rare title for a woman in Silicon Valley, especially in 2010.\n\nWennmachers explained her strategy for positioning the firm in the early days, in an interview on Andreessen Horowitz\u2019s in-house podcast. \u201cI wanted to try and get a cover story because \u2013 this sounds old fashioned now, because everybody reads their news on Twitter and it's all online whatever \u2013 but there's still a statement that comes with a cover story that is in print, that you see at the airport,\u201d she recalled.\n\nWith Wennmachers\u2019 press savvy, Marc Andreessen\u2019s idea-a-second patter, and Ben Horowitz\u2019s gravitas, the trio took Silicon Valley by storm. The firm outbid competitors for sought-after companies, spun up a slew of services for founders, and pitched its story relentlessly to the press.\n\nWith Wennmachers\u2019 encouragement Andreessen penned a now historic Op-Ed in the Wall Street Journal in 2011, titled, \u201cWhy Software is Eating the World.\u201d The phrase became so ubiquitous that it can seem like everything eats the world these days.\n\n\u201cMargit is really a hidden founder of this firm,\u201d a startup founder who has raised money from the firm told me. \u201cThe power dynamics there is, Marc, Ben, and Margit.\u201d\n\nCommunications executives and reporters alike are in awe of Wennmachers for her sway with the media. \u201cComms in the Valley \u2013 she's the number 1 draft pick. She's very good,\u201d one public relations person said. \u201cI would never want to be crosswise with her.\u201d\n\nWhile Wennmachers\u2019 strategy in that early period would have been familiar to many in Hollywood and Washington, it was less common in clubby and decorous Silicon Valley at the time. \u201cShe would say that was her job \u2013 to manage information so that she could shape the narrative,\u201d one person who knows Wennmachers said.\n\nWennmachers deployed industry gossip and access to her firm\u2019s partners to stay in the good favor of many reporters.\n\nOne communications executive at an Andreessen Horowitz portfolio company recalled Wennmachers fishing for information about an upcoming story on behalf of reporters. Wennmachers pushed to deliver the information to the reporters herself, this executive said.\n\nA high-powered rival PR executive described Wennmachers as an enforcer: \u201cYou don't cross us and if you do, we shut off the information flow.\u201d\n\nOne member of the press recalled Wennmachers chasing down a potential bit of news. When Wennmachers returned the call and threw cold water on the story, the reporter took it in stride. Wennmachers told the reporter that they were now a \u201cfriend of the firm.\u201d The comment struck the reporter as odd. They were just trying to get the facts right. But Wennmachers\u2019 attitude seemed to reflect a coziness that she had come to expect from reporters dutifully covering Silicon Valley.\n\nWennmachers regularly hosted salon-style dinners at her home near the Presidio in San Francisco with reporters, portfolio companies, and the firm\u2019s partners. While it\u2019s not unusual for a venture capital firm to host reporters for a dinner, Wennmachers did it better than anyone else. When I attended one of her dinners in 2014, it felt like I\u2019d finally gotten invited inside the Silicon Valley.\n\n\u201cI was invited to dinner at her house with members of the media. It was incredible,\u201d one founder recalled. \u201cThat was like a cool invitation. If you were in the press you were really excited about getting dinner at Margit\u2019s house, which is kind of mind-blowing if you think about it.\u201d\n\nWennmachers can put on a friendly fa\u00e7ade, but if you spend much time with her you come to realize that she\u2019s a profoundly serious person. She doesn\u2019t win over reporters because she\u2019s fun. It\u2019s because she cuts through the bullshit. She fundamentally understands what makes a good story. She knows what motivates reporters better than many reporters. That\u2019s part of what makes the firm\u2019s turn against the media so worrying.\n\nBack in that early media heyday, Wennmachers doled the firm\u2019s partners out to media companies who were eager to have them speak at lucrative conferences. And the firm\u2019s partners were game to prophesize in audacious terms about what the future might look like, filling column inches.\n\nSwisher and Wennmachers \u2013 two women who climbed their way to dominance in an industry overrun with men \u2013 are friendly to this day. Many Silicon Valley insiders strongly suspect that Wennmachers was one of Swisher\u2019s sources when she covered Silicon Valley\u2019s day-to-day dramas. But Swisher describes Wennmachers as fiercely loyal to her own companies, though Wennmachers \u201calso did not pretend a problem I might call about was not one.\u201d\n\nMarc Andreessen himself had a cozy relationship with reporters like Swisher. \"Marc is fully in charge of his own communications and has over time stopped wanting to deal with the press for lots of reasons, including some mistakes he made.\u201d\n\nFrom 2009 through 2015, Andreessen Horowitz earned a run of truly phenomenal press coverage, catapulting itself into the very top echelon of venture capital firms \u2013 at least based on its reputation. The media loved that bald headed internet geek and his gruff business guru sidekick. And over cocktails or lunch at the Battery, reporters probed Wennmachers for information, even if she rarely earned a mention in their stories.\n\nThen the magazine writer Tad Friend pitched a profile of Marc Andreessen for the New Yorker. Wennmachers already saw the firm\u2019s position in the popular consciousness beginning to shift. The firm was no longer an exciting upstart. \u201cAt that point I was at a stage where it was like \u2018enough.\u2019 \u2018We're done talking about ourselves,\u2019\u201d Wennmachers said in the podcast interview. On the other hand, she reasoned, \u201cThe New Yorker is the New Yorker, so how often is the New Yorker going to write the definitive piece on venture capital? Once in a decade? Maybe? Well, do I want that to be about whoever or do I want that to be about us? At that point, I want to occupy that spot?\u201d\n\nShe decided to do the story \u2013 though of course she wouldn\u2019t occupy it. She merited a single passing mention. Marc Andreessen\u2019s shiny head was the focus instead.\n\nThe story ran in May 2015, with the title, \u201cTomorrow\u2019s Advance Man.\u201d The glowing portrait instantly became one of the seminal stories on the venture capital industry.\n\nDespite running more than 10,000 words, the article made a single passing mention of one of Andreessen Horowitz\u2019s most important investments, Zenefits. The firm had backed up the truck to invest in Parker Conrad\u2019s human resources software company and hyped it in the press. But cracks had started to appear. By the end of 2015, Buzzfeed reporter Will Alden had planted the seeds of Zenefit\u2019s unraveling, raising questions about whether the company\u2019s health insurance brokers were properly licensed.\n\nThat same year, John Carreyrou revealed that Theranos had \u201cstruggled with its blood testing technology.\u201d Theranos had mostly raised money outside of Silicon Valley. But some venture capitalists tied their reputations to defending the company anyway. Marc Andreessen, who tweeted prolifically at the time, appeared to develop an affinity for blocking people who tweeted negatively about Theranos.\n\nFor reporters \u2013 and especially their editors \u2013 the Theranos story was a sign that tech reporters needed to take a more critical eye towards the startups they covered. Carreyrou was an investigative reporter, not on the Silicon Valley beat at all. Tech reporters shouldn\u2019t have missed Theranos\u2019s shortcomings. So, they redoubled their efforts.\n\nThen, indeed, Andreessen Horowitz found itself with a troubled portfolio company of its own. In May 2016, I reported in Businessweek about Zenefit\u2019s self-disruption. Many other tech reporters also dug into the company\u2019s failings.\n\nBehind the scenes, Kim Milosevich, then Wennmachers\u2019 top lieutenant, went to task pointing much of the blame for the company\u2019s implosion on its co-founder Parker Conrad \u2013 who certainly deserved a big portion of the credit for the company\u2019s problems.\n\nBut Andreessen Horowitz partner Lars Dalgaard had cheered Conrad on, telling Conrad to double the company\u2019s revenue growth target, I reported back then. Zenefits was the company Andreessen Horowitz wanted to grow, grow, grow. Years earlier, Ben Horowitz himself had penned an essay making the case for \u201cfat startups,\u201d ones that spend aggressively to block out their competitors. Zenefits seemed to be running that playbook.\n\nBut as the company\u2019s business started to falter and with mounting regulatory risk, Horowitz stepped in to help push Conrad to step aside. The firm laid the blame at Conrad\u2019s feet. And David Sacks, who had been the company\u2019s chief operating officer, was sold to the world as a turnaround CEO.\n\nThat same year, the press started to turn its attention to Andreessen Horowitz itself, wondering if this firm really lived up to all the hype we\u2019d been fed over the years.\n\nMy occasional bridge partner, Wall Street Journal reporter Rolfe Winkler, wrote in September 2016 that the firm\u2019s returns trailed firms like Benchmark and Sequoia.\n\nIn an early sign of things to come, Andreessen Horowitz published a meandering public response.\n\nIt\u2019s a style of rebuttal reporters are all too familiar with. Publicly the subject shouts that the story is all wrong and after the story runs, it publishes a blog post without specific contradicting details. Internally the communications team doesn\u2019t object to anything credible in fact-checking, nor do they ask for a correction after the fact. Really, it\u2019s the impression generated by the story that the subject is contesting \u2013 not any particular factual assertion. Andreessen Horowitz wants people to think it\u2019s doing well, not poorly. And, of course, the blog post triggers a swarm of fanatic loyalists on Twitter who attack the media messenger.\n\nAndreesen Horowitz partner Scott Kupor\u2019s post carried the philosophical title, \u201cWhen is a \u201cMark\u201d Not a Mark? When it\u2019s a Venture Capital Mark.\u201d To this day, the firm takes issue with the story\u2019s conflation of \u201cmarks\u201d with \u201creturns\u201d \u2014 the word the Wall Street Journal used.\n\nI asked Kupor to talk for another story and he declined. \u201cFor now, we are going to stick with our twitter wars :)\u201d\n\nI replied, \u201cGo direct.\u201d And he sent me another smiley face.\n\nJust a few weeks after Winkler\u2019s story, Marc Andreessen deleted his old tweets and largely disappeared from the social network.\n\n(I suspect that Winkler\u2019s story holds up in hindsight, but someone will have to leak me their returns.)\n\nBy 2017, the election of Donald Trump had, I believe, solidified reporters\u2019 intentions to ferret out unscrupulous founders. Reporters and their coastal readers couldn\u2019t remove the strong man in the White House, so the least they could do was hold Silicon Valley leaders to a higher ethical standard. Couple that with reporters working to replicate the Harvey Weinstein #MeToo reporting, and journalists set off on a course of much needed radical accountability.\n\nIn 2018, Facebook \u2013 with Wennmachers\u2019 Outcast co-founder, Marooney, serving as one of its key communications executives \u2013 dramatically published information about Cambridge Analytica ahead of an imminent New York Times report. It was a key instance of getting ahead of a negative news story. Facebook\u2019s defiance of the New York Times only seemed to solidify the media\u2019s perception that there was a real scandal there. It was another signal of a deteriorating relationship between tech communications executives and the reporters who write about the tech industry.\n\nAs the relationship between Silicon Valley and the media was breaking down, Andreessen Horowitz pursued an alternative marketing strategy. The firm had been building up a media operation of it own. In 2014, Andreessen Horowitz hired Sonal Chokshi who was an opinion editor at Wired magazine. Chokshi serves as the firm\u2019s editor-in-chief, running an expanding fleet of popular podcasts.\n\nToday, roughly 10% of the 200-person firm works on its marketing team. The company is expanding its editorial operation.\n\nTalk to anyone around the firm and Chokshi\u2019s in-house media strategy is the future of the firm. One communications person remarked to me, \u201cThey've become a media company basically.\u201d\n\nCompanies and venture firms have long tried their hands at content marketing \u2014 that\u2019s what all those Medium posts are for. The thing that\u2019s unique about Wennmachers\u2019 and Chokshi\u2019s operation is simply that they do it well and at a greater scale. Their podcasts are actually interesting. And since the media has adopted an extremely negative posture toward the tech industry, there\u2019s a big appetite for coverage with a more upbeat slant. Andreessen Horowitz can fill that market demand.\n\nAt the same time, the firm has largely stopped cooperating with the media. I\u2019ve talked to a number of reporters at top outlets and that\u2019s the consensus. For the past couple years, the firm has been quiet even anonymously. Andreessen Horowitz has mostly abstained from participating in media coverage except for a story for the Midas List in Forbes \u2013 among the most shamelessly positive sources of media in the tech industry today.\n\nAndreessen has taken to dropping in on chats on the firm\u2019s portfolio company Clubhouse. His comments have raised eyebrows. How much of the firm\u2019s silence is tactical? And how much simply reflects an anti-media ethos that has penetrated the firm\u2019s leaders?\n\nIt\u2019s not lost on me that I\u2019m writing this story from one such platform for \u201cgoing direct\u201d \u2014 Andreessen Horowitz portfolio company Substack.\n\nThis past Thanksgiving the cryptocurrency company Coinbase published a blog post warning about an impending critical news article. Coinbase is essentially a satellite orbiting the venture capital firm Andreessen Horowitz. The company\u2019s Vice President of Communications is Milosevich, the long-time Andreessen PR handler who had managed the story of Conrad\u2019s ouster.\n\n\u201cThe New York Times is planning to publish a negative story about Coinbase,\u201d the public notice read. By posting its blog post before the news story ran, Coinbase made a daring and conscious faux pas in the delicate dance between the tech press and the companies they cover, one facilitated by behind-the-scenes communications professionals. The post included the line, \u201cwe don\u2019t care what The New York Times thinks.\u201d\n\nIt was an eyebrow raising statement given Milosevich\u2019s years helping to court the media at Andreessen Horowitz and the many magazine profiles written about Coinbase board member Marc Andreessen.\n\nBefore the newspaper had even published its investigation, partisans took to their sides. Mike Isaac \u2013 the New York Times reporter with a Charmin bear avatar \u2013 tweeted, \u201cthis attempt at a front-run is mindblowing. They\u2019ve guaranteed readership for the coming story AND torched any semblance of trust or relationship they had [with the] media.\u201d Balaji Srinivasan \u2013 the chief apostle at the church of \u201cgoing direct\u201d \u2013 quoted Isaac\u2019s tweet and replied, \u201cWho trusts the New York Times Company?\u201d\n\nThat Friday, The New York Times ran its story, headlined, \u201c\u2018Tokenized\u2019: Inside Black Workers\u2019 Struggles at the King of Crypto Start-Ups.\u201d The story read, \u201caccording to 23 current and former Coinbase employees, five of whom spoke on the record, as well as internal documents and recordings of conversations, the start-up has long struggled with its management of Black employees.\u201d\n\nBut Andreessen Horowitz has carried on publicly like nothing happened. In December, the firm published an elaborate illustrated package of stories called \u201cSocial Strikes Back.\u201d It looked like something you might see in Fortune or Wired. But, of course, no space was earmarked to examine the firm\u2019s culpability for Coinbase\u2019s corporate culture.\n\n\u201cThe question that tech companies have to ask themselves is whether they have a commitment to the role of media in an informed society,\u201d says Adam Mendelsohn, who ran communications for TPG and is the long-time media advisor to LeBron James. \u201cJust because it's easy to put out your own information doesn't necessarily make it right.\u201d\n\nWennmachers understands the media like few others do. But she\u2019s also seen its dark side. When the tech media wanted utopian fables, she saw how eagerly they took her calls. And now that the press has soured on tech, her brand of media molding is less powerful. Now that reporters are stuck on a narrative that she doesn\u2019t like, she\u2019s come to the conclusion that she doesn\u2019t need them anyway.\n\nShe might well be correct that non-cooperation is a good strategy for Andreessen Horowitz. The firm has the tools to speak directly to its audience. But is her strategy good for the world? Isn\u2019t there value in engaging with an independent press?\n\nI talked to Wennmachers for over an hour for this story on the condition that I wouldn\u2019t quote her. She asked me to write that she strenuously disagreed with many of this story\u2019s characterizations and facts.\n\nI wish I could tell you what else she had to say."
        },
        {
            "authors": [],
            "title": "Raspberry Pi Foundation launches $4 microcontroller with custom chip \u2013 TechCrunch",
            "contents": "Meet the Raspberry Pi Pico, a tiny little microcontroller that lets you build hardware projects with some code running on the microcontroller. Even more interesting, the Raspberry Pi Foundation is using its own RP2040 chip, which means that the foundation is now making its own silicon.\n\nIf you\u2019re not familiar with microcontrollers, those devices let you control other parts or other devices. You might think that you can already do this kind of stuff with a regular Raspberry Pi. But microcontrollers are specifically designed to interact with other things.\n\nThey\u2019re cheap, they\u2019re small and they draw very little power. You can start developing your project with a breadboard to avoid soldering. You can pair it with a small battery and it can run for weeks or even months. Unlike computers, microcontrollers don\u2019t run traditional operating systems. Your code runs directly on the chip.\n\nLike other microcontrollers, the Raspberry Pi Pico has dozens of input and output pins on the sides of the device. Those pins are important as they act as the interface with other components. For instance, you can make your microcontroller interact with an LED light, get data from various sensors, show some information on a display, etc.\n\nThe Raspberry Pi Pico uses the RP2040 chip. It has a dual-core Arm processor (running at 133MHz), 264KB of RAM, 26 GPIO pins including three analog inputs, a micro-USB port and a temperature sensor. It doesn\u2019t come with Wi-Fi or Bluetooth. And it costs $4.\n\nIf you want to run something on the Raspberry Pi Pico, it\u2019s quite easy. You plug your device to your computer using the micro-USB port. You boot up the Raspberry Pi Pico while pressing the button. The device will appear on your computer as an external drive.\n\nIn addition to C, you can use MicroPython as your development language. It\u2019s a Python-inspired language for microcontrollers. The Raspberry Pi Foundation has written a ton of documentation and a datasheet for the Pico.\n\nInterestingly, the Raspberry Pi Foundation wants to let others benefit from its own chip design. It has reached out to Adafruit, Arduino, Pimoroni and Sparkfun so that they can build their own boards using the RP2040 chip. There will be an entire ecosystem of RP2040-powered devices.\n\nThis is an interesting move for the Raspberry Pi Foundation as it can go down this path and iterate on its own chip design with more powerful variants. It provides two main advantages \u2014 the ability to control exactly what to put on board, and price.",
            "published_at": "2021-01-21T00:00:00"
        },
        {
            "authors": [],
            "title": "Smarter Kids and Fewer Sick Days Through Clean Air",
            "contents": "Smarter Kids and Fewer Sick Days Through Clean Air\n\nBy Peter Crona & Flora Crona\n\nDecember 27, 2020\n\nguides\n\nIf you read our other blog posts you might already know that around 90% of the people on our planet are breathing dirty air, and that an estimate of 7 million people die every year due to it [1]. You might also know that nearly half a million babies died in 2019 due to dirty air [2].\n\nThere is little doubt that dirty air is bad for both children and adults, but all this might be a bit abstract for a parent. This blog post will mainly look at how the air can impact the development of children and how the air quality relates to absenteeism in school, for both children and teachers.\n\nImpact on Children\u2019s Cognitive Development\n\nThere are multiple studies indicating that poor air quality can have an negative impact on a child\u2019s cognitive development.\n\nIn a study [3] carried out in Los Angeles and surrounding counties, it was found that for every increase 2.5\u00b5g/m3 PM 2.5 during the pre-teen and teen years, one performance IQ point was lost. Performance IQ is related to ones ability to solve new problems, and it is to a greater extent controlled by brain function, as opposed to something you can learn [4].\n\nIn a report by Unicef [5] it is mentioned how air pollution can impact memory and verbal and non-verbal IQ. Furthermore, it is mentioned that test scores and average grades (GPA) have been shown to be negatively impacted by air pollution exposure. Also prenatal exposure is discussed. A study they mention showed that prenatal exposure to air pollution resulted in a four-point drop in IQ by the age of 5. In addition to this study, also [6] shows that exposure during fetal life can have negative consequences on brain development. Air pollution seems to haunt us throughout our whole lives, from foetus to when old (especially if you are male) [7].\n\n[8]\n\nChildren attending schools with higher traffic-related air pollution had a smaller improvement in cognitive development.\n\nImportantly, these findings do not prove that traffic-related air pollution causes impairment of cognitive development. Rather, they suggest that the developing brain may be vulnerable to traffic-related air pollution well into middle childhood, a conclusion that has implications for the design of air pollution regulations and for the location of new schools. [8]\n\nIn [9] researchers looked at children when they were 9 months old and then 3 years old. They observed that exposure to high levels of NO 2 was associated with lower verbal ability at the age of 3. They also looked at indoor damp or condensation (which can be a hint for poor ventilation), and observed that it was associated with reduced verbal ability. Furthermore, they looked at exposure to second hand smoking, which perhaps unsurprisingly was also associated with lower verbal ability at the age of 3 years.\n\nResearchers have also looked at the commute to school and found that PM 2.5 and black carbon (BC) exposure was associated with diminished growth of working memory [10].\n\nThe findings seem fairly consistent - dirty air is harmful. We therefore believe it is worth paying attention to the air quality in the environments where our children spend a large portion of their early lives. We can not eliminate air pollution exposure, but at least it can be reduced thorugh awareness and taking actions if needed.\n\nAlso IQAir has looked at this topic and written an article Can clean air increase child IQ? . They found some of the same references as we, but also some others. We recommend you to have a look if you are curious about this topic.\n\nCleaner Air Leads to Fewer Sick Days\n\nPerhaps more concrete than air pollution\u2019s impact on cognitive development is how it affect the number of sick days. This can naturally also in turn impact a child\u2019s development, since day care (or school) is supposed to be a place where children both have fun and learn new things.\n\nOne study [11] found that with air purification non-attendance was decreased by 55% in a preschool. Another experiment, also in a Swedish preschool, showed a reduced sick-leave of at least 22.5% [12].\n\nA school in Finland also observed reduced sick leave [13], as well as improved concentration, after installing air purifiers. However, we couldn\u2019t find any numbers for how much.\n\nIn London, following the release of The Mayor\u2019s School Air Quality Audit Programme [14], a number of schools followed the recommendation to get air purifiers. 56 teachers were surveyed after getting air purifiers, and more than half of them reported having noticed a decrease in sick leave among themselves and children [15].\n\nThere is also some support for reduced absenteeism among teachers with better air quality, for instance [16] found such a connection. We will in the future look into absenteeism at workplaces and its relation to air quality. We suspect there will be more research about that than teachers specifically.\n\nThe V\u00e4stra G\u00f6taland Regional Council in Sweden has observed large variations in sick leave among preschools and theorize that this might be due to differences in indoor air quality [17]. This is expanded on [18] in L\u00e4kartidningen, the official Journal of the Swedish medical association. The doctors point out that the indoor air often has more particles than the outdoor air (in Sweden), both because of indoor activities and that small particles from traffic get in. Children are said to be especially vulnerable to air pollution. The doctors theorize that indoor pollution act as an additional burden on children\u2019s immune system, which risks making it less able to cope with viruses.\n\nImportant to note is that many of the above references have links to companies with commercial interests. Sadly, it is hard to find studies without any ties to industry. We found at least two double-blind studies [19] [20] supporting health benefits of air pollution reduction through the usage of air purifiers though. Note that also the European Commission did a research project called SINPHONIE looking at improving air quality in schools and kindergartens. See the final report for a deep dive into this topic. They also cover absenteeism, and on top of [11], they reference Association between substandard classroom ventilation rates and students\u2019 academic achievement, Indoor air microbes and respiratory symptoms of children in moisture damaged and reference schools and The Impact of School Building Conditions on Student Absenteeism in Upstate New York. All in support for that there\u2019s a link between air quality and absenteeism.\n\nConclusion\n\nWe believe there\u2019s enough supportive evidence for it being worth to make sure that the indoor air quality is good (below WHO limits), and if not, take action. We\u2019ll try to write a new post in the future more focused on what one can do, but meanwhile, some suggestions:\n\nEspecially if you are pregnant, have a baby or young child, pay attention to outdoor air pollution and consider an air purifier if pollution is high. It appears that air pollution may be more harmful the younger the child is.\n\nConsider asking your preschool to get an air purifier if it is located close to heavy traffic or the air quality seems low. Some preschools are already aware of this, for instance Eduwings have a strategy in place. Sadly it appears that mainly private expensive preschools are using air purifiers, at least in China [21].\n\nReduce indoor activities that can lead to increased air pollution. For instance high temperature cooking, usage of candles or other indoor combustion. A well maintained kitchen hood and ventilation system can mitigate air pollution build-up. But you can also just open your window when cooking with high temperature.\n\nPrefer walking in the forest or through a park rather than along roads with high traffic.\n\nInstall an app, such as IQAir\u2019s Air Visual. It can help you to know when air pollution is high, so that you can reduce air pollution exposure. For instance by staying indoors the worst days, preferably in a room with an air purifier. Or at least avoiding heavy outdoor exercise.\n\nMinimize second hand smoking, especially if you are pregnant, have a baby or young children.\n\nWe hope you enjoyed the article. As always, reach out to your doctor or other official services if you have any concerns about your or your families health.\n\nReferences"
        },
        {
            "authors": [],
            "title": "Changes in Tcl",
            "contents": ""
        },
        {
            "authors": [],
            "title": "",
            "contents": "JavaScript is not available.\n\nWe\u2019ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.\n\nHelp Center"
        },
        {
            "authors": [],
            "title": "COVID-19 Vaccine Availability",
            "contents": "This crowdsourced website tracks where you can get vaccinated in California\n\n\"[M]ore than 200 volunteers for VaccinateCA have been calling hospitals, pharmacies and other medical centers to compile information about each California county\u2019s vaccination availability. [VaccinateCA] appears to be the most complete resource currently available to eligible Californians...\""
        },
        {
            "authors": [],
            "title": "We Are Not Special \u2022 Hillel Wayne",
            "contents": "This is part two of the crossover project. Part 1 is here.\n\nNo one thinks about moving the starting or ending point of the bridge midway through construction. -Justin Cave I had to move a bridge. -Anonymous\n\nCarl worked as a mechanical verification engineer: he tested oil rigs to see how much they vibrated. Humans work and live on oil rigs for long stretches of time, and if they vibrate too much it can be impossible to sleep. While rigs are designed to stay below the threshold, the design and the final version often diverge.\n\nCurrently, he was explaining how they find oil. After drilling a hole, he said, \u201cyou might also want to add other additives, like acid or hazelnut shells, but also-\u201d\n\nI stopped him. \u201cHazelnut shells?\u201d\n\nHe smiled. \u201cYour oil reservoir is not like a balloon full of oil. It\u2019s more like a porous structure in the rock.\u201d When drilling in, you might hit a sudden loss of pressure in the pipe. This is extremely dangerous as you don\u2019t know if you\u2019ve broken into the open ocean or have just hit a local void. In the latter case, digging further and suddenly hitting a high pressure area could destroy the pipe. By pumping in hazelnut shells they can gradually fill in small voids and test if they are still in the structure, gradually equalizing the pressure if they are.\n\nAccording to Carl, oil companies are the biggest purchasers of hazelnut shells in Norway.\n\nDebates about traditional versus software engineering revolve around what makes us different. The clich\u00e9s here come from a different camp than the \u201cwhat makes an engineer\u201d clich\u00e9s. The ways people try to define \u201cengineering\u201d often work to marginalize software. Differences like \u201cwe don\u2019t have licenses\u201d or \u201cwe\u2019re not rigorous\u201d are ways of saying that software is less prestigious, less \u201cgood\u201d than traditional engineering. As we saw in the last essay, most of these don\u2019t hold up, and the crossover engineers think the two jobs are much closer in nature than people who have only done one.\n\nWhen people talk about the fundamental differences, in contrast, they\u2019re usually not aiming to make delegitimize software development. Rather, the implicit emphasis is on making software \u201cspecial\u201d. Making software something that can\u2019t be understood by the narrow lens of engineering.\n\nI see this as a defense mechanism. If software is so different from trad, then it\u2019s okay for us to \u201cnot be engineers\u201d. We can say that, sure, we aren\u2019t planning upfront, but that\u2019s because software requirements change so much faster than everywhere else. We don\u2019t apply engineering because we shouldn\u2019t be applying engineering. A good example of this is the NoEstimates movement: because estimates are difficult to make in software, supposedly unlike trad engineering, we should do away with them entirely.\n\nThere\u2019s just one problem with this: as software engineers, we deeply misunderstand the nature of trad engineering. We are not special. Almost everything we think is unique about software appears in every other field of engineering. The upside of this is that\u2014well, we are not special. Almost everything we think is awful about software is something everyone else struggles with, too.\n\nThe claimed differences\n\nThere\u2019s a fallacy in comparing \u201ctrad\u201d, which is an umbrella of different engineering disciplines, to software. Most arguments about engineering don\u2019t go much beyond \u201csoftware vs engineering\u201d, or conflate \u201cengineering\u201d with civil engineering. All of the subfields are different, and the qualities of one don\u2019t reflect on the qualities of others. After a few interviews, I settled on a set of \u201cuniversal\u201d differences to discuss:\n\nTraditional engineering is best done in a Waterfall style, while software is best done in an Agile one. Trad engineering is very predictable, while software is very unpredictable. Engineering is mostly about manufacture, while code is mostly about design, because \u201cthe code is the design\u201d. Trad engineering is much more rigorous than software engineering is. Software moves much faster than traditional engineering does.\n\nThese aren\u2019t all wrong. As we\u2019ll see later, there are absolutely some differences between software and trad. But the majority of the differences are \u201cwrong\u201d, or at least lacking critical nuance.\n\nTraditional is Waterfall, Software is Agile\n\nIf there\u2019s one thing we think is uniquely software, it\u2019s Agile.\n\nAs it\u2019s told, Agile was a rejection of \u201cWaterfall\u201d, the old paradigm Winston Royce invented in 1970. Royce came up with the Waterfall model to mimic how \u201creal\u201d engineers built buildings. Waterfall says that you should do everything in a strict order, only progressing to the next stage of development when the current stage is completed. You only develop after you complete design, only test after you finish development, etc. This works for \u201creal\u201d engineering but utterly fails for software, where requirements change and often the customer doesn\u2019t know what they want before you build it. So this is rejected by the Agile Manifesto in 2001, and everybody lived happily ever after.\n\nOf course, this story is more fiction than fact. While Waterfall was a dominant model for some time, it was never quite as strict as we think it was today. Nor was it all that ubiquitous: most developers in the 70s and 80s were either working with an ad hoc plan or working in one of the many, many, many \u201cincremental\u201d models that were in fashion, like the Spiral Model and the V Model. Agile wasn\u2019t a radical shift as much as a natural consequence of the trends at the time.\n\nBut all that is tangential to the core claim: software engineering is Agile, while trad engineering is Waterfall. And this, unsurprisingly, is a significant oversimplification.\n\nIt\u2019s true that traditional engineers do a lot more upfront design and spend more time in dedicated testing than software engineers do. But this doesn\u2019t mean they have Waterfall level rigidity, nor does it mean that our Agile is alien to them. Rather, spending a lot of time in phases is a natural consequence of the economic model. When iterations are longer and more expensive it makes more sense to spend more time planning them out. But people use \u201cdesign\u201d and \u201cimplement\u201d in very different ways. When a civil engineer does a scale model, is that design or implementation? When an automotive engineer makes a full clay replica of a car to test its aesthetics and aerodynamics, is that design or implementation?\n\nWhen it comes to things like making circuit boards, it\u2019s pretty common for things not to work the first time. You have to do it again, and you have to send it out to the factory and do it again. You know it costs you another however many \u00a31000 and another two weeks on the schedule. -Mike (mechanical)\n\nWe see Agile-like innovations in every industry. Many tunnels today are built with Austrian Tunneling, which relies on iterative development and lots of room for improvisation. The Handbook of Industrial Engineering emphasises cross-collaboration and rapid client feedback. Even the most Waterfall form of engineering, civil engineering, gradually shifts to a more Agile-like process once construction starts. People need open communication and adaptation to deal with on-the-ground issues in building.\n\nWhy do we think software needs Agile and not Waterfall? Unpredictability. The Waterfall model only makes sense if we can accurately predict and estimate the obstacles inherent in our projects. Because software is unpredictable, the argument goes, we can\u2019t use Waterfall. It could be that halfway through planning, we decide we need to start over. Is it any different for engineers? Is their work any more predictable, more certain than ours?\n\n\u201cSoftware is More Unpredictable\u201d\n\n*Laughter* -Dawn, Matt, Steve, Mike, Mat, Another Matt\n\nPart of this misconception comes from us seeing the results of the engineering process, not the process itself. We don\u2019t see the friction, the overruns, the delays that happen because someone built the wall one inch to the left, or when a critical supplier goes out of business. To assume that software is uniquely unpredictable is a special kind of arrogance.\n\nAnother part comes from how fast software seems to change. It feels like every year or two there is a new dominant framework or language that everybody\u2019s trying to switch to. We expect that traditional engineering doesn\u2019t have the same constant churn of tooling and paradigm. To an extent, it doesn\u2019t. But less churn is not the same as no churn, and there are many other ways that things can become unpredictable. One chip designer, Steve, found this especially funny:\n\nIn the environment of software world, people are thinking \u2018what\u2019s the new JavaScript bundler of the month.\u2019 In the hardware world, it\u2019s \u2018what can the silicon fab people do for us this month.\u2019 If the foundry has new machinery to create chips, your plans change. Not as fast as libraries do, but still pretty fast. -Steve (electrical)\n\nThere are too many anecdotes to go into them all. Territory claims changing in the middle of construction, hardened procedures suddenly and permanently failing, new discoveries well into development. One person talked about how frustrating it is to start work on a bridge foundation, only to find that this particular soil freezes in a weird way that makes it liquefy too much in an earthquake. Back to the drawing board.\n\n\u201cThe Code is the Design\u201d\n\n[Code is design] was a reaction to the people who think \u201cOh, if we just build a beautiful model in UML and then autogenerate all the code from UML then everything will be fine.\u201d\n\nThat\u2019s from Nick Coghlan. I was surprised when he reached out for an interview; I knew about him as a core CPython developer. Before that, though, he was a systems integration engineer for Boeing. \u201cThe diplomatic style of system architecture\u201d, he called it. Boeing would have multiple independent systems- in his example, an airplane, air traffic control, and an antenna array. He had to work with all three teams to make sure they built compatible interfaces. Integration work, essentially. He saw \u201csoftware is design\u201d as the fundamental difference between his old job and his current one.\n\nOthers weren\u2019t so sure. \u201cFrom my view,\u201d said one, \u201cit was all design\u201d, from the first schematics of the CPU to the final chips rolling out of the foundry. All of their time was spent on design, just like software. Construction was the easy part: just hand the design over to a foundry and get back completed chips.\n\nOf course, the chips will come back flawed, which means a change in design. Design and construction aren\u2019t nearly as disjoint as we software engineers think. Many of the mechanical engineers felt this strongly. The constructed product might show issues in the design, or it might show issues in the circumstances of construction. Mike, one mechanical engineer, called it \u201cfettling\u201d. Tweaking the design to deal with the slight imperfections of the construction process. Every construction changes the design, which changes the construction.\n\nBeyond that, there\u2019s another problem: \u201cdesign\u201d is not well specified. Are we talking about the architectural overview? The formal specification? The detailed blueprints? If the code is the design then what\u2019s the formal specification of our code? For complex projects there are a lot of different kinds of design at a lot of different levels of detail. If you look at bridge plans, you see there are many layers of fractal detail.\n\nI think this is another case where what we mean by design and construction really vary between different kinds of engineering. Yes, in civil engineering the construction phase takes the most time and money, but that\u2019s civil engineering. And only certain kinds of civil engineering, too, the \u201cbridges and buildings\u201d work. The same kind of work on which we base our stereotypes.\n\nI find myself having to explain that there are many aspects of civil engineering that you wouldn\u2019t specifically consider. [\u2026] I tell people it refers to anything that you need to build the city. -Jen (civil)\n\nRigor\n\nThere\u2019s like a 1,000,000 million times more checks and balances in software than in traditional engineering. [\u2026] Whenever someone\u2019s tweeting about, like Excel horror stories *laughter* I have amazing Excel horror stories [\u2026] there\u2019s days that I am shocked skyscrapers don\u2019t fall over daily and planes don\u2019t crash. -Mat\n\nPeople say that trad engineering is a lot more rigorous than software. Trad engineers reason carefully from first principles rather than copy-paste. This is usually presented as a problem with software that we need to change to become \u201creal\u201d engineering. It\u2019s also wrong.\n\nFirst of all, any less rigor we have isn\u2019t entirely cultural, because the essential nature of software makes it a viable tradeoff. Nick explained it as a difference in \u201cvalidating assumptions\u201d: \u201cyou reach a point with the easiest way to check if your assumption is right is to just go in and do it.\u201d Software lets us gather empirical information more easily, which itself is a source of rigour.\n\nBut statements about rigor are predicated on the end results of software being different, that trad products are more coherent and less slapdash. That\u2019s not true. For example, we keep much better records and use more comprehensive verification. I heard plenty of horror stories of critical information stored in Excel sheets and old filing cabinets, growing obsolete and corrupt. Plenty would kill to get the same kind of automated testing we treat as a given.\n\nYou can always add more brackets. -Carl\n\nThe Real Differences\n\nSo how is software different from everything else? A few ways.\n\nConsistency\n\nSoftware is entirely synthesized. It\u2019s bounded entirely by logic. It doesn\u2019t wear out like a spring, right? It just does what it\u2019s supposed to do. The only thing that can actually go wrong is the specifications. The software was bad. -Nathan\n\nSoftware is far more consistent than any other kind of engineering. We usually think of software as a giant mess, a haze of interdependencies and incompatible hardware, but we actually have it pretty nice. If I give you a sorting function, you can expect it to sort. You do not expect it to sort a given list of (nonpathological) numbers only 95% of the time. That would be ridiculous.\n\nBy contrast, let\u2019s look at physical materials. In electrical engineering, a core component is the resistor: a wire that reduces the flow of current. Resistors are measured in ohms, and commodity off-the-shelf resistors can range from a single ohm to hundreds of millions of ohms. Many commodity resistors follow a color chart to make identifying the resistance easy:\n\nSo a band with green, blue, red will have 5600 ohms. Note the last band, though: that specifies the tolerance. The color code only describes the theoretical resistance: if the tolerance band is gold, that means the resistor can vary by up to 5%. If you have a batch of 100 of these resistors, some will be 5320 ohms, some will be 5880 ohms, and the only way to know which is which is to test each one individually. And this is to say nothing about wear and tear, or how resistance varies with temperature, etc.\n\nThis is the case of all physical materials. One of my favorite discoveries on the internet, well before I started this project, was the Fastenal labs page. Fastenal makes screws. One of their pages warns against using stainless steel screws on an aluminum plate.\n\nSoftware engineering has nothing on that.\n\nVelocity\n\nThe code is the spec for how the hardware should be running. In traditional engineering, we would have to do all that same work to share the spec, but then we have to wait and, like, let the fab or the machine shop finish building the damn thing. And then we\u2019d have to sit there and install it either ourselves or hire somebody. And then we\u2019d have to run testing on it for several weeks to see if it worked or not. -Matt (chemical)\n\nThis is irrefutable. We can change software much faster than anybody else can change their systems. Several engineers told me that changes had a quantifiable price tag: every time you needed to adjust something, you knew you were taking $5000 from of the budget. Whereas I can change some code and run all the tests in seconds.\n\nThe closest I found to this in any other field was chemical engineering. \u201cI\u2019d go in the morning and look at what went wrong last night,\u201d said Raja, a former chemical engineer. Something like a one minute turnover would be unheard of, and that\u2019s already in the fastest field of non-software engineering.\n\nThis is the nature of our material. And it\u2019s also why other engineering fields are progressively using more software by creating these design tools and simulations, engineers can prototype their ideas with software before implementing them.\n\nThere is also a darker side to this. Because software can iterate faster than trad, trad often relies on software to compensate for problems in physical equipment.\n\nThe software engineer is being squeezed by the rest of the team. They\u2019re usually asked to save everyone\u2019s bacon. If something is not quite working in the electronics or mechanics, often you can work around it with a software kludge. -Mike\n\nAnd sometimes this reliance has catastrophic consequences. In 2019 over 300 people died in two 737 MAX airplane crashes. Investigation pinned this to a bug in the \u201cManeuvering Characteristics Augmentation System\u201d (MCAS), one of the automated flight control systems. But Boeing only added MCAS in the first place to compensate for late-discovered issues with the plane\u2019s aerodynamic profile. Rather than fix a trad issue with trad engineering, Boeing opted for the software kludge, and then people died.\n\nConstraints\n\nThey have a [chip] time budget, you have a time budget, you say \u2018I can\u2019t quite make it. Do you have a little slop there? Can I get a fraction of a nanosecond?\u2019 -Steve\n\nOne implicit undercurrent in all the interviews was the notion of constraint. There are hard physical limits that their products needed to obey. It has to be light enough, or strong enough, or resistant enough, or run cool enough. There is a known quantity they have to maximize or minimize.\n\nConstraints are present in software, too. Code must fit in the memory of an embedded hardware device, the sensor needs a response in exactly 10 cycles, the API must stay under the rate limit. But constraints in software tend to be soft constraints. It is bad to go over them and the more we go over the worse it gets. We can slightly fudge the line if doing so will give us other benefits, like faster development time or simpler algorithms. In traditional engineering, most constraints are hard constraints. If the area of your box is a bit too wide then it won\u2019t fit in the door.\n\nSometimes this leads to unusual solutions. One time Carl\u2019s team had to install a screw conveyor in an oil rig, then discovered it was just a couple inches too tall for the room. They couldn\u2019t shrink the equipment, and they couldn\u2019t raise the ceiling, since there were another four floors above it. The team\u2019s solution? Cut a hole in the ceiling, put the equipment in, then put a box around the hole on the next floor so that nobody trips over it. Now that change is permanently part of the rig\u2019s structure, something that has to be accommodated in every future change forever. And that leads to another difference: software engineers can undo their kludges. Trad engineers cannot.\n\nDo the differences make us special?\n\nWhile there are many ways we are different, there\u2019s a difference between being \u201cdifferent\u201d and being \u201cspecial\u201d. Yes, mechanical and chemical engineers don\u2019t have to deal with the same security concerns we do. They also don\u2019t have to deal with weather patterns to the same degree that civil engineers need to, and none of those three need to deal with the problems inherent in chemical engineering. Every field of engineering has unique challenges and software is no different.\n\nBut they are all much more similar than they are different. Every field values upfront, abstract thinking, tidy work, and a good kludge in just the right place. Every field faces shifting requirements and unknown unknowns. Every field is siloed from the others: we know as little about the work of mechanical engineers as chemical engineers do. Many times in my interviews, people asked what the other fields of engineering were like. Nobody ever gets a chance to leave their bubble.\n\nAnd it\u2019s a good thing that software isn\u2019t special. It means that we can learn a lot on how to make software better from these other fields. And it means that they have a lot to learn from us. Because surprisingly, there are some ways that we are better at engineering than traditional engineers. Next time, we\u2019ll cover what we can learn and what we can teach.\n\nPart three, What Engineering Can Teach (and Learn from) Us, will be posted on Friday. You can check back here, subscribe to RSS, or join my newsletter to be notified. You can also follow me on twitter.\n\nThanks to Glenn Vanderburg , Chelsea Troy , Will Craft , and Dan Luu for feedback, and to all of the engineers whom I interviewed.",
            "published_at": "2021-01-20T00:00:00+00:00"
        },
        {
            "authors": [],
            "title": "Bitcoin Slumps to $31K on Sell-Off in US and Europe",
            "contents": "Bitcoin\u2019s price took another hit Thursday morning, dropping close to $31,000 since markets opened in Europe and the U.S.. Investors rushed to take short-term profit, concerned about when \u2013 or if \u2013 another wave of new buyers would come into the market soon.\n\nAt the press time, bitcoin\u2018s price was at $31,910.61, down 6.61% in the past 24 hours, according to CoinDesk 20. In the U.S., at around 9:40 a.m. ET (14:40 UTC), bitcoin\u2019s price was as low as $31,006.59.\n\nOne indicator showing the severity of the U.S. and European sell-offs is the so-called \u201cCoinbase premium,\u201d the gap between Coinbase\u2019s BTC/USD pair and Binance\u2019s BTC/USDT pair involving the tether stablecoin, according to South Korea-based on-chain data site CryptoQuant. The number dropped to as low as -$212.79 at 4:17 a.m. ET (09:17 UTC) on Thursday.\n\nCoinbase premium Source: CryptoQuant\n\n\u201cCoinbase naturally has to trade higher than Binance by, like, 20 basis points, I believe, due to the minor tether price difference,\u201d Ki Young Ju, the chief executive at CryptoQuant, told CoinDesk. \u201cSo if it is actually trading at the same price or even lower, it would mean really, really, very super-bearish.\u201d\n\nTether is the largest stablecoin in cryptocurrency. Trading close to \u2013 but not exactly \u2013 at par with the U.S. dollar that is supposed to back it, tether is the popular method for those on Binance and other Asian exchanges to get into and out of bitcoin.\n\nEven though the premium fell in deep red territory during Asian trading hours on Thursday, it does not mean traders in the U.S. were not involved in the latest correction.\n\n\u201cU.S. traders have been attempting to trade in anticipation of lower Asian sessions,\u201d said John Todaro, director of institutional research at cryptocurrency analysis firm TradeBlock. \u201cSo depending on the times this premium tightening occurred in the day, it could be an indicator of U.S. selling ahead of that.\u201d\n\nSeveral factors seem to have triggered the latest bitcoin sell-off: the unwinding of leverage, especially in Asia; concerns that fewer buyers are coming into the market; and uncertainty about policies on cryptocurrencies from newly inaugurated President Joe Biden\u2019s administration, according to analysts and traders.\n\n\u201cWe saw some selling from institutions, but not significant,\u201d Chris Thomas, head of digital assets at Geneva-based Swissquote bank, told CoinDesk. \u201cThe trigger was Asian leveraged positions late in Asian hours. They move the market quite a lot because of the leverage.\u201d\n\nData from coinalyze.net shows that a reasonable amount of leverage on major derivatives exchanges. Source: coinalyze\n\nOn the technical side, traders said the market has broken the price uptrend since Dec. 11 and is looking at a new support level in the $29,000-30,000 range.\n\nCoinDesk's Bitcoin Price Index Source: CoinDesk 20\n\n\u201dThe next support level down is the 61.8% Fibonnacci retracement at $26,700,\u201d Jean-Marc Bonnefous, partner at investment firm Tellurian Capital, told CoinDesk. \u201cThat is, if the new investors\u2019 allocations do not come in as widely expected to buy the much-awaited dip.\u201d\n\nAs the number of traditional investors and traders entering the bitcoin market increased in recent months, the price movement has become more technical-driven, according to Bonnefous. Before it was mainly affected by bitcoin\u2019s supply and demand, he said.\n\nBitcoin\u2019s price is below its 10-hour and 50-hour moving averages on the hourly chart, a shorter-term bearish signal for market technicians.\n\nBitstamp's BTC/USD pair (x) Source: TradingView\n\nA few institutions, including some hedge funds, could be using the uncertainty in the market as an excuse to take some profit, Todaro added. Many of these traditional financial players in the U.S. and Europe came to the market prior to the steeper portion of bitcoin\u2019s run-up and are thus more likely to be at higher profit levels given current prices.\n\nBut some potential investors may be spooked by not knowing what the Biden Administration will do regarding bitcoin and cryptocurrencies.",
            "published_at": "2021-01-21T17:58:18+00:00"
        },
        {
            "authors": [],
            "title": "Efficient custom shapes in QtQuick with Rust",
            "contents": "One of the advantages of QWidgets when building a Qt application is the ability to build in a simple way custom widgets with the QPainter API. This gives the Qt developer almost total freedom to implement complex geometries for their widgets.\n\nOn the other hands, QML contains by default only rectangles. These rectangles can change the radius to create circles and rounded rectangles, but more complex shapes are more complicated.\n\nThe current state of custom geometry in QtQuick\n\nFortunally, the Qt API provides multiple ways to implement custom shapes, that depending on the needs might be enough.\n\nThere is the Canvas API using the same API as the canvas API on the web but in QML. It\u2019s easy to use but very slow and I wouldn\u2019t recommend it.\n\nInstead of the Canvas API, from the QML side, there is the QtQuick Shapes module. This module allows creating more complex shapes directly from the QML with a straightforward declarative API. In many cases, this is good enough for the application developer but this module doesn\u2019t offer a public C++ API.\n\nIf you need more controls, using C++ will be required to implement custom QQuickItem. Unfortunately drawing on the GPU using QQuickItem is more complex than the QPainter API. You can\u2019t just use commands like drawRect , but will need to convert all your shapes in triangles first. This involves a lot of maths like it can be seen in the example from the official documentation or from the KDAB tutorial (Efficient custom shapes in Qt Quick).\n\nA QPainer way is also available with QQuickPaintedItem, but it is slow because it renders your shape in a textured rectangle in the Scene Graph.\n\nThe Rusty way\n\nWhat if we could transform arbitrary shapes into triangles? We would get a high level API but still get great performance. This process is called tessellation and there are a few libraries that implement it. For example in C++, we have Skia and CGAL. Unfortunatelly, both aren\u2019t easy to use, so I decided to look at the Rust library ecosystem and in particular at Lyon, which was designed with performance and compliance to the SVG standard in mind since the goal is to use it in Servo in the future.\n\nLyon doesn\u2019t have any C++ bindings but I got inspired by the recent blog post from Jonah and I need to say the experience of writing bindings was a breeze.\n\nThe first step was creating wrapper struct s around the Lyon primitives. LyonPoint , LyonGeometry and LyonBuilder will later be directly usable from the C++ side.\n\n#[cxx::bridge] mod ffi { pub struct LyonPoint { x : f32 , y : f32 , } pub struct LyonVector { x : f32 , y : f32 , } pub struct LyonGeometry { vertices : Vec < LyonPoint > , indices : Vec < u16 > , } extern \"Rust\" { type LyonBuilder ; fn new_builder () -> Box < LyonBuilder > ; fn move_to ( self : & mut LyonBuilder , point : & LyonPoint ); fn line_to ( self : & mut LyonBuilder , point : & LyonPoint ); fn relative_move_to ( self : & mut LyonBuilder , to : LyonVector ); fn close ( self : & mut LyonBuilder ); fn quadratic_bezier_to ( self : & mut LyonBuilder , ctrl : & LyonPoint , to : & LyonPoint ); fn cubic_bezier_to ( self : & mut LyonBuilder , ctrl1 : & LyonPoint , ctrl2 : & LyonPoint , to : & LyonPoint ); fn build_fill ( builder : Box < LyonBuilder > ) -> LyonGeometry ; fn build_stroke ( builder : Box < LyonBuilder > ) -> LyonGeometry ; } }\n\nWe then need to define the methods we declared above. These are all trivial to implement since they are just wrapping the Lyon API.\n\nuse ffi : { LyonPoint , LyonVector , LyonGeometry }; // Create a wrapper arround Lyon svg path. This struct is opaque from // the C++ side so we won't be able to access the internal object, but // we still can call the methods on it. pub struct LyonBuilder { builder : WithSvg < Builder > , } // Implement wrapping methods impl LyonBuilder { fn close ( & mut self ) { self . builder . close (); } fn move_to ( & mut self , to : & LyonPoint ) { self . builder . move_to ( point ( to . x , to . y )); } fn line_to ( & mut self , to : & LyonPoint ) { self . builder . line_to ( point ( to . x , to . y )); } fn quadratic_bezier_to ( & mut self , ctrl : & LyonPoint , to : & LyonPoint ) { self . builder . quadratic_bezier_to ( point ( ctrl . x , ctrl . y ), point ( to . x , to . y )); } ... } // Lyon Builder constructor pub fn new_builder () -> Box < LyonBuilder > { return Box :: new ( LyonBuilder { builder : Path :: builder (). with_svg () }) }\n\nThe next step was to add the build_fill that will transform the SVG path instructions into a set of vertices and indices. These vertices and indices will be directly available from the C++ side. This is extremely handy since this can be directly fed into the QSGGeometry painting method.\n\npub fn build_fill ( builder : Box < LyonBuilder > ) -> LyonGeometry { let mut buffers : VertexBuffers < Point , u16 > = VertexBuffers :: new (); { let mut vertex_builder = simple_builder ( & mut buffers ); // Create the tessellator. let mut tessellator = FillTessellator :: new (); let path = builder . builder . build (); // Compute the tessellation. let result = tessellator . tessellate_path ( & path , & FillOptions :: tolerance ( 0.01 ), & mut vertex_builder ); assert ! ( result . is_ok ()); } LyonGeometry { // convert_points transform lyon::point to our LyonPoint wrapper vertices : convert_points ( buffers . vertices ), indices : buffers . indices , } }\n\nAnd we are almost done with the Rust side, we still need to create the cargo and corrosion configuration, but I won\u2019t go into details in this post. You can look at how it was done in this pet project.\n\nUsing the generated bindings\n\nTo make it easy to store and manipulate the path, I create a simple abstraction to the various SVG path instructions.\n\n#include <QList> #include <variant> #include <tessellation.rs.h> /// Move to the point without drawing a line. struct MoveTo { /// The destination. LyonPoint to ; }; /// Drawe a line to a specific point. struct LineTo { /// The destination. LyonPoint to ; }; /// Draw a cubic bezier curve to the point. struct CubicBezierTo { /// First control point. LyonPoint ctrl1 ; /// Second control point. LyonPoint ctrl2 ; /// The destination. LyonPoint to ; }; /// Close a path. struct Close {}; /// SVG conform path commands using PathSection = std :: variant < MoveTo , LineTo , CubicBezierTo , Close > ; template < class ... Ts > struct overloaded : Ts ... { using Ts :: operator ()...; }; template < class ... Ts > overloaded ( Ts ...) -> overloaded < Ts ... > ; /// The SVG path data. It contains a list of instruction (move to, line to, ...). using PathData = QList < PathSection > ;\n\nNow let finally use Lyon to generate the geometry primitives. This will need to be called every time the list of commands is updated. It\u2019s using the command abstraction, I build previously, but this could directly call the LyonBuilder methods.\n\nconst auto commands << MoveTo { LyonPoint { 0.0 , 0.0 } } << LineTo { LyonPoint { 0.0 , 40.0 } } << LineTo { LyonPoint { 40.0 , 40.0 } } << CubicBezierTo { LyonPoint { 70.0 , 40.0 }, LyonPoint { 70.0 , 0.0 }, LyonPoint { 50.0 , 20.0 } } << LineTo { LyonPoint { 40.0 , 0.0 } } << Close {}; auto lyonBuilder = new_builder (); for ( const auto & command : commands ) { std :: visit ( overloaded { [ & lyonBuilder ]( MoveTo moveTo ) { lyonBuilder -> move_to ( moveTo . to ); }, [ & lyonBuilder ]( LineTo lineTo ) { lyonBuilder -> line_to ( lineTo . to ); }, [ & lyonBuilder ]( CubicBezierTo cubicBezierTo ) { lyonBuilder -> cubic_bezier_to ( cubicBezierTo . ctrl1 , cubicBezierTo . ctrl2 , cubicBezierTo . to ); }, [ & lyonBuilder ]( Close ) { lyonBuilder -> close (); }, }, command ); } auto m_geometry = build_fill ( std :: move ( lyonBuilder ));\n\nAnd finally here is our updatePaintNode method. It\u2019s using the GL_TRIANGLES drawing mode and the vertices and indices are copied directly from the geometry Lyon gave us.\n\nQSGNode * PathItem :: updatePaintNode ( QSGNode * oldNode , UpdatePaintNodeData * ) { QSGGeometryNode * node = nullptr ; QSGGeometry * geometry = nullptr ; if ( ! oldNode ) { node = new QSGGeometryNode ; geometry = new QSGGeometry ( QSGGeometry :: defaultAttributes_Point2D (), m_geometry . vertices . size (), m_geometry . indices . size ()); geometry -> setIndexDataPattern ( QSGGeometry :: StaticPattern ); geometry -> setDrawingMode ( GL_TRIANGLES ); node -> setGeometry ( geometry ); node -> setFlag ( QSGNode :: OwnsGeometry ); QSGFlatColorMaterial * material = new QSGFlatColorMaterial ; material -> setColor ( QColor ( 255 , 0 , 0 )); node -> setMaterial ( material ); node -> setFlag ( QSGNode :: OwnsMaterial ); } else { node = static_cast < QSGGeometryNode *> ( oldNode ); geometry = node -> geometry (); geometry -> allocate ( m_geometry . vertices . size (), m_geometry . indices . size ()); } QSGGeometry :: Point2D * points = geometry -> vertexDataAsPoint2D (); std :: size_t i = 0 ; for ( const auto & vertice : m_geometry . vertices ) { points [ i ]. set ( vertice . x , vertice . y ); i ++ ; } quint16 * indices = geometry -> indexDataAsUShort (); i = 0 ; for ( const auto indice : m_geometry . indices ) { indices [ i ] = indice ; i ++ ; } node -> markDirty ( QSGNode :: DirtyGeometry ); return node ; }\n\nIt is only using Lyon SVG path rendering, but Lyon provides a lot more APIs. For example, there is an abstraction that allows to draw circle, ellipse, rounded rectangle and other basic geometric forms.\n\nThere is also the possibility to add custom attributes for texture coordinate or color coordinate. Depending on your need more part of the API could be wrapped and I might create a small library wrapping most of the API.\n\nCustom shape in action\n\nI used this technique in a new toy I\u2019m building. I\u2019m not sure where it is going, but I currently have this:",
            "published_at": "2021-01-20T00:00:00"
        },
        {
            "authors": [],
            "title": "swadey/LispSyntax.jl: lisp-like syntax in julia",
            "contents": "LispSyntax.jl: A clojure-like lisp syntax for julia\n\nThis package provides a lisp-to-julia syntax translator with convenience macros that let you do this:\n\nlisp \" (defn fib [a] (if (< a 2) a (+ (fib (- a 1)) (fib (- a 2))))) \" @test lisp \" (fib 30) \" == 832040 @test fib ( 30 ) == 832040\n\nLispSyntax.jl is implemented as an expression translator between lisp/clojure-like syntax and julia's AST. Julia's compiler, JIT and multiple-dispatch infrastructure is used for code generation and execution. Because of this, LispSyntax.jl is not really clojure or lisp in most meaningful ways. The semantics are entirely julia-based (which are very similar to scheme/lisp in many ways). The net result is that LispSyntax.jl is really an alternative S-expression-like syntax for julia, not an implemention of clojure or lisp.\n\nSpecial Forms\n\n(def symbol init)\n\n(quote form)\n\n(defn symbol [param*] expr*)\n\n(defmacro symbol [param*] expr*)\n\n(lambda [param*] expr*)\n\n(fn [param*] expr*)\n\n(let [binding*] expr*)\n\n(global symbol*)\n\n(while test expr*)\n\n(for [binding*] expr*)\n\n(import package*)\n\nNotable Differences\n\nSymbol names cannot have -, *, /, ? ... - Julia symbol naming is used for everything, as a result, Julia syntax restrictions are maintained in LispSyntax.jl .\n\n. Reference to global variables in function scopes - Julia requires declaration of global symbols that are referenced in function scope. Because of this functions need to declare which symbols are global. This is done via the special form (global symbol*) .\n\n. Binding forms not implemented - Clojure has very awesome destructuring binds that can used in most special forms requiring bindings (e.g. let , fn parameter lists, etc.). This is not currently implemented.\n\n, parameter lists, etc.). This is not currently implemented. Lack of loop/recur - Currently, this is not implemented. As with Clojure, julia does not currently support TCO, so something like this may be needed (but a macro-implementation of tail call rewriting may be more appropriate for julia).\n\nOptional typing - Currently not implemented.\n\nNamed functions are julia methods - For efficiency, functions defined with defn are translated to normal julia function expressions. This means the act as named lambdas in local scope.\n\nare translated to normal julia expressions. This means the act as named lambdas in local scope. Method definition - Also not currently implemented. If implemented it will probably not be a full implementation of Clojure's sophisticated dispatch system.\n\nMacros differences - Macros defined in LispSyntax.jl look like standard Lisp macros but because expressions are special objects in julia, S-expressions returned from macros require a special translation step to generate julia expression trees. The result is that LispSyntax.jl macros are directly translated into Julia macros and must be called via special syntax (e.g. (@macro expr) ). Macro hygiene follows the Julia approach of hygenic-by-default with explicit escaping using esc . This is the opposite of Clojure's macros which use explicit hygiene with specially named variables.\n\nlook like standard Lisp macros but because expressions are special objects in julia, S-expressions returned from macros require a special translation step to generate julia expression trees. The result is that macros are directly translated into Julia macros and must be called via special syntax (e.g. ). Macro hygiene follows the Julia approach of hygenic-by-default with explicit escaping using . This is the opposite of Clojure's macros which use explicit hygiene with specially named variables. Julia's string macro dispatch not supported (yet) - for macros like @r_str which in Julia can be called via r\"\" , it is currently necessary to call these via standard macro syntax: (@r_str \"string\")\n\nREPL Mode\n\nLispSyntax.jl provides a convenience REPL, alleviating one from having to type lisp\"( ... )\" for each top level expression. In order to use REPL mode, simply initialize it:\n\njulia > using LispSyntax julia > LispSyntax . init_repl () REPL mode Lisp Mode initialized. Press ) to enter and backspace to exit.\n\nAt this point, type ) , and you're ready to Lisp:\n\nj\u03bb> ( * 2 ( reduce + ( : 1 6 ))) 42 j\u03bb> ( defn fib [a] ( if ( < a 2 ) a ( + ( fib ( - a 1 )) ( fib ( - a 2 ))))) fib ( generic function with 1 method) j\u03bb> ( fib 10 ) 55\n\nTo return to the Julia prompt, simply type the backspace type or Ctrl-C . Once there, you'll still have access to the fuctions you defined:\n\njulia > fib fib (generic function with 1 method) julia > fib ( 10 ) 55\n\nYou may also create a customized REPL.\n\nTODO"
        },
        {
            "authors": [
                "Memo Akten"
            ],
            "title": "The Unreasonable Ecological Cost of #CryptoArt (Part 1).",
            "contents": "The NFT market is a scam\n\nI don\u2019t mean this in a philosophical, \u201cnothing is real, money is a scam\u201d sense. I mean it in a very practical sense, in that the blockchain as a technology simply does not provide any of the affordances that it is touted as providing. In particular, the many problems that plague more traditional \u2018real world\u2019 markets, still plague NFT markets. Except, a transaction using cryptocurrency is literally hundreds of thousands of times worse for the environment than a non-crypto exchange.\n\nThe blockchain does not provide scarcity or authenticity for NFT content\n\nAn NFT is simply a a long hexadecimal (base16) number (tokenId), such as: 0x41A322b28D0fF354040e2CbC676F0320d8c8850d/1\n\n(This is the very first NFT on SuperRare, by the very talented Robbie Barrat).\n\nThis is what you own when you buy an NFT. What is permanently etched into the records of the blockchain (in this case evaporating 6 Olympic sized swimming pools in the process), is that you own this number. And it is true that the blockchain guarantees uniqueness of this number, and the history of the records of who owns, and has previously owned \u2014 i.e. the provenance of\u2014 this number cannot be tampered with (currently).\n\nAlso written on the blockchain is an association between this tokenId and some metadata which further links to a URL which contains the \u2018original\u2019 file. (FWIW this is all intentionally out in the open and transparent. I did not do any \u2018hacking\u2019 to reveal this, just some digging).\n\nHowever, this association is non-exclusive.\n\nNot only is there nothing within the blockchain as a technology which can stop anybody from downloading this artwork, and re-selling it as their own. But the blockchain as a technology does not even prevent anybody (whether its the artist themselves, or somebody else) from creating new tokens that link straight to the exact same URL\u2074. There is nothing within the blockchain as a technology that provides exclusive access to the contents or URL of a work.\n\nIn other words, (a crypto-analogy if you will) I could sell someone the Brooklyn Bridge, and guarantee its authenticity by giving them a one of a kind, unique certificate. While it is true that there is indeed only one of that particular certificate (token) in the world, which they hold in their hands, I can still go and create other certificates (tokens) that link to the exact same object \u2014 the Brooklyn Bridge \u2014 and sell them on to others.\n\nOf course to do this would be immoral, if not illegal. But that does not prevent it from happening in the \u2018Real\u2019 Art Market, and it won\u2019t prevent it from happening in the CryptoArt / NFT Market.\n\nThe blockchain does not even provide artificial scarcity within the rules of its own imaginary game. It simply does not provide any scarcity or authenticity for NFT related content at all.\n\nAll that you really \u2018own\u2019, is the hexadecimal tokenId. I would not mind this, if the ecological cost of buying that hexadecimal tokenId was not 100s KgCO2 boiling on average 3 Olympic sized swimming pools.\n\nThe blockchain does not guarantee secondary market income\n\nIt is not uncommon for artists to request royalties from future sales of their work. There are established mechanisms in the worlds of music, film and literature which try to ensure that artists receive royalties (these are of course plagued with issues of their own). But in the fine-art world, no such established mechanisms exist (and this topic was brilliantly tackled by Caleb Larson in his 2009 piece \u201cA Tool to Deceive and Slaughter\u201d [19]).\n\nIt was proposed that the blockchain could address this and automate secondary sales royalties. However this is not the case. By switching platforms, collectors can resell NFTs without triggering the functionalities implemented in the original smart-contract \u2014 which is typically bound to the platform. The collector can then choose not to respect the original Terms & Conditions, and avoid paying the \u2018spoiled artists\u2019 [20].\n\nOf course to do so would be immoral, if not illegal. But again my point is, this happens in the \u2018Real\u2019 Art Market, and it happens in the CryptoArt / NFT Market. The blockchain as a technology does not prevent this from happening\u2075.\n\nThe myths of decentralization\n\nWhen people speak about the blockchain and NFTs, at some point they will inevitably speak about the wonders of decentralization, as an \u2018alternative\u2019 to The Old Centralized Ways of The Past, and an antidote to the horrors that this beget.\n\nThe blockchain is essentially a database, a list of records. And it is true that instead of this list of records being stored in one single place (or a few key places, all owned by the same entity), the database is instead duplicated \u2014 potentially in its entirety \u2014 to anybody who wishes to become part of the network. No one body can control the data, or the flow of data. No one body has the power to alter the records to their own favor. Anybody can download the entire history of the database \u2014 or alternatively, they may simply download enough of the data to verify each and every transaction or block. It is true that this is one of the greatest affordance that the blockchain offers.\n\nAnd in principle, this does sound wonderful. What the ingeniously idiotic\u00b2 approach of the (PoW\u00b9) blockchain has managed to do (apart from exploit humanity\u2019s greed and gluttonous craving for games of value and ownership, at the expense of the planet), is to provide a statistically and game-theoretically [23] sound solution to the problem of distributing a database among non-trusting agents.\n\nAnd while a decentralized database does have its merits, it is important to remember that decentralizing the database, does not decentralize power.\n\nWhile the underlying Technology concerning the blockchain might indeed appear to be decentralized, when we consider The Real World of Technology [10] \u2014 which takes into account the social, cultural and political context, structures and values in which Technology is designed, developed and deployed \u2014 it is impossible not to notice a trend towards re-centralization. Who would have thought \u00af\\_(\u30c4)_/\u00af\n\n\u2018Mining\u2019 and adding new blocks to the blockchain has reached stages where it requires ludicrous amounts of compute power. So around the world, enormous data-center-like mining-farms (not to be confused with mining-pools) are popping up, containing thousands of computers [11, 12, 13], dedicated to sitting around doing nothing but generating random numbers all day to mine blocks for the blockchain, and reap the rewards for their owners\u00b2.\n\nThis is further exacerbated by the fact that dedicated, improved hardware is continuously being developed specifically to mine for blocks. These ASICs as they are called, are so much more efficient at mining, that as new ASICs are released, from a competitive standpoint it becomes non-profitable to mine without them [14, 15]. Furthermore, the production and distribution of this hardware is very much centralized. (ASICs are produced in China, which is perhaps one of the reasons why 65% of Bitcoin is mined in China [24]. The second reason being the cheap cost of electricity).\n\nIn other words, the protocol of the blockchain might be decentralized, but the hardware required to mine it is most definitely not (interesting to note that Satoshi Nakamoto either could not foresee this happening, or chose to stay quiet about it).",
            "published_at": "2021-01-22T02:06:23.888000+00:00"
        },
        {
            "authors": [],
            "title": "Evolving a Curated Retro Game Collection & Fine-Tuning AV Setups",
            "contents": "Evolving a Curated Retro Game Collection & Fine-Tuning AV Setups\n\nRB Retro Collecting Interview 008\n\nCheck out Interviews #1 , #2, #3 , #4 , #5, #6 and #7\n\nBrandon, aka SuperSparkster on Instagram, is one of the retro collectors I most relate to personally. Everyone has their own collecting style and priorities, but Brandon and I both seem to align on curating a collection that is tailored to what makes them most excited. A curated collection not only requires a great amount of restraint to avoid purchasing too much that you aren\u2019t really going to enjoy, but also pruning certain pieces from your collection to keep it to the size you desire.\n\nA few years back, Brandon and I discussed his buying and selling techniques to build a strong collection while keeping his expenses down. He purchased large lots of games, kept the ones he wanted most for his collection and then sold or traded the others to reduce his overall investment. This recent interview was a great follow-up opportunity to see how his collection has been refined and also check in on how his setup has evolved as well.\n\nAs you\u2019ll see below, Brandon also takes his video displays, connections, and organization very seriously as well. I wanted to dig into his strategies and tips that can make a similar setup rather accessible for many in our audience. A side benefit of spending less on accumulating loads of games is that you can instead invest more on the equipment and environments that you will use to play those games.\n\nI hope you find this interview useful in evolving your collection and I\u2019d love to hear your feedback in the comments section below. Also, if you haven\u2019t read our first interview with Brandon, I recommend you check it out now.\n\nBasic Bio:\n\nName: Brandon / Supersparkster / 1Upgrade on Etsy\n\nBrandon / Supersparkster / 1Upgrade on Etsy Age: 36\n\n36 Collecting For: 20+ Years\n\nExpanding The Retro Gaming Setup\n\nSince we last talked in detail, you have made some upgrades to your main gaming setup\u2026 You added the second display \u2014 can you tell us about that and the thoughts that lead up to the decision.\n\nThat\u2019s correct\u2026since we last talked I went down the challenging and financially painful RGB rabbit hole. I came across a Sony PVM-2950Q locally for a good price, not really knowing what an impact it would have on my setup.\n\nNow a couple of years later I have two identical PVM-2950Q\u2019s, all systems RGB capable, SCART cables, switchers\u2026I honestly don\u2019t want to think about the money this ended up taking. All worth it though! So, the second display is turned 90 degrees for all TATE games in my setup. These mostly come from the Sega Saturn, Pi2SCART (for arcade emulation), and Xbox 360.\n\nSo you said \u201cnot really knowing what an impact it would have on my setup\u201d. Now you must have had some expectations aside from general online hype. What were you expecting going in and how did realty differ?\n\nI think the results of RGB was all I was focused on\u2026not really thinking about the economic ripple effect of the transition. It\u2019s getting a PVM, then RGB modding systems, buying the SCART cables, realizing they aren\u2019t shielded enough and buying better SCART cables, figuring out video switching solutions, etc. It\u2019s an absolute money pit, but I do not regret it.\n\nCan you tell us about the speakers on the setup as well?\n\nSure, the speakers are Sony SS-X7A, made for the PVM-295Q\u2019s. Normally these would be powered by the PVM, however, I have an external 2.1 amp that runs to these speakers, along with a Sony subwoofer I found for a cheap at a Goodwill. I have the right speaker attached with a zip tie to the bottom of the TATE monitor.\n\nTo make room for that extra display, you also now have a wider storage unit. The unit looks the same as your older one but just has one addition section to make it wider? Is this an IKEA mod and did you just build a newer unit? Did you have to do anything to reinforce for the display weight?\n\nYes, for the two displays I had to make a new cabinet. I wanted to retain the certain aspects of my earlier design, such as the LED lights and spaces for controllers\u2026but make some needed improvements.\n\nThe main cabinet is an 8-cube Threshold set from Target, nearly identical to the sets from IKEA. I purchased two of these units, knowing I would use the wood from the second one for all the modifications.\n\nI made extra shelves to hold the systems over their respective controllers, and a top tier for the Wii, Xbox 360, and gscartsw lite SCART switch (a 8-port dual-output SCART switch) (see other SCART switches on eBay).\n\nThe back of the unit is where I made the biggest improvement. I had a piece of white hardboard cut at Lowe\u2019s to the exact dimensions of the back of the cabinet, along with a piece of pegboard. I installed the pegboard to sit about an inch off the hardboard, allowing me to thread twist ties for cable management. My goal was really just to have every single cord possible off of the ground. These two boards do help with the overall stability of the cabinet and weight distribution.\n\nIn regards to weight, the most important thing was making sure each display was over a vertical piece (or pieces) running from top to bottom in the cabinet. This is the real key in making sure the frankly cheap particle board/cardboard filled material could hold the 200+ pounds of collective weight on the top.\n\nThat\u2019s awesome that you were thinking of the physical limitations and challenges!\n\nCan you talk a but about the little compartments in your storage unit for your controllers, and other things you tuck away in there?I bought an additional cube set from Target and used the wood to make the controller compartments. They are each around 4\u2033 from the base of the cube, so they are tall enough to store most controllers, lightguns, and joysticks.\n\nCable Management To Keep All Those Connections Organized\n\nCould you tell us more about your Cable Management goals, the process, and what lessons you learned along the way?\n\nWhen I did the initial install of systems and cables in this cabinet, I was a bit Jackson Pollock with my cable management. The pegboard and twist ties worked great, but man it was a mess. Just recently when incorporating some downscalers I decided to rework everything and tidy up\u2026ended up being way more work than I planned.\n\nI went in with a general sketch of where I thought the cables would run, but didn\u2019t account for every device and every plug and device. I also ended up changing sizes of cable management tubes.\n\nIf I had to do it again, I would do way more sketching beforehand, keeping in mind the slack from the various cables that will need to be hidden.\n\nI\u2019m personally getting ready to move very soon, so this type of project will be in my future. I can imagine that it\u2019s a bit easier to manage video cables alone (although that may get more complicated with upscalers, etc), but I imagine keeping all the power cables/bricks managed could be a bit more complicated. Do you have any insights/tips/tricks on that matter?\n\nUsing the pegboard [and the related pieces] on the back of your setup [really makes the biggest difference, in my opinion] Use twist ties to hold the cords, a power strip with wide enough outlet spacing to allow for brick power adapters, and cable management tubes to keep everything together.\n\nAny other little things that could be overlooked when doing a cable organization project \u2014 either on a large or small scale?\n\nOne thing I overlooked on this [cable management] setup was the slack. I drew out kind of where I wanted the cords to go, and started attaching everything, but didn\u2019t really have a solid plan as to where I would put the slack. If I had to do it again I would have purchased larger tubing for the cable management to hold the excess cord.\n\nWhat parts of that setup are the geekier parts that are kind of \u201cbehind the scenes\u201d elements?\n\nOh, there\u2019s some seriously cool stuff with the downscalers and corresponding power management pieces. I have two tvONE Corio T1-C2-400 units for the Wii and Xbox 360.\n\nEven though the Wii can output 480i, I always found the flicker distracting. Now the signal is being converted from 480p to 240p, not to mention being changed from YPbPr to RGB. Amazing that one unit can do that work!\n\nThe Xbox 360 is going from one of its 4:3 HD signals to 240p RGB. However, when setting all of this up, I ran into a problem \u2013 the Corio units are turned on all of the time\u2026therefore sending a signal to my gscartsw lite automatic SCART switch at all times, preventing any other system from getting a signal through. I needed to figure out a way for these units to be powered on at the same time as the systems hooked to them.\n\nAfter some searching I found a couple of different power strips by Trickle Star (see them on eBay / Amazon), originally designed to conserve energy. Essentially, a device (in this case, one of the aforementioned game systems) is plugged into a \u201ccontrol\u201d plug, and whenever it is turned on and drawing power, it activates the other plugs. So when I power up the Xbox 360, it powers on the Corio, sending the signal to the gscartsw lite and then to the PVM. Very geeky, and very awesome.\n\nWhat other interesting displays have you been collecting or enjoying?\n\nI see that Samsung GX and some smaller PVMs. Can you tell us about the decisions to pick those up and other other units I may have missed?\n\nSince getting my PVM\u2019s I\u2019ve become very interested in other CRT\u2019s, PVM\u2019s, and BVM\u2019s. Given their size and weight, this is an interest that has been thankfully tolerated by my loving and supportive wife!\n\nBeing in Los Angeles, I discovered I can access these monitors a lot easier than if I were in other parts of the country. I\u2019ve bought and sold many of these monitors, helping get them into the hands of people who live outside of this area, often for exactly what I paid for them.\n\nMy motivation in buying them is a general curiosity\u2026how does the image on a Sony PVM-2950Q look compared to a PVM-20L5? Would I prefer a BVM-D24E1WU to a modern LED? I\u2019ve had more monitors in my hands now than I can count. The ones I keep holding onto are my two Sony PVM-2950Qs and my two PVM-20L5s. I still have the Samsung GX, too. Such a cool CRT.\n\nEvolving a Collection & Finding Contentment\n\nIt\u2019s cool to see more of your shelving/game storage. As I think we\u2019ve discussed, we have very similar game collection sizes and adding/pruning habits (aside from you beating me out on Neo-Geo and the lovely more valuable pieces I haven\u2019t plunged on). What planning/decision process did you have for building and pruning your collection?\n\nFor me personally, my collecting initially probably had more of an emphasis on a \u201cwhat is rare\u201d regardless of whether or not it\u2019s a game I would like to play, or on a system I would even enjoy or have some nostalgia for. As I\u2019ve gotten further along, the types of games I\u2019m collecting, there is still a rarity appeal, but the games themselves are more the genres I enjoy. For instance, I have no desire to go out and collect all the rare RPG\u2019s on Super Nintendo. I just don\u2019t play them, and it\u2019s kind of the opposite genre of what I like. Also, living somewhere like Los Angeles where real estate is so expensive, I don\u2019t quite have the room to just go nuts with my collecting. My wife has graciously let me set up one of our bedrooms as essentially the game room, but I don\u2019t have a basement I can fill with candy cabs (which, don\u2019t get me wrong, would be amazing).\n\nAlso, as I\u2019ve gone along I\u2019ve sold games and/or systems I just don\u2019t have an attachment to, or don\u2019t quite fit in with my current setup. I never grew up with PlayStation, so I don\u2019t have a connection to anything from the PS1 era. And with PS2 it outputs primarily 480i, which I really do not enjoy on my setup. Recently I sold nearly my entire, small, PS2 collection to pay for a single Neo Geo AES Game, US Aero Fighters 2. I do hope to get more into PS1 here at some point, as that would fit in with the setup quite nicely.\n\nAs for, what themes I would base my collection goals on? It\u2019s shifted over time. I\u2019ll put those themes into some questions I ask myself when looking at a game. Is this a genre I enjoy? Did I have this game as a kid? Do I have room to put this game on a shelf?\n\nWhat parts of your collection have you been building up in the two and a half years since we last discussed your collection?\n\nA lot of my game collecting has slowed in the past couple of years, but not due to declining interest. I feel I\u2019ve gotten most of the titles I want on the more mainstream systems, Genesis, SNES, etc. The main systems I\u2019m collecting on now are JP Sega Saturn and Neo Geo AES\u2026the latter of course moving at the speed of a continental drift.\n\nI\u2019ve become more interest maximizing the quality of the picture, system modifications, CRT\u2019s, signal processing, etc. Boxes of spare games in my closet have been replaced with boxes of Extron video units and SCART cables.\n\nAny new buying/selling/trading techniques that differ from what we discussed in your first interview?\n\nYeah things have changed quite a bit in the past couple of years. I\u2019m finding it harder and harder to buy collections, cherry pick, and resell to recoup costs.\n\nHonestly I\u2019ve only made a couple of large purchases since we last talked. Maybe there\u2019s too much competition? Maybe there\u2019s more price awareness? I\u2019m not really sure. It also takes a lot of time and leg work that maybe I\u2019m not as willing to put in any more since my focus has somewhat changed.\n\nDiving into Handhelds and the Modding Hobby\n\nWhat part of your collection have you been enjoying the most that the majority of people might not expect?\n\nHonestly \u2013 Gameboy. I never had a Gameboy growing up. Not the original, Pocket, Color, Advance, anything. It\u2019s a whole new world to me. I recently beat Super Mario Land, TMNT, Mega Man, and some other common Gameboy titles. It\u2019s been so much fun!\n\nCan you tell me if there\u2019s much of a story behind all the Game Boy units that you\u2019ve been distributing in your IG claim sales?\n\nGreat unintentional follow up question! I try to learn a new thing each year, and for 2020 it was how to solder. After destroying a handful of Game Gears (RIP), I\u2019ve gotten to where I\u2019m doing RGB mods, recaps, and Gameboy modding.\n\nThe Gameboys have been a lot of fun simply due to all the customizations that can be made. Different screens, buttons, cases, backlights, speakers, and more. Endless. I started to think\u2026if I can\u2019t fund my collection with buying and selling, maybe this is the new route.\n\nI\u2019m now selling modded Gameboys and doing commissioned mod work \u2013 essentially letting my labor fund my game collection. It may take me selling 20 Gameboys to afford a basic AES game, but that still feels better than it coming all out of my wallet.\n\nPlus, taking a Gameboy DMG that\u2019s not working, with a shell that\u2019s yellowed and broken, and making into something new and amazing that someone can enjoy feels great.\n\nDo you feel like you\u2019re hitting your stride on the modification process? Have you thought about paint treatments (I\u2019m assuming currently you\u2019re just dealing with custom pre-made cases?)\n\nI feel like I\u2019m hitting my stride on certain systems and modifications. Gameboy DMG, Pocket, and Color I have down. Still need to get into Gameboy Advance mods. I get asked about GBA a lot! As for other mods, I\u2019m working on all the various RGB mods that can be done on systems. I think having gone through a process of getting these types of mods done when I picked up my first PVM, I love knowing that I can actually do them myself now.\n\nIt seems like you\u2019ve been spending most of your modification time on handhelds, but I\u2019ve seen you mentioning upgrading joysticks and consoles as well. Can you briefly share what projects you\u2019ve had success with, any interesting learning experiences and if this is included in the modding services you\u2019re offering now?\n\nI\u2019ve been having a ton of fun modding consoles \u2013 and you\u2019re correct, the work has been primarily focused on handhelds. I\u2019ve done screen, amp, speaker, case, etc., mods on most Nintendo handhelds. My favorite of these is the Gameboy Advance IPS V2 screen mod, with the RetroSix CleanAmp, de-hiss mod with rechargeable USB-C battery. This system accomplishes nearly everything I would want in a retro handheld. But I have worked quite a bit with consoles as well.\n\nSome of the mods I have done are RGB mods on N64, SNES, and Triple Bypass RGB mods on Sega Genesis Model 2 and Model 3. I especially like the Triple Bypass RGB on the Model 3 V1, which restores Game Genie, Virtual Racing, and Mega SD use with Sega CD and Master System support. It literally gives the best picture and audio out of any Sega I\u2019ve messed with, which is kind of crazy considering the Genesis 3 is widely regarded as the least desirable of all Genesis systems. I also worked in a blue power LED under fogged acrylic. Additionally, I\u2019ve done a number of Optical Drive Emulator (ODE) installations for Saturn, PlayStation 1, and GameCube.\n\nAny other projects you haven\u2019t done that you\u2019re looking forward to?\n\nTons! I have parts in stock for WiiDual, which is dual digital HDMI and analogue RGB output for the Wii, NES RGB, Advanced N64 RGB, and McWill screens for GameGear.\n\nOr are there any other learning experiences you see for yourself in the next year or two?\n\nIn the next couple of years I\u2019d like to further refine what work can be done for retro game systems to keep them new and exciting. Exploring new mods, custom shells, scalers, etc., to keep the hobby alive and looking the best it possibly can.\n\nHow Emulation & Flash Carts Factor into a Well-Rounded Collection\n\nDuring my work on our RetroArch guide, we chatted a bit about your emulation setup that you have used to play retro stuff online with your buddies. Can you tell us a bit about that?\n\nTotally! The emulation stuff is something I came across while on lockdown. I have several local friends I would play games with every couple of weeks, and it was really a bummer to suddenly not have those gaming sessions.\n\nI discovered that if I made us each the exact same RetroPie image (same game titles, cores/emulators, and file paths) then Netplay could work. Basically I FaceTime with a friend while running Netplay and we can go through any number of retro co-op games, or even a round of Neo Turf Masters. It\u2019s been so much fun.\n\nAny other emulation pieces that you work into your routine or do you mostly play on more original hardware?\n\nThe Pi2SCART is the main emulation piece in my setup, but that\u2019s primarily for arcade stuff. Other than that, I play on original hardware. However, in that realm I\u2019ve picked up the SD2SNES Pro, Mega SD, NeoSD Pro, and Turbo Everdrive [flash carts to play game ROMs on original systems].\n\nI don\u2019t really see Everdrives as a deterrent in collecting, more of a way to preserve the games I already have. Since I picked up the Mega SD, it\u2019s been really nice to not touch my Sega CD collection. I was finding I was getting those games out of their cases and putting them in my Sega CD like it was some sort of bomb diffusion. Just not fun. All that being said, when I go to try to beat a game for the first time, I usually will put in the actual real game\u2026maybe that\u2019s the purist in me.\n\nThanks again to Brandon for taking the time for this interview and sharing his photos. If you\u2019d like to get to know him better, follow him on Instagram and check out his Etsy shop of handheld and console mods.",
            "published_at": "2021-01-19T13:42:02+00:00"
        },
        {
            "authors": [
                "Madeleine Schwartz",
                "Matthew Sperling",
                "Thomas Marks",
                "Rosalind P. Blakesley",
                "Florence Evans",
                "Martin Oldham",
                "Will Wiles",
                "Samuel Reilly"
            ],
            "title": "Fakery and the Russian avant-garde",
            "contents": "At first glance, they might be identical. Two orange lozenges leaning out of shards of blue. Two paintings purportedly by the same artist. But look a little closer, and you\u2019ll start to notice differences. The one on the left seems clunkier, its gradations in colour less subtle. The palette seems reduced, the brushwork less varied and interesting. The one on the right is Painterly Architectonic (1917) by Liubov Popova, a cubist and suprematist painter who lived a brief and active life in early 20th-century Moscow. The one on the left is a fake.\n\nThe paintings have been on show in \u2018Russian Avant-Garde at the Museum Ludwig: Original and Fake \u2013 Questions, Research, Explanations\u2019 (currently closed due to Covid-19 restrictions; due to run until 7 February 2021), an attempt by this museum in Cologne to answer questions about its collection. The field of the \u2018Russian avant-garde\u2019 \u2013 paintings made from the late 1890s through to the early 20th century \u2013 is especially prized by the art market; a painting by Kazimir Malevich, whose work is represented in the exhibition, sold for nearly $86m at Christie\u2019s in 2018. The market is also rife with fakes. In the same year, an Israeli art dealer named Itzhak Zarug was convicted of falsifying provenance statements. Separately in 2018, an exhibition at the Museum of Fine Arts in Ghent closed when scholars questioned the authenticity of some of the show\u2019s works. The Museum Ludwig, which was built in 1976 to display the private collection of the chocolate heirs Irene and Peter Ludwig, has been going through its paintings \u2013 a mix of Russian avant-garde, American Pop art, and Picassos \u2013 and examining them. This exhibition is a result of that action, and it is presented as an educational experience.\n\nMuch of the research done by the museum\u2019s team looks specifically at the materials used to make the paintings. Synthetic fibres such as nylon and acrylic can be seen from microscopic specks, and make it easy to determine a painting\u2019s age. One painting on display in the show, a canvas of dark swathes of paint, was once ascribed to Kliment Redko, a painter who briefly had a suprematist moment before turning to figurative work in 1923 (suprematism was a subset of avant-garde painting pioneered by Malevich, and focused on stark geometric forms). The back of the painting shows an inscription purportedly by Redko\u2019s wife. \u2018The work of my husband K. Redko 1922 Varying Composition (with extensive restorations\u2026 partial distortions c\u2026[sic] 5 Oct. 1981, Redko.\u2019 The first clue to the painting\u2019s inauthenticity comes from the paint: one colour used is \u2018titanium white\u2019, an acrylic that first appeared on the market after the painting\u2019s purported date. The canvas itself also had synthetic fibre in it: polyamide, available in the Soviet Union only from the 1930s onwards. Dealers made reference to a similar and more clearly provenanced painting in the Museum of Modern Art in Thessaloniki when selling to the Ludwigs. They argued that this proved the work\u2019s authenticity. But that line of argument is dubious. In their research for the show, the curators found an image of the real painting in an art catalogue that more closely resembles the colours of the Ludwig collection\u2019s work, suggesting that the artwork on display in Cologne was copied from the catalogue.\n\nPictures from the so-called Russian avant-garde movement have long appealed to Western tastes. As the critic Konstantin Akinsha explains in the exhibition catalogue, during the early Soviet era \u2018diplomats willingly exchanged [\u2026] desired objects from the West, such as blue jeans, American LPs, and bottles of whiskey\u2019 for painted pastiches. One collector described how fakes of Kandinsky\u2019s paintings were so good they confused even his widow. A shift after the late 1960s from \u2018Soviet\u2019 to \u2018Russian\u2019 avant-garde helped sales by removing the whiff of radicalism from these works.\n\nThe sales of these frauds may have also changed the course of art history. One of the potent suggestions of the show is that the flood of \u2018avant-garde\u2019 art into the Western market created a school of art that existed only in the minds of Western collectors. One artist, Nina Kogan, may never have even existed. A gallery exhibit from 1985 stated that her works had been sent abroad by friends after her death. But little is known about the woman herself. The only fixed point was the fact that she starved to death in 1942 during the siege of Leningrad. What about a life and career before? Most of the images once attributed to her no longer carry her name. Who was she? Was she even a painter?\n\nThe curators of the exhibition refuse to say definitively that any of the paintings included in it are fake. Instead, each picture is accompanied by a series of clues, looking at, for example, the composition, style and craquelure. (This latter can be an unreliable clue, as the Russian avant-gardists anyway played with surface thickness and composition.)\n\n\u2018We\u2019ll never bring it to a conclusion,\u2019 says curator Rita Kersting in a short video that plays in the exhibition hall. There may be legal reasons for her evasiveness. Galerie Gmurzynska, which sold hundreds of works to the museum \u2013 including some the Ludwig has now deemed falsely attributed as well as others shown to be real \u2013 filed a lawsuit demanding to see the research done for the exhibition in advance of its opening. The gallery\u2019s owner told the New York Times that what she objected to was the use of the word \u2018fake\u2019 in the show\u2019s title. The gallery\u2019s claim was ultimately unsuccessful. In the same video, the head of the museum Yilmaz Dziewior stresses that part of the purpose of the exhibition is to teach visitors \u2018how to look\u2019.\n\nWhen the video stopped playing, I attempted to do just this. I went upstairs to the museum\u2019s permanent collection. In one nook were paintings by Malevich and Rodchenko. I tried to examine the craquelure on one of the paintings. It seemed thick and fissured. A sign of age? An indication of the avant-garde interest in surfaces? A sign of fakery? Or just something to be taken in and appreciated? The wall text gave little information aside from a name and a date. It occurred to me that the museum curators were now in the same position as everyone else \u2013 journalists, politicians, experts \u2013 for whom the default response in an age of information saturation and distortion must be scepticism. I stopped trying to reach a conclusion and walked back down the stairs, checking the news on my phone.\n\nFrom the December 2020 issue of Apollo. Preview the current issue and subscribe here.",
            "published_at": "2021-01-16T00:00:00"
        },
        {
            "authors": [],
            "title": "Tailscale on NixOS: A New Minecraft Server in Ten Minutes",
            "contents": "NixOS is a Linux distribution built from the ground up to make it easy to deploy services. Tailscale is a peer-to-peer VPN built to make it easy to connect machines. In this article I will show how to set up a Java Edition Minecraft server (exposed only over Tailscale) in ten minutes on Digital Ocean.\n\nBefore you begin this guide, you\u2019ll need the following:\n\nA Digital Ocean account\n\nA Minecraft Java Edition account\n\nYou\u2019ll also need a Tailscale account. You can make a free solo account using an @gmail.com address.\n\nIn NixOS one of the core principles is that the entire system is configurable with a modular language called Nix. This allows you to configure everything using a common syntax so you don\u2019t need to remember every configuration file format for every service you use. As such, you can configure an entire system from a single file.\n\nLet\u2019s make a new configuration file called host.nix and set up a system that has Tailscale start up on boot:\n\n{ config , pkgs , ... }: { # make the tailscale command usable to users environment . systemPackages = [ pkgs . tailscale ]; # enable the tailscale service services . tailscale . enable = true ; }\n\nThis will have Tailscale start up, however to authenticate to Tailscale we need to take a few more steps. First, head to the Pre-Auth Keys page in the admin panel.\n\nCreate a new one-time use key and then copy it to your notes. We will use it below.\n\nWith this key we can write a systemd oneshot unit that will log in to Tailscale using this key.\n\nA systemd oneshot job is something that systemd expects to be run once as opposed to being a persistent service. The above Digital Ocean link explains this in more detail.\n\nCopy this configuration below the services.tailscale.enable line in your host.nix file:\n\n# ... # create a oneshot job to authenticate to Tailscale systemd . services . tailscale-autoconnect = { description = \"Automatic connection to Tailscale\" ; # make sure tailscale is running before trying to connect to tailscale after = [ \"network-pre.target\" \"tailscale.service\" ]; wants = [ \"network-pre.target\" \"tailscale.service\" ]; wantedBy = [ \"multi-user.target\" ]; # set this service as a oneshot job serviceConfig . Type = \"oneshot\" ; # have the job run this shell script script = with pkgs ; '' # wait for tailscaled to settle sleep 2 # check if we are already authenticated to tailscale status=\"$( ${ tailscale } /bin/tailscale status -json | ${ jq } /bin/jq -r .BackendState)\" if [ $status = \"Running\" ]; then # if so, then do nothing exit 0 fi # otherwise authenticate with tailscale ${ tailscale } /bin/tailscale up -authkey tskey-examplekeyhere '' ; };\n\nMake sure to replace tskey-examplekeyhere with the key you got from the Admin panel. Without this key your VPS cannot connect to Tailscale.\n\nNow that we have Tailscale configured to authenticate and connect to your network, let\u2019s enable the lightweight NixOS firewall so that we can only allow Minecraft traffic over Tailscale. The NixOS manual has more details, but at a high level we need to:\n\nEnable the firewall\n\nMark tailscale0 as a trusted interface\n\nAllow anyone on the internet to connect to Tailscale\u2019s UDP port (don\u2019t worry, they will be rejected if they are not a part of your network)\n\nAllow anyone on the internet to connect to your server\u2019s SSH port (this will also work over Tailscale, but having it exposed over the public internet can help when debugging issues that happen before tailscale starts)\n\nWe can do that with the following configuration added to the end of your host.nix file:\n\n# ... networking . firewall = { # enable the firewall enable = true ; # always allow traffic from your Tailscale network trustedInterfaces = [ \"tailscale0\" ]; # allow the Tailscale UDP port through the firewall allowedUDPPorts = [ config . services . tailscale . port ]; # allow you to SSH in over the public internet allowedTCPPorts = [ 22 ]; };\n\nNow we can configure the Minecraft server. The Minecraft options expose a number of settings you can use to configure your server however you want. Please note that you must set the EULA agreement to true yourself. The Minecraft server is a closed-source program, so you must give NixOS permission to use closed source packages.\n\n# ... services . minecraft-server = { enable = true ; eula = false ; # set to true if you agree to Mojang's EULA: https://account.mojang.com/documents/minecraft_eula declarative = true ; # see here for more info: https://minecraft.gamepedia.com/Server.properties#server.properties serverProperties = { server-port = 25565 ; gamemode = \"survival\" ; motd = \"NixOS Minecraft server on Tailscale!\" ; max-players = 5 ; enable-rcon = true ; # This password can be used to administer your minecraft server. # Exact details as to how will be explained later. If you want # you can replace this with another password. \"rcon.password\" = \"hunter2\" ; level-seed = \"10292992\" ; }; }; # enable closed source packages such as the minecraft server nixpkgs . config . allowUnfree = true ;\n\nNow we have everything we need. We set up Tailscale, we configured an automatic login to Tailscale, we set up the firewall so that it rejects all incoming traffic (except for traffic from you Tailscale network) and finally we configured the Minecraft server so that your can play in survival mode.\n\nNow let\u2019s get this put into a new Digital Ocean server. Open a new text editor window and create a file called user-data.yaml . Copy this template into it:\n\n#cloud-config write_files : - path : /etc/nixos/host.nix permissions : \"0644\" content : | { pkgs, config, ... }: { } runcmd : - curl https : //raw.githubusercontent.com/elitak/nixos-infect/master/nixos-infect | PROVIDER=digitalocean NIXOS_IMPORT=./host.nix NIX_CHANNEL=nixos -20.09 bash 2 > &1 | tee /tmp/infect.log\n\nAt the time of this article being written, Digital Ocean doesn\u2019t have a NixOS image by default. So we will use nixos-infect in order to automatically replace an Ubuntu installation with a NixOS one. We are also going to use it to automatically configure the server with the host.nix file we just made.\n\nCopy the contents of host.nix into the content value of the write_files block. Be sure to indent the file by four spaces so that it will work as yaml user data. When you are done you should have a file that looks something like this reference file.\n\nNow that you have the cloud config, let\u2019s launch the server in the cloud.\n\nOpen the Digital Ocean control panel and click Create and then click Droplets. Choose an Ubuntu 20.04 image and a plan with at least 4 GB of ram (the Minecraft server needs a lot of resources, unfortunately).\n\nThen choose a datacenter near you and enable the IPv6 and User Data options. Paste the contents of your user-data.yaml file into the text box that shows up. Choose your SSH key in the list of SSH keys that shows up and then give the droplet a hostname, such as \u201cminecraft\u201d. If you want you can enable backups.\n\nOnce you have everything set up, click the large green button labeled Create Droplet. It may take up to 10 minutes to install and configure NixOS, however you will be able to see the machine in your Tailscale network once everything is done setting up. Go grab a cup of your favorite caffeinated beverage.\n\nAfter it shows up in your network list, you can fire up your Minecraft client and connect to your new server. You may need to select a slightly older version of Minecraft if the in-game UI tells you to use an older version of Minecraft. You can adjust the Minecraft version you are running using the launcher UI.\n\nIf you want to administer your minecraft server, you can add the mcrcon package to your system config next to the tailscale package:\n\nenvironment . systemPackages = with pkgs ; [ tailscale mcrcon ];\n\nThen you can connect to the Minecraft server command line with this command:\n\n$ mcrcon -p hunter2\n\nIf you adjusted the rcon password above, you will need to adjust this command to include that new password. From there you can change gamemodes, adjust the time of day and anything else you want.\n\nOnce node sharing is generally available, you can use it to invite people you trust to your server. (Node sharing is still in private beta.) Generate an invite link in the admin panel and they can use that to join your adventure."
        },
        {
            "authors": [],
            "title": "Enormous parts of society are broken. Free markets and capitalism could fix them. Sign me up.",
            "contents": "In December, in an interview on Yang Speaks, Chamath Palihapitiya of Social Capital talks about a range of the biggest human problems, from climate to education to sustainability to health, that could be solved with free markets and capitalism but aren't.\n\nWhat stands in the way? It's about capital, says Palihapitiya:\n\nOn one dimension, it's capital in terms of investment. He observes that venture capital has, in the main, moved from investing in innovation to market arbitrage. By contrast, he cites his own returns while investing to solve \"the world's hardest problems.\"\n\nOn another dimension, it's human capital. On this, he says:\n\nThe single biggest thing that America has right now going against it is that we have four or five companies - Facebook, Apple, Amazon, Microsoft, Google - that suck up the enormity and the overwhelming majority of all talented, technical human capital.\n\nHe describes it like a professional sports team with no salary cap. Hire the best people and, essentially, bench them. Innovation, progress and wage growth stagnates. To this, to Chamath and anyone of like mind, I say:\n\nPut me in coach, I'm ready to play!\n\nIf you read my last article, you'll know that I'm seeing what happens when I - if not flip, at least re-balance - my usual pattern. So, instead guiding my actions through a \"minimize emotions, maximize returns\" lens, I'm using an \"acknowledge my feelings, pursue work that really matters\" lens. With that in mind, I wrote Chamath the following:\n\nHi, Chamath:\n\nI'm struggling. It's a sort of privileged struggle, but a struggle nonetheless. By way of a metaphor, there's a scene in the television show the Wire when one of the characters compares things to a 40 degree day. Summarizing, a 40 degree day is nothing to celebrate or complain about. For a long time, my work has felt like a series of 40 degree days.\n\nI'm working on this with a therapist who, while on a long drive to relocate her family to Florida for the winter, listened to your interview on the Andrew Yang podcast. I hadn't spoken to her for a few years, so it was a bit of serendipity that she heard your interview just days before we reconnected. I opened the session by saying:\n\nI want to use my product, technology, process and leadership experience \u2013 along with the tools of math and statistics; modeling and simulation; and financial and economic thinking \u2013 to do deep, disciplined work on a hard problem that really matters.\n\nNeedless to say, she made the connection to the podcast and that's what's prompted me to write.\n\nI was a modestly successful entrepreneur during the dotcom era. When my kids were little, I owned one of those little businesses you described - I felt proud that our little mobile development shop kept 15 men, women and children warm, clothed and fed. When my kids got older, I could once again do ambitious work. The work I'm most proud of was at Wildflower, an \"evening the starting line\" education non-profit that was initially funded by Chan Zuckerberg after it was spun off from the MIT Media Lab.\n\nEach Wildflower Montessori school spawns another every two years. From the three dozen schools already established, this growth will lead to 2% of the US population having a Montessori education in a generation. These are charter schools, so they are free to attend, and placing them in economically disadvantaged neighborhoods brings Montessori education to kids who would normally not have access to it.\n\nIf what I've said above sparks your interest, this \"Long read on vulnerability, an honest account of 2020, and an ask for help\" provides more context and examples.\n\nIn the podcast, you said that human capital is often the scarce resource. I'm a systems thinker. I've successfully led dev and product teams. I code, better than ever actually, but generally limit myself to modeling and simulation in support of my endeavors. In short, I'm a unit of human capital that I think you could put to good work. Could we speak? Name the date and time and I'll make it work.\n\nWith warm regards,\n\nDan Grigsby\n\n\n\n"
        },
        {
            "authors": [],
            "title": "Instacart Will Lay Off All of Its Unionized Workers",
            "contents": "Instacart is firing 10 employees who voted to form the first and only union on the grocery delivery platform at a Mariano's grocery store in Skokie, Illinois, and inspired other Instacart employees to organize their coworkers at grocery stores around the country.\n\nAdvertisement\n\nThe decision to terminate the employees who unionized with the United Food and Commercial Workers Local 1546 in Skokie in early 2020 is part of a larger series of layoffs that Instacart announced on Tuesday. The news was buried in a blog post and the layoffs impact Instacart's in-store shoppers, who are direct employees of the company that pick and pack groceries at supermarkets around the country. According to the UFCW, Instacart is firing nearly 2,000 of its 10,000 grocery grocery store workers as part of these layoffs, and offering as little as $250 as severance.\n\nThe layoffs come amid Instacart's rapid expansion during the Coronavirus pandemic as millions of people have turned to on-demand grocery delivery. The company reported profitability for the first time in 2020, is preparing to IPO at some point this year, and is estimated to be valued at up to $30 billion, CNBC reported.\n\nThe decision to terminate the workers who voted to unionize last February and were in the process of negotiating their first contract could put a damper on other efforts to unionize Instacart's in-store shoppers around the country. At the time of their union drive in 2020, Instacart manager ran a union-busting campaign, circulating anti-union literature and memos intended to convince workers to vote down the union.\n\nAdvertisement\n\n\"These layoffs are totally discouraging for any gig workers who are trying to do something to make these jobs better,\" a unionized Instacart in-store shopper in Skokie said. Motherboard granted the worker anonymity because they feared retaliation from Instacart.\n\nThe worker said they hoped to secure health insurance and vacation time in their first contract, and hoped to set a higher standard for working conditions that could improve the lives of gig workers around the country. In-store shoppers also want a say in the algorithm that determines their schedules and the pace of work. According to the worker, the union learned about the layoffs\n\n\"Anger is the first thing I feel because they eliminated my job during a pandemic and the reason they gave us is 'cost-cutting,'\" the worker continued. \"The second thing I feel is fear. Am I going to be lucky enough to get another job? I had a business that was closed for being non-essential and I didn't have enough money to keep it going and open it back up. From a financial standpoint, I lost everything. With this job, while it doesn't pay super well, I could at least pay for my cell phone and car insurance and buy groceries.\"\n\nNews of the layoffs, which will impact Instacart's in-store shoppers at several supermarket chains, including Kroger-owned grocery stores, was buried in a blog post published Tuesday that Instacart wrote announcing two new curbside grocery pickup models that it intends to expand as COVID-19 has shifted the needs of customers. In July, Instacart laid off its employees at Aldi and Sprouts grocery stores, some who were offered jobs to continue working as employees of the grocery store.\n\nAdvertisement\n\n\"We know this is an incredibly challenging time for many as we move through the COVID-19 crisis, and we\u2019re doing everything we can to support in-store shoppers through this transition,\" a spokesperson for Instacart told Motherboard.\n\n\u201cInstacart firing the only unionized workers at the company and destroying the jobs of nearly 2,000 dedicated frontline workers in the middle of this public health crisis, is simply wrong,\u201d said Marc Perrone, president of UFCW International, in a statement. \"As the union for Instacart grocery workers in the Chicago area and grocery workers nationwide, UFCW is calling on Instacart to immediately halt these plans.\u201d\n\nInstacart did not respond to a question about how many workers would be terminated, but has said that it will help laid off workers transition into new roles at other grocery stores\u2014or into gig worker positions at Instacart, and provide severance packages to all laid off workers depending on their tenure. By laying off in-store shoppers who are employees who are eligible to unionize and transitioning them into non-union eligible gig workers roles, Instacart is also making it more difficult for its workforce to unionize.\n\nOver the past year, UFCW organizers have organized Instacart in-store shoppers around the country, though the workers at the Mariano's in Skokie remain the only group to have unionized. In May 2020, Instacart employees at a grocery store in Chicago voted down a union."
        },
        {
            "authors": [
                "Chris Golden"
            ],
            "title": "A Simple and Efficient Real Time Application Powered by Materialize\u2019s TAIL Command",
            "contents": "Within the web development community, there has been a clear shift towards frameworks that implement incremental view maintenance and for good reason. When state is updated incrementally, applications perform better and require fewer resources. Using Materialize, developers and data analysts can adopt the same, event driven techniques in their data processing pipelines, leveraging existing SQL know-how. In this blog post, we will build an application to demonstrate how developers can create real-time, event-driven experiences for their users, powered by Materialize.\n\nNote:\n\nThis post is the fulfillment of the goals that I had when writing Streaming TAIL to the Browser. That post is not required to understand this post but familiarizing yourself with the TAIL command is recommended.\n\nOverview\n\nDemonstration of What We Will Be Building\n\nIn this post, we are going to use the demo from our documentation\u2019s Get Started tutorial as the basis for building a minimal web application. The application allows users to see the total number of edits made to Wikipedia, as well as a bar chart of the top 10 editors. Here is an animation that shows the final result:\n\nWant to run this demo yourself? Awesome! Clone our repository and follow the instructions for Running it Yourself.\n\nDesired Properties of the Solution\n\nBefore jumping into how to build this, let\u2019s outline the desired properties for our solution. The final solution should be:\n\nPush-Based \u2013 Instead of having the client poll for updates, updates should be initiated by the server and only when the client state is outdated.\n\n\u2013 Instead of having the client poll for updates, updates should be initiated by the server and only when the client state is outdated. Complete \u2013 Clients should present both a consistent and complete view of the data, regardless of when the application is loaded.\n\n\u2013 Clients should present both a consistent and complete view of the data, regardless of when the application is loaded. Unbuffered \u2013 Clients should receive updates as soon as changes are made to the underlying datasets, without arbitrary delays in the data pipeline.\n\n\u2013 Clients should receive updates as soon as changes are made to the underlying datasets, without arbitrary delays in the data pipeline. Economic \u2013 Size of updates should be a function of the difference between the current state and the desired state. Additionally, clients should not be required to replay state from the start of history.\n\n\u2013 Size of updates should be a function of the difference between the current state and the desired state. Additionally, clients should not be required to replay state from the start of history. Separation of Concerns \u2013 Applications modifying the source data (writers) should not need to know anything about the applications consuming the data (readers).\n\nWhile it is possible for other applications to meet several of these properties, I hope that this application will demonstrate why the solution presented is ideal for this scenario. A discussion of why this application meets the above properties, and why other solutions likely do not, is presented further down in this post.\n\nOverall Architecture\n\nFor those unfamiliar with our getting started demo, here is the flow of data in our pipeline:\n\nLooking at the system diagram below, we see that the entire data pipeline is contained within a single materialized instance:\n\nLet\u2019s discuss the role of each service in the diagram above.\n\nNote: If you started the application, you can run mzcompose ps to see the containers started.\n\ncurl \u2014 wikirecent_stream_1\n\nThis container runs curl to stream Wikimedia\u2019s recent changelog to a file called recentchanges within a Docker volume shared with our materialized instance.\n\nmaterialized \u2014 wikirecent_materialized_1\n\nThis container runs an instance of materialized , configured to tail the recentchanges file and maintain our two materialized views: counter and top10 . The views in this instance are configured exactly as documented in Getting Started \u2013 Create a real-time Stream.\n\nPython web server \u2014 wikirecent_app_1\n\nThis container runs a Python web server that hosts the code for our JavaScript application and converts the row-oriented results from TAIL to the batch-oriented results expected by our application.\n\nBuilding Our Application\n\nOur example application is an asynchronous Python web server built using two libraries: Tornado and psycopg3. There are three components to our application that I would like to call out:\n\nPython code to subscribe to materialized view updates using the TAIL command and convert the row-oriented results into batches.\n\ncommand and convert the row-oriented results into batches. Python code to broadcast batches to listeners.\n\nJavaScript code to apply batch updates for efficient browser updates.\n\nNote: Materialize uses the word batch to refer to a data structure that expresses the difference between prior state and desired state. You can think of a batch as a \u201cdata diff\u201d.\n\nOutput for Our JavaScript Code\n\nTo efficiently update our client view state, we wish to present a stream of batches over a websocket to any configured listeners. We define a batch as the following:\n\nbatch = {\"inserted\": [ // array of rows to add to our dataset ], \"deleted\": [ // array of rows to remove from our dataset ], \"timestamp\": 0 // Materialize-defined timestamp }\n\nWhen a client first connects, we send a compacted view of all batches from the beginning of history:\n\ndef add_listener(self, conn): \"\"\"Insert this connection into the list that will be notified on new messages.\"\"\" # Write the most recent view state to catch this listener up to the current state of the world conn.write_message( { \"deleted\": [], \"inserted\": self.current_rows, \"timestamp\": self.current_timestamp, } ) self.listeners.add(conn) # subscribe to the stream of batches\n\nHow can listeners bootstrap their state using a batch object? If there is one property that I find particularly beautiful about this solution, it\u2019s that first batch applied to an empty list is our initial state. This means that initializing and updating are the same operation. This batch object is so useful that D3\u2019s update and Vega\u2019s change APIs expect updates to come in a similar form.\n\nHowever, results from tail are row-oriented. We need a little bit of code to convert from rows to batches; here is an example of the desired conversion:\n\n# We need to convert this stream of rows... 1608081358001 f -1 ['Lockal', '4590'] 1608081358001 f 1 ['Epidosis', '4595'] 1608081358001 f -1 ['Matlin', '5220'] 1608081358001 f 1 ['Matlin', '5221'] 1608081359001 t \\N ['\\\\N', '\\\\N'] # ...to this data structure. timestamp = 1608081358001 inserted = [('Epidosis', '4595'), ('Matlin', '5221')] deleted = [('Lockal', '4590'), ('Matlin', '5220')]\n\nLet\u2019s look at the code to subscribe to view updates and transform rows into batches.\n\nSubscribing to TAIL\n\nTo process rows from TAIL , we must first declare a cursor object that will be used to indefinitely iterate over rows. To help with our code know when to broadcast an update, we ask for progress markers in the response:\n\nasync def tail_view(self): \"\"\"Spawn a coroutine that sets up a coroutine to process changes from TAIL.\"\"\" async with await self.mzql_connection() as conn: async with await conn.cursor() as cursor: query = f\"DECLARE cur CURSOR FOR TAIL {self.view_name} WITH (PROGRESS)\" await cursor.execute(query) await self.tail_view_inner(cursor)\n\nConverting Rows to Batches\n\nWe\u2019ve now created a communication channel which can be used to await results from the tail query. Whenever our view changes, our application will be notified immediately and we can read the rows from our cursor object. tail_view_inner implements the logic to process rows and convert them to batches:\n\nasync def tail_view_inner(self, cursor): \"\"\"Read rows from TAIL, converting them to updates and broadcasting them.\"\"\" inserted = [] deleted = [] while True: # Block until there are new results (FETCH is not buffered) await cursor.execute(f\"FETCH ALL cur\") async for (timestamp, progressed, diff, *columns) in cursor: # The progressed column serves as a synchronization primitive indicating that all # rows for an update have been read. We should publish this update. if progressed: self.update(deleted, inserted, timestamp) inserted = [] deleted = [] continue # Simplify our implementation by creating \"diff\" copies of each row instead # of tracking counts per row if diff < 0: deleted.extend([columns] * abs(diff)) elif diff > 0: inserted.extend([columns] * diff) else: raise ValueError(f\"Bad data from TAIL: {row}\")\n\nUpdating Internal State and Broadcasting to Listeners\n\nNow that we have a batch object, we apply this change to our own internal VIEW and broadcast the change to all listeners:\n\ndef update(self, deleted, inserted, timestamp): \"\"\"Update our internal view based on this diff.\"\"\" self.current_timestamp = timestamp\ufffc # Remove any rows that have been deleted for r in deleted: self.current_rows.remove(r) # And add any rows that have been inserted self.current_rows.extend(inserted) # If we have listeners configured, broadcast this diff if self.listeners: payload = {\"deleted\": deleted, \"inserted\": inserted, \"timestamp\": timestamp} self.broadcast(payload)\n\nDesign Decision:\n\nExperienced readers will note that by maintaining an internal copy of the view in our Python web server, we can reduce the number of connections to the materialize instance. This is a strictly optional design decision that I made when writing this code \u2014 materialized connections are very light-weight when compared to other databases. We expect that there will be use cases where you will want one or more materialized connection per user. Consider temporary materialized views feeding dashboards personalized to each user, for example.\n\nIn the case of this application, I opted to reduce the connections out of habit rather than necessity. It does also enable a larger degree of fan-out, if we wanted to serve millions of clients, for example.\n\nUpdating User Views\n\nNow that we have looked at the code for broadcasting an update, let\u2019s show how our JavaScript code consumes these batches. Our application is showing two things: a total edits counter and a top 10 chart:\n\nUpdating Total Edits Count\n\nThe Total Edits Counter only cares about the latest value from the counter view, which itself consists of only a single row. This means that we can implement a WebSocket listener for total counts that simply reads the first row from inserted and uses that to update our counter HTML element:\n\nvar path = \"ws://\" + location.host + \"{{reverse_url('api/stream', 'counter')}}\"; var connection = new WebSocket(path); connection.onmessage = function(event) { var data = JSON.parse(event.data); // Counter is a single row table, so every update should contain one insert and // maybe one delete (which we don't care about) document.getElementById(\"counter\").innerHTML = data.inserted[0][0] }\n\nUpdating Our Top 10 Chart\n\nThe Top 10 Chart uses Vega-Lite to render a bar chart. Because our batch data structure maps directly to the vega.change method, we can follow their Streaming Data in Vegalite example. We do need to write a small amount of code to enable property lookups:\n\nvegaEmbed('#' + view_name, chart, config).then(function(chart) { var path = \"ws://\" + location.host + \"{{reverse_url('api/stream', '')}}\" + view_name; var connection = new WebSocket(path); function convert_to_subject(row) { return {subject: row[0], count: parseInt(row[1])}; } function subject_in_array(e, arr) { return arr.find(i => i.subject === e.subject && i.count === e.count); } connection.onmessage = function(event) { var data = JSON.parse(event.data); var insert_values = data.inserted.map(convert_to_subject); var delete_values = data.deleted.map(convert_to_subject); var changeSet = vega.changeset() .insert(insert_values) .remove(d => subject_in_array(d, delete_values)); chart.view.change('data', changeSet).resize().run(); } });\n\nAnd that\u2019s it! When a new batch is received, Vega / Vega-Lite updates just the elements that have changed and redraws our chart. We now have a real-time chart powered by a materialized view.\n\nWrapping Up Our Application\n\nIn this section, we saw how to build a truly end-to-end, event-driven pipeline that minimizes the amount of work required to build a real-time user experience. The code for synchronizing client state is simple and straightforward. There are no unnecessary delays introduced by polling and the updates are efficient to send and process. Now, let\u2019s revisit our desired properties to see how we did and to compare against other potential solutions.\n\nRevisiting Our Desired Properties\n\nFrom the example code above, we can see that our application meets the desired properties:\n\nPush-Based \u2013 Our Python server and Javascript applications receive batches as soon as they are available, sent over existing connections. Because materialized only produces batches when a view has changed, updates are only triggered when the client\u2019s state must change.\n\n\u2013 Our Python server and Javascript applications receive batches as soon as they are available, sent over existing connections. Because materialized only produces batches when a view has changed, updates are only triggered when the client\u2019s state must change. Complete \u2013 The Python server always presents a complete view of the data, no matter when it starts up. Likewise, our Javascript clients always have a complete view of the data, no matter when they connect.\n\n\u2013 The Python server always presents a complete view of the data, no matter when it starts up. Likewise, our Javascript clients always have a complete view of the data, no matter when they connect. Unbuffered \u2013 Materialize calculates batch updates as soon as the event data is written to the source file.\n\n\u2013 Materialize calculates batch updates as soon as the event data is written to the source file. Economic \u2013 Batch sizes are proportional to the difference between the prior state and new state. This reduces both the amount of data being sent over the network and the amount of work required to process each update. When clients first connect, they are not required to replay state from all of history; instead, clients receive an already compacted view of the current state.\n\n\u2013 Batch sizes are proportional to the difference between the prior state and new state. This reduces both the amount of data being sent over the network and the amount of work required to process each update. When clients first connect, they are not required to replay state from all of history; instead, clients receive an already compacted view of the current state. Separation of Concerns \u2013 The application writing data, curl, knows nothing about materialized views nor our JavaScript applications. It doesn\u2019t matter if we add additional views, join the wikirecent stream with another data source or even change the existing queries \u2014 we never need to modify our writer.\n\nThings We Avoided\n\nPeople have been building real-time applications for a long-time and Materialize makes it simple to build these applications without traditional limitations. Common drawbacks in other solutions include:\n\nRepeated Polling / Continuous Query\n\nWithout incremental view updates, applications must constantly query the database for the results of a query. This results in increased load and network traffic on the database, as the full results of the query must be computed everytime. It also results in increased load on the web servers, as they must process the full result set on every query response.\n\nMicroservice Sprawl\n\nWithout incrementally maintained views defined in SQL, each materialized view would have required writing a custom stream processing function, as well as the creation of intermediate sources and sinks. Adding microservices would result in increased operational overhead and complexity of deployments.\n\nDelays or Stalls in our Data Processing Pipeline\n\nWhen batch updates are buffered, such as in ELT/ETL pipelines, applications are operating on old state. While it\u2019s tempting to think that it\u2019s just 5 minutes for a single view, the cumulative latency in the pipeline can be much worse. Delays in processing result in applications presenting incomplete and/or inconsistent state, especially when joining data across multiple sources. This reduces customer trust in your data pipeline.\n\nComplicated Synchronization / Resynchronization Logic\n\nWithout incremental view updates, applications must implement their own logic to compute batches by comparing client and server state. This results in duplication of logic where you have one implementation for the initial update and another implementation for the incremental update. It also introduces edge cases when clients reconnect after a connection is dropped or closed. Implementing state synchronization logic in the application introduces additional complexity.\n\nDuplicated Logic to Remove \u201cOld\u201d Data\n\nWithout the server telling the client what data is obsolete, long-lived clients are forced to implement their own logic to remove old data. While this might work for append-only datasets, most datasets have both inserts and deletes. Even if the sources are append-only, the downstream views may not be append-only, such as a top K query. Forcing the client to duplicate the server\u2019s logic to remove data leads to extra complexity during implementation and makes it harder to roll-out updates to the data pipeline.\n\nReader-Aware Writers\n\nWhen the database cannot produce incremental updates, writers may notify listeners directly that the underlying data has changed. This is often done using side channels, such as LISTEN / NOTIFY , but this comes with its own set of drawbacks. Either the writer implements the logic required to produce an incremental update or the reader must fetch the entire dataset on each notification. Additionally, in the presence of dataflows, even simple ones such as our example application, determining who to notify is a non-trivial task.\n\nLate-Arriving / Out-of-Order Data\n\nWithout joins that work over all data ingested, most stream processing systems will expire data based on the size of the window or the age of the data. In other frameworks, once the data is expired, you can no longer join against it. Temporal joins make it difficult to trust the results from your data pipeline.\n\nConclusion\n\nMaterialized makes writing real-time, event-driven applications simple and straightforward. This blog post presents an example application that demonstrates how to build real-time, data-driven application using the TAIL statement. Our application maintains several desirable properties in a real-time application while avoiding the common limitations present in other methods. Check out Materialize today!\n\nDisagree with something I said? Have another method for performing the same task? I\u2019d love to hear from you, either in our community or directly!\n\nLove building applications like this? We\u2019re hiring!",
            "published_at": "2021-01-20T16:24:25+00:00"
        },
        {
            "authors": [
                "Raphael",
                "Kena"
            ],
            "title": "Why and how exceptions are still better for performance, even in Go \u00b7 dr knz @ work",
            "contents": "All the generated data files and Jupyter notebooks can be downloaded here.\n\nThe reader is invited to review the more detailed summaries from the previous analyses, where the situation is presented in more detail.\n\nHowever, we can also point out that Go still promotes the use of dynamic dispatch (via interface methods) as an idiomatic approach to software design. For example, string composition via bytes.Buffer interfaces with the fmt packages using interface calls ( io.Writer ), and the proportion of cross-interface calls is relatively high in that case compared to the computation load.\n\nAs previously, we can discuss what is the proportion of function calls relative to other types of code. Arguably, in computation-heavy algorithms, functions often get inlined so that calls disappear. In that case, it is possible for the Go compiler, assisted by the CPU micro-architecture, to reach similar instruction throughputs as the equivalent C++ code.\n\nIn Go, memory is used for return values even when there is just one or two words worth of data. In these cases, a C++ compiler also typically uses registers. This choice in the Go compiler is unfortunate, as it mandates copying the return values through memory at every call level when functions return each other\u2019s value in a cascade. This cost thus multiplies the overhead of the memory-based return by the call depth, in a way that does not occur, or much less, with a typical C++ code generator.\n\nThe particular CPU we used this year uses a micro-architecture able to more aggressively optimize memory-heavy code, and so the Go performance overhead relative to C++ is erased for very small functions; unfortunately, these optimizations become ineffective with more realistic, wider function bodies.\n\nGo uses memory to pass function arguments and return values. This makes Go code perform about twice slower than equivalent C++ code on function calls, where C++ compilers use register-based calling conventions instead.\n\nOnly the new hardware CPU influences the measurements: the new CPU has a higher clock frequency than previously, so for an equivalent mix of instructions we expect and observe a 10-40% raw performance improvement. Additionally, the Ryzen 7 3950X architecture has a somewhat more powerful superscalar execution unit than the 1800X, and is able to detect inter-instruction dependencies across more memory accesses. This causes more measurements artifacts when the workload is extremely small, and erases the performance differences between 1 and 5 function arguments or return values.\n\nHere the findings are largely unchanged from 2018: for the selected benchmark code, the code generation by our selected compilers has not changed significantly.\n\nThis analysis calls for a benchmark which performs a \u201cunit of work\u201d in a loop, with the size of the loop configurable as \u201cworkload\u201d.\n\nWe are modeling the extremely common case where any error inside a unit of work aborts the entire workload: the error may or may not be generated inside the unit of work, but needs only be detected once for the entire outer workload loop.\n\nIn Go, we originally used the following code as benchmark:\n\n//go:noinline func leafErr ( arg int ) ( int , error ) { if arg == 0 { // Unlikely error case. return 0 , errObj } // Common non-error case. return id ( arg ), nil } //go:noinline func workErr ( work int ) int { var n int for i := 0 ; i < work ; i ++ { val , err := leafErr ( work ) if err != nil { return 42 } n += val } return n }\n\nIn this code, leafErr represents the \u201cunit of work\u201d, where errors can originate. The workErr represents the main work computation, where leafErr is called repeatedly.\n\n(The go:noinline annotation is explained in more detail in the previous analysis.)\n\nIn the case where the error return is used, we need to check the err return on every iteration of the loop. If we used exception instead, we could factor the exception check out of the loop:\n\n//go:noinline func workExc ( work int ) ( res int ) { // Exception check factored out of the loop: defer func () { if r := recover (); r != nil { res = 42 } }() // Main workload follows: var n int for i := 0 ; i < work ; i ++ { n += leafExc ( work + 1 ) } return n } //go:noinline func leafExc ( arg int ) int { if arg == 0 { // Unlikely error case. panic ( errObj ) } // Common non-error case. return id ( arg ) }\n\n(The accompanying C++ code is implemented similarly, using std::tuple for error returns and try/catch/throw for exceptions.)\n\nJust looking at the shape of the source code, we can already suspect the findings that follow: there is less work inside the workload loop when using exceptions. So as the workload increases, the cost of setting up the \u201ccatch\u201d logic gets amortized.\n\nIndeed, this is exactly what was observed in 2018. (Summary and conclusions here.)\n\nHowever, when running the same experiment in 2020 on our new test machine, with Go 1.15, the measurements are all over the place, the results appear chaotic, and the difference we observed last time is nearly entirely erased. What happened?\n\nIt turns out that the new CPU used to run the benchmark really has a much better micro-architecture. It is able to peek through the call from the work function to the leaf function, analyze inter-instruction dependencies across the call, and fully pipeline the processing of the leaf function across multiple iterations of the work loop. This effectively invalidates the model we used: in a realistic application, there is enough complexity in the code inside the \u201cworkload\u201d, when it contains function calls, that the CPU cannot pipeline loop iterations.\n\nTo restore a valid model, we should thus tweak the benchmark program by adding just enough additional complexity in the \u201cunit of work\u201d. Here, we achieve that by adding just one extra level of intermediate call:\n\n//go:noinline func leafErr ( arg int ) ( int , error ) { if arg == 0 { // Unlikely error case. return 0 , errObj } // Common non-error case. return id ( arg ), nil } //go:noinline func intermediateErr ( arg int ) ( int , error ) { return leafErr ( arg ) } //go:noinline func workErr ( work int ) int { var n int for i := 0 ; i < work ; i ++ { val , err := intermediateErr ( work ) if err != nil { return 42 } n += val } return n }\n\n(Likewise for the benchmark using exceptions.)\n\nThis program, at least on the CPU considered, is \u201ccomplex\u201d enough to stop the superscalar unit from peeking across the function call. The resulting code then behaves regularly in measurements.",
            "published_at": "2020-12-01T00:00:00+01:00"
        },
        {
            "authors": [
                "Peter \u0160irka"
            ],
            "title": "Open-source JavaScript platform",
            "contents": "We want to share with you our plans for this year. This blog post contains what we should be doing and what our expectations are."
        },
        {
            "authors": [
                "Donna Lu"
            ],
            "title": "Australian lungfish has largest genome of any animal sequenced so far",
            "contents": "The Australian lungfish has a very long genome Paulo Oliveira / Alamy\n\nThe Australian lungfish has the largest genome of any animal so far sequenced.\n\nSiegfried Schloissnig at the Research Institute of Molecular Pathology in Austria and his colleagues have found that the lungfish\u2019s genome is 43 billion base pairs long, which is around 14 times larger than the human genome.\n\nIts genome is 30 per cent larger than that of the previous record holder: the axolotl, a Mexican amphibian that the team sequenced in 2018.\n\nAdvertisement\n\nThe researchers used high-powered computer sequencers to piece together the lungfish genome.\n\nTo account for inherent errors that the sequencers introduce, they used multiple copies of the genome, each fragmented into small pieces of DNA. After all the fragments were sequenced, the team used algorithms to reassemble the pieces into a complete genome.\n\nThe result took roughly 100,000 hours of computer processing power, Schloissnig estimates.\n\nThe Australian lungfish (Neoceratodus forsteri), native to south-east Queensland, has changed little in appearance since the time when animals began transitioning from a water-based to a terrestrial-based lifestyle, says Schloissnig.\n\nThe animal\u2019s fins are fleshy and flipper-like, and it has a single dorsal lung, which it can use to breathe air at the water\u2019s surface.\n\nPreviously, it was unclear whether lungfish or coelacanths \u2013 a group of archaic fish found in the Indian Ocean and around Indonesia \u2013 were more closely related to land-based vertebrates such as mammals and birds.\n\nThe new genomic analysis shows unequivocally that lungfish are more closely linked to the evolutionary line that gave rise to four-legged animals. Coelacanths diverged earlier, while lungfish branched off 420 million years ago.\n\n\u201cIn order to get out of the water, you need to adapt towards a terrestrial lifestyle,\u201d says Schloissnig. \u201cYou have to be able to breathe air, you have to be able to smell.\u201d\n\nThe Australian lungfish is similar to amphibians when it comes to the raw number of genes associated with the development of lungs and articulated limbs, as well as the detection of air-borne smells.\n\n\u201cWhen you look at it from a genomic perspective, it is genomically halfway between a fish and a land-based vertebrate,\u201d says Schloissnig.\n\nJournal reference: Nature, DOI: 10.1038/s41586-021-03198-8"
        },
        {
            "authors": [],
            "title": "Ask HN: Who has started a successful side business while having a full time job?",
            "contents": "To me it just seems simply impossible to start a side business while having a job. Between general life things, health, work and family/significant other, there seems to be barely any time to work on a side business let alone launch one and make it successful. If you have, how did you do it?"
        },
        {
            "authors": [],
            "title": "Ask HN: What other forums or social networks do you use?",
            "contents": "A lot of people are leaving major social networks like Reddit and Facebook - especially hackers - so I'm curious where they're settling in instead. I hear about Lobsters, Notabug, Ruqqus, Mastodon, etc., and there's probably dozens more, so I'd like to hear about what you use and how it suits you."
        },
        {
            "authors": [
                "Brooks Mckinney",
                "Jenni W. Gritters",
                "Kelly Mcsweeney",
                "Rick Robinson"
            ],
            "title": "Smart Drone Mailbox Secures the Future of \u201cLast Inch\u201d Deliveries \u2013 Now. Powered by Northrop Grumman",
            "contents": "The concept of delivering food, packaged goods and medical supplies to consumers by drone did not begin with COVID-19. But the pandemic has added urgency to the race, as companies such as Zipline, Wing Aviation, Amazon Prime Air and UPS Flight Forward attempt to mobilize networks of drones to deliver packages safely and quickly to consumers and commercial offices.\n\nPutting the Customer First\n\nUnfortunately, these companies plan to deliver time-sensitive packages to the ground outside a home or business, leaving them vulnerable to \u201cporch pirates,\u201d inclement weather or curious pets.\n\nEnter the smart drone mailbox: a technology-enabled container that opens, receives and stores packages in a safe, secure, climate-controlled box accessible only by a homeowner or business owner. It offers a potential solution to the more than 1.7 million packages lost or stolen in the U.S. daily, as CNBC reports.\n\n\u201cThe drone companies have been focused largely on all the innovative features of their drones, but if that innovation doesn\u2019t create a safe, secure and convenient experience for consumers, it will never be adopted,\u201d Ryan Walsh, CEO and co-founder of Chicago-based startup Valqari, a developer of smart drone mailboxes, said in an interview with Now.\n\nHowever, Walsh and Valqari are not alone in recognizing an opportunity to make drone delivery a safe, convenient and preferred service by consumers.\n\n\u201cI believe that when one person has an idea, 10 others have the same idea at the same time,\u201d Dan O\u2019Toole, CEO for Indianapolis-based DRONEDEK, another competitor in the smart drone mailbox space, told Now.\n\nPrecision Made Easy\n\nBoth Valqari and DRONEDEK are in the early stages of developing their mailbox concepts; Valqari is already producing units while DRONEDEK is working to complete its first functional prototype. The basic operating principles of their mailboxes are quite similar. Pending broader deployment of drone delivery networks, neither company expects smart drone mailboxes to become commonplace just yet.\n\nWhen a customer orders a delivery by drone, the package is loaded into a drone-compatible container, then flown to the customer location guided by GPS coordinates. Once the drone is within approximately 100 feet of the mailbox, it reverts to a vision-based landing system, then begins an authentication process with the smart drone mailbox. This process verifies that it\u2019s the right drone delivering the right package to the right mailbox.\n\nWhen the authentication process is complete, the drone lands on the mailbox, the mailbox opens its door, and the package is delivered. The drone then sends a notification of the delivery to both the shipper and the recipient to let them know that the smart drone mailbox has taken digital custody of the shipment and that it\u2019s now available for the recipient to pick up at their convenience. The recipient communicates with the smart drone mailbox via a phone-based app.\n\n\u201cOne of the great things about a smart drone delivery mailbox is that you don\u2019t have to leave work and stand in your front yard to receive a package,\u201d said Walsh. \u201cDrones are designed to provide a convenience for us, not an inconvenience.\u201d\n\nAnd yes, he adds, smart drone mailboxes are designed to be \u201cdrone agnostic\u201d and compatible with conventional delivery services. The same app that will allow a consumer to open the smart drone mailbox to retrieve a package will also allow a FedEx driver, a U.S. Postal Service employee or even an Amazon Prime driver to open the mailbox to deliver a package.\n\nReinventing the Mailbox\n\n\u201cThe mailbox has not been disrupted since 1858 (when the U.S. Postal Service was launched),\u201d said O\u2019Toole. \u201cBetween smart cars, smart phones and smart houses, it\u2019s time for the mailbox to catch up.\u201d\n\nTo that end, O\u2019Toole plans to give each DRONEDEK a patented, climate-controlled cargo area that could be set digitally by a shipping organization.\n\nO\u2019Toole is also thinking about how to streamline the logistics of customer returns. Future versions of DRONEDEK, he believes, could include a scale (to weigh packages), a printer (to address packages) and software to calculate shipping charges and bill the customer directly.\n\nAdapting to a \u201cNew Normal\u201d\n\nWhile customer convenience is important, public concern over the spread of COVID-19 has become a more significant factor in the design and development schedule of smart drone mailboxes.\n\n\u201cWe feel that COVID-19 has accelerated drone delivery by 10 years,\u201d said O\u2019Toole. \u201cIt\u2019s had a huge impact on people who\u2019ve been resistant to adopting new technology and relying on the Internet for goods and services.\u201d\n\nIn a nod to concerns about potential COVID-19 contamination of package surfaces, both Valqari and DRONEDEK are considering including an ultraviolet package sterilization feature in their mailboxes.\n\nMeeting Multi-User Demand\n\nThe types of smart drone mailboxes being developed also reflect the nascent market for such units. Valqari, for example, is focusing its early efforts on multi-user community boxes. These boxes could be located on, say, a hospital campus or within a village where it could serve many customers and handle multiple inbound and outbound package deliveries. Packages for different customers would be automatically sorted, stored and managed within separate internal compartments.\n\n\u201cOne of our biggest concerns is managing the safety needs of both our customers and the drones,\u201d explained Walsh. \u201cWe made our commercial unit tall enough, for example, for people to retrieve their packages easily from the station without being able to interfere with drone landing or take-off operations.\u201d\n\nValqari is also developing a stand-alone residential unit and, eventually, a window-attached unit for high-rise apartments, he added.\n\nFor its part, DRONEDEK plans to develop small, medium and large smart drone mailboxes for business owners, a residential box and a multi-family or urban cluster box.\n\nValqari plans to make its boxes available through subscriptions, lease agreements or as a retail purchase. If you decide to purchase a Valqari unit outright, Walsh says you can expect \u201cpricing in line with any other smart appliance in your house.\u201d\n\nHowever, both Valqari and DRONEDEK have recognized the need to monetize the \u201cbig data\u201d available from a community-based network of electronic sensor stations to help reduce costs of smart drone mailboxes for consumers.\n\n\u201cEvery DRONEDEK has the potential to be a weather station,\u201d said O\u2019Toole. \u201cWe think we can harvest and sell very localized weather data to the National Weather Service, for example.\u201d\n\nKeeping Life Simple\n\nFor all the extra features promised by Valqari and DRONEDEK, the main goal of smart drone mailboxes is to be effectively invisible, to be a non-issue for consumers and business owners alike.\n\n\u201cOur goal is to automate package deliveries for people so that they don\u2019t have to be home or worry about getting home faster because someone might steal their package,\u201d said Walsh. \u201cWe want to be that company that allows them to spend more time doing what they love and less time worrying about the logistics of their life.\u201d\n\nAre you interested in all things related to technology? We are, too. Check out Northrop Grumman career opportunities to see how you can participate in this fascinating time of discovery."
        },
        {
            "authors": [],
            "title": "dmca/2021-01-14-mpa.md at master \u00b7 github/dmca \u00b7 GitHub",
            "contents": "Dear GitHub Inc.:\n\nThe Motion Picture Association, Inc. (\u201cMPA\u201d) represents each of the major motion picture studios in the United States, specifically, Paramount Pictures Corporation, Sony Pictures Entertainment Inc., Universal City Studios LLC, Warner Bros. Entertainment Inc., Walt Disney Studios Motion Pictures, Netflix Studios, LLC, and their respective affiliates (collectively, the \u201cMPA Member Studios\u201d), which own or control exclusive rights under copyright in and to a vast number of motion pictures and television shows.\n\nWe are writing to notify you of, and request your assistance in addressing, the extensive copyright infringement of motion pictures and television programs that is occurring by virtue of the operation and further development of the Bittorrent website Nyaa.si\u2019s \u201cnyaa\u201d repository (the \u201cProject\u201d), which is hosted on and available for download from your repository GitHub.com (the \u201cRepository\u201d) found at https://github.com/nyaadevs/nyaa/ (the \u201cURL\u201d). Specifically, at the URL, the Repository hosts and offers for download the Project, which, when downloaded, provides the downloader everything necessary to launch and host a \u201cclone\u201d infringing website identical to Nyaa.si (and, thus, engage in massive infringement of copyrighted motion pictures and television shows).\n\nAttached as Exhibit A is a series of screenshots taken from the Nyaa.si website for the Project that include images of copyrighted works available through the Project. The representative titles shown in Exhibit A consist of only some of the motion pictures and television programs that are owned or controlled by the MPA Member Studios and that are being infringed via the Project.\n\nExhibit A is provided as a representative sample of the infringements being committed as a result of the operation of the Project and to demonstrate the readily apparent nature of the massive infringement occurring via the Project. The list is not intended to suggest that the identified infringements are the only ones occurring via the Project. Having been informed, through the representative examples, of the nature and scope of infringements occurring through the Project, we hope that you will act appropriately to address all infringement by the Project, not merely the identified representative examples.\n\nExhibit A, moreover, merely provides concrete examples of what is obvious from even a cursory review of the Project. The Project blatantly infringes the MPA Member Studios\u2019 copyrights and countless other copyrights. Indeed, copyright infringement is so prevalent within the Project that infringement plainly is its predominant use and purpose.\n\nFor the avoidance of any doubt, we are also providing you with the attached file tilted \u201cGitHub_code_Nyaa\u201d which shows code hosted on GitHub that provides all of the source code, templates, and utility API tools needed to host a copy of the Nyaa.si Bittorrent site, used to access infringing copies of motion pictures and television shows for which Nyaa.si provides torrent files and magnet links for the infringing content that users are looking for. The identified files and code are preconfigured to find and provide infringing copies of our Members\u2019 film and tv content to Nyaa.si users in violation of copyright law. For your convenience, we have included links to the infringing files below:\n\nhttps://github.com/nyaadevs/nyaa/blob/master/utils/api_info.py\n\nhttps://github.com/nyaadevs/nyaa/blob/master/utils/api_uploader_v2.py\n\nhttps://github.com/nyaadevs/nyaa/blob/master/nyaa/static/search-sukebei.xml\n\nhttps://github.com/nyaadevs/nyaa/blob/master/nyaa/api_handler.py\n\nhttps://github.com/nyaadevs/nyaa/blob/master/.docker/nyaa-config-partial.py\n\nhttps://github.com/nyaadevs/nyaa/blob/master/nyaa/torrents.py\n\nhttps://github.com/nyaadevs/nyaa/blob/master/nyaa/templates/home.html\n\nhttps://github.com/nyaadevs/nyaa/blob/master/migrations/versions/2bceb2cb4d7c_add_comment_count_to_torrent.py\n\nhttps://github.com/nyaadevs/nyaa/blob/master/config.example.py\n\nhttps://github.com/nyaadevs/nyaa/blob/master/.docker/es_sync_config.json\n\nBy this notification, we are asking for your immediate assistance in stopping your customer\u2019s unauthorized activity. Specifically, we request that you remove or disable access to the infringing Project\u2019s repositories referenced herein in accordance with either 17 U.S.C. \u00a7 512(c)(3)(A)(ii) (DMCA \u201crepresentative list\u201d provision), 17 U.S.C. \u00a7 512(i)(1)(A) (DMCA \u201crepeat infringer\u201d provision), and/or GitHub\u2019s Terms of Service, which prohibit use of your facilities for copyright infringement, see https://help.github.com/articles/github-terms-of-service. Moreover, the Project in question hosts software that is distributed and used to infringe on the MPA Member Studios\u2019 copyrights. See Metro-Goldwyn-Mayer Studios, Inc. v. Grokster Ltd., 545 U.S. 913, 940 n.13 (2005) (\u201cthe distribution of a product can itself give rise to liability where evidence shows that the distributor intended and encouraged the product to be used to infringe\u201d).\n\nWe are providing this notice based on our good faith belief that the use of motion pictures and television programs owned by the MPA Member Studios in the manner occurring via the Project is not authorized by the copyright owners, their agents, or the law. The information in this notification is accurate and, under penalty of perjury, we are authorized to act on behalf of the MPA Member Studios, which own or control exclusive rights under copyright that are being infringed in the manner described herein. This letter is without prejudice to the rights and remedies of the MPA Member Studios and their affiliates, all of which are expressly reserved.\n\nIf you have any questions, please contact me by telephone at [private] or via email at [private].\n\nRegards,\n\n[private]\n\nAttachments:\n\n[private]",
            "published_at": "2021-01-20T00:00:00"
        }
    ]
}